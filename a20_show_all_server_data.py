#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
a20_show_all_server_data.py - ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µãƒ¼ãƒãƒ¼çµ±åˆç›£è¦–ãƒ„ãƒ¼ãƒ«
=============================================================
èµ·å‹•: streamlit run a20_show_all_server_data.py --server.port=8502

ã€ä¸»è¦æ©Ÿèƒ½ã€‘
âœ… 4ã¤ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µãƒ¼ãƒãƒ¼ã®æ¥ç¶šçŠ¶æ…‹ãƒã‚§ãƒƒã‚¯ï¼ˆRedis, PostgreSQL, Elasticsearch, Qdrantï¼‰
âœ… å„ã‚µãƒ¼ãƒãƒ¼ã®ãƒ‡ãƒ¼ã‚¿è¡¨ç¤ºæ©Ÿèƒ½
âœ… ãƒ‡ãƒãƒƒã‚°æ©Ÿèƒ½ï¼ˆè©³ç´°ãƒ­ã‚°ã€æ¥ç¶šãƒ†ã‚¹ãƒˆã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šï¼‰
âœ… ã‚µãƒ¼ãƒãƒ¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤ºï¼ˆãƒ¡ãƒ¢ãƒªã€CPUã€ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒ ï¼‰
âœ… è‡ªå‹•ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥æ©Ÿèƒ½
âœ… ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½ï¼ˆCSV, JSONï¼‰
"""

import streamlit as st
import pandas as pd
import json
import time
import logging
import traceback
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
from pathlib import Path
import socket
import subprocess
import platform

# psutilã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning("psutil not available. Install with: pip install psutil")

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    import redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False
    logger.warning("Redis client not available. Install with: pip install redis")

try:
    import psycopg2
    POSTGRESQL_AVAILABLE = True
except ImportError:
    POSTGRESQL_AVAILABLE = False
    logger.warning("PostgreSQL client not available. Install with: pip install psycopg2")

try:
    from elasticsearch import Elasticsearch
    ELASTICSEARCH_AVAILABLE = True
except ImportError:
    ELASTICSEARCH_AVAILABLE = False
    logger.warning("Elasticsearch client not available. Install with: pip install elasticsearch")

try:
    from qdrant_client import QdrantClient
    QDRANT_AVAILABLE = True
except ImportError:
    QDRANT_AVAILABLE = False
    logger.warning("Qdrant client not available. Install with: pip install qdrant-client")

# ===================================================================
# ã‚µãƒ¼ãƒãƒ¼è¨­å®š
# ===================================================================
SERVER_CONFIG = {
    "redis": {
        "name": "Redis",
        "host": "localhost",
        "port": 6379,
        "icon": "ğŸ”´",
        "default_db": 0,
        "health_check_cmd": "redis-cli ping",
        "docker_image": "redis:latest"
    },
    "postgresql": {
        "name": "PostgreSQL",
        "host": "localhost",
        "port": 5432,
        "icon": "ğŸ˜",
        "database": "postgres",
        "user": "postgres",
        "password": "postgres",
        "health_check_cmd": "pg_isready",
        "docker_image": "postgres:latest"
    },
    "elasticsearch": {
        "name": "Elasticsearch",
        "host": "localhost",
        "port": 9200,
        "icon": "ğŸ”",
        "scheme": "http",
        "health_check_endpoint": "/_cluster/health",
        "docker_image": "elasticsearch:8.10.0"
    },
    "qdrant": {
        "name": "Qdrant",
        "host": "localhost",
        "port": 6333,
        "icon": "ğŸ¯",
        "url": "http://localhost:6333",
        "health_check_endpoint": "/collections",
        "docker_image": "qdrant/qdrant"
    }
}

# ===================================================================
# ã‚µãƒ¼ãƒãƒ¼æ¥ç¶šãƒã‚§ãƒƒã‚¯ã‚¯ãƒ©ã‚¹
# ===================================================================
class ServerHealthChecker:
    """å„ã‚µãƒ¼ãƒãƒ¼ã®æ¥ç¶šçŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯"""
    
    def __init__(self, debug_mode: bool = False):
        self.debug_mode = debug_mode
        self.connection_cache = {}
        
    def check_port(self, host: str, port: int, timeout: float = 2.0) -> bool:
        """ãƒãƒ¼ãƒˆãŒé–‹ã„ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(timeout)
            result = sock.connect_ex((host, port))
            sock.close()
            return result == 0
        except Exception as e:
            if self.debug_mode:
                logger.error(f"Port check failed for {host}:{port}: {e}")
            return False
    
    def check_redis(self) -> Tuple[bool, str, Optional[Dict]]:
        """Redisæ¥ç¶šãƒã‚§ãƒƒã‚¯"""
        config = SERVER_CONFIG["redis"]
        start_time = time.time()
        
        # ã¾ãšãƒãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯
        if not self.check_port(config["host"], config["port"]):
            return False, "Connection refused (port closed)", None
        
        if not REDIS_AVAILABLE:
            return False, "Redis client not installed", None
        
        try:
            client = redis.Redis(
                host=config["host"],
                port=config["port"],
                db=config["default_db"],
                socket_timeout=5,
                socket_connect_timeout=5
            )
            # PINGãƒ†ã‚¹ãƒˆ
            ping_result = client.ping()
            
            # ã‚µãƒ¼ãƒãƒ¼æƒ…å ±å–å¾—
            info = client.info()
            metrics = {
                "version": info.get("redis_version", "Unknown"),
                "uptime": info.get("uptime_in_seconds", 0),
                "connected_clients": info.get("connected_clients", 0),
                "used_memory": info.get("used_memory_human", "Unknown"),
                "response_time_ms": round((time.time() - start_time) * 1000, 2)
            }
            
            self.connection_cache["redis"] = client
            return True, "Connected", metrics
            
        except Exception as e:
            error_msg = str(e)
            if self.debug_mode:
                error_msg = f"{error_msg}\n{traceback.format_exc()}"
            return False, error_msg, None
    
    def check_postgresql(self) -> Tuple[bool, str, Optional[Dict]]:
        """PostgreSQLæ¥ç¶šãƒã‚§ãƒƒã‚¯"""
        config = SERVER_CONFIG["postgresql"]
        start_time = time.time()
        
        # ã¾ãšãƒãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯
        if not self.check_port(config["host"], config["port"]):
            return False, "Connection refused (port closed)", None
        
        if not POSTGRESQL_AVAILABLE:
            return False, "PostgreSQL client not installed", None
        
        try:
            conn = psycopg2.connect(
                host=config["host"],
                port=config["port"],
                database=config["database"],
                user=config["user"],
                password=config["password"],
                connect_timeout=5
            )
            
            cursor = conn.cursor()
            # ãƒãƒ¼ã‚¸ãƒ§ãƒ³å–å¾—
            cursor.execute("SELECT version()")
            version = cursor.fetchone()[0]
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒªã‚¹ãƒˆå–å¾—
            cursor.execute("SELECT COUNT(*) FROM pg_database")
            db_count = cursor.fetchone()[0]
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«æ•°å–å¾—
            cursor.execute("""
                SELECT COUNT(*) 
                FROM information_schema.tables 
                WHERE table_schema = 'public'
            """)
            table_count = cursor.fetchone()[0]
            
            metrics = {
                "version": version.split(' ')[1] if version else "Unknown",
                "database_count": db_count,
                "table_count": table_count,
                "response_time_ms": round((time.time() - start_time) * 1000, 2)
            }
            
            self.connection_cache["postgresql"] = conn
            return True, "Connected", metrics
            
        except Exception as e:
            error_msg = str(e)
            if self.debug_mode:
                error_msg = f"{error_msg}\n{traceback.format_exc()}"
            return False, error_msg, None
    
    def check_elasticsearch(self) -> Tuple[bool, str, Optional[Dict]]:
        """Elasticsearchæ¥ç¶šãƒã‚§ãƒƒã‚¯"""
        config = SERVER_CONFIG["elasticsearch"]
        start_time = time.time()
        
        # ã¾ãšãƒãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯
        if not self.check_port(config["host"], config["port"]):
            return False, "Connection refused (port closed)", None
        
        if not ELASTICSEARCH_AVAILABLE:
            return False, "Elasticsearch client not installed", None
        
        try:
            # Elasticsearch 8.xç”¨ã®è¨­å®š
            from elasticsearch import __version__ as es_version
            import urllib3
            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
            
            es = Elasticsearch(
                [f"{config['scheme']}://{config['host']}:{config['port']}"],
                request_timeout=5,
                verify_certs=False,
                ssl_show_warn=False,
                basic_auth=None  # å¿…è¦ã«å¿œã˜ã¦èªè¨¼æƒ…å ±ã‚’è¿½åŠ 
            )
            
            # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
            health = es.cluster.health()
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æƒ…å ±
            indices = es.indices.stats()
            
            metrics = {
                "cluster_name": health.get("cluster_name", "Unknown"),
                "status": health.get("status", "Unknown"),
                "node_count": health.get("number_of_nodes", 0),
                "index_count": len(indices.get("indices", {})),
                "response_time_ms": round((time.time() - start_time) * 1000, 2)
            }
            
            self.connection_cache["elasticsearch"] = es
            return True, "Connected", metrics
            
        except Exception as e:
            error_msg = str(e)
            if self.debug_mode:
                error_msg = f"{error_msg}\n{traceback.format_exc()}"
            return False, error_msg, None
    
    def check_qdrant(self) -> Tuple[bool, str, Optional[Dict]]:
        """Qdrantæ¥ç¶šãƒã‚§ãƒƒã‚¯"""
        config = SERVER_CONFIG["qdrant"]
        start_time = time.time()
        
        # ã¾ãšãƒãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯
        if not self.check_port(config["host"], config["port"]):
            return False, "Connection refused (port closed)", None
        
        if not QDRANT_AVAILABLE:
            return False, "Qdrant client not installed", None
        
        try:
            client = QdrantClient(url=config["url"], timeout=5)
            
            # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å–å¾—
            collections = client.get_collections()
            
            metrics = {
                "collection_count": len(collections.collections),
                "collections": [c.name for c in collections.collections],
                "response_time_ms": round((time.time() - start_time) * 1000, 2)
            }
            
            self.connection_cache["qdrant"] = client
            return True, "Connected", metrics
            
        except Exception as e:
            error_msg = str(e)
            if self.debug_mode:
                error_msg = f"{error_msg}\n{traceback.format_exc()}"
            return False, error_msg, None
    
    def check_all_servers(self) -> Dict[str, Tuple[bool, str, Optional[Dict]]]:
        """ã™ã¹ã¦ã®ã‚µãƒ¼ãƒãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯"""
        results = {}
        
        check_methods = {
            "redis": self.check_redis,
            "postgresql": self.check_postgresql,
            "elasticsearch": self.check_elasticsearch,
            "qdrant": self.check_qdrant
        }
        
        for server_key, check_method in check_methods.items():
            results[server_key] = check_method()
        
        return results

# ===================================================================
# ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¯ãƒ©ã‚¹
# ===================================================================
class ServerDataFetcher:
    """å„ã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    
    def __init__(self, checker: ServerHealthChecker):
        self.checker = checker
    
    def fetch_redis_data(self, pattern: str = "*", limit: int = 100) -> pd.DataFrame:
        """Redisã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        if "redis" not in self.checker.connection_cache:
            return pd.DataFrame({"Error": ["Not connected"]})
        
        try:
            client = self.checker.connection_cache["redis"]
            keys = client.keys(pattern)[:limit]
            
            data = []
            for key in keys:
                key_type = client.type(key).decode('utf-8')
                
                if key_type == "string":
                    value = client.get(key)
                    if value:
                        value = value.decode('utf-8', errors='ignore')
                elif key_type == "list":
                    value = f"List[{client.llen(key)} items]"
                elif key_type == "set":
                    value = f"Set[{client.scard(key)} items]"
                elif key_type == "hash":
                    value = f"Hash[{len(client.hkeys(key))} fields]"
                else:
                    value = f"Type: {key_type}"
                
                data.append({
                    "Key": key.decode('utf-8', errors='ignore'),
                    "Type": key_type,
                    "Value": value,
                    "TTL": client.ttl(key)
                })
            
            return pd.DataFrame(data) if data else pd.DataFrame({"Info": ["No data found"]})
            
        except Exception as e:
            return pd.DataFrame({"Error": [str(e)]})
    
    def fetch_postgresql_data(self, query: str = None) -> pd.DataFrame:
        """PostgreSQLã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        if "postgresql" not in self.checker.connection_cache:
            return pd.DataFrame({"Error": ["Not connected"]})
        
        try:
            conn = self.checker.connection_cache["postgresql"]
            cursor = conn.cursor()
            
            if query is None:
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ã‚’å–å¾—
                query = """
                    SELECT 
                        schemaname as "Schema",
                        tablename as "Table",
                        pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as "Size"
                    FROM pg_tables 
                    WHERE schemaname NOT IN ('pg_catalog', 'information_schema')
                    ORDER BY schemaname, tablename
                    LIMIT 100
                """
            
            # ã‚«ãƒ¼ã‚½ãƒ«ã§ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ
            cursor.execute(query)
            
            # ã‚«ãƒ©ãƒ åã‚’å–å¾—
            columns = [desc[0] for desc in cursor.description]
            
            # çµæœã‚’å–å¾—
            rows = cursor.fetchall()
            cursor.close()
            
            # DataFrameã‚’ä½œæˆ
            if rows:
                df = pd.DataFrame(rows, columns=columns)
                return df
            else:
                return pd.DataFrame({"Info": ["No data found"]})
            
        except Exception as e:
            return pd.DataFrame({"Error": [str(e)]})
    
    def fetch_elasticsearch_data(self, index: str = "*", size: int = 100) -> pd.DataFrame:
        """Elasticsearchã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        if "elasticsearch" not in self.checker.connection_cache:
            return pd.DataFrame({"Error": ["Not connected"]})
        
        try:
            es = self.checker.connection_cache["elasticsearch"]
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸€è¦§ã‚’å–å¾—
            indices = es.indices.get_alias(index=index)
            
            data = []
            for idx_name, idx_info in indices.items():
                stats = es.indices.stats(index=idx_name)
                idx_stats = stats["indices"][idx_name]["total"]
                
                data.append({
                    "Index": idx_name,
                    "Docs Count": idx_stats["docs"]["count"],
                    "Size": idx_stats["store"]["size_in_bytes"],
                    "Size (Human)": self._format_bytes(idx_stats["store"]["size_in_bytes"]),
                    "Aliases": ", ".join(idx_info.get("aliases", {}).keys()) or "None"
                })
            
            return pd.DataFrame(data) if data else pd.DataFrame({"Info": ["No indices found"]})
            
        except Exception as e:
            return pd.DataFrame({"Error": [str(e)]})
    
    def fetch_qdrant_data(self) -> pd.DataFrame:
        """Qdrantã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        if "qdrant" not in self.checker.connection_cache:
            return pd.DataFrame({"Error": ["Not connected"]})
        
        try:
            client = self.checker.connection_cache["qdrant"]
            collections = client.get_collections()
            
            data = []
            for collection in collections.collections:
                try:
                    info = client.get_collection(collection.name)
                    data.append({
                        "Collection": collection.name,
                        "Vectors Count": info.vectors_count,
                        "Points Count": info.points_count,
                        "Indexed Vectors": info.indexed_vectors_count,
                        "Status": info.status
                    })
                except:
                    data.append({
                        "Collection": collection.name,
                        "Vectors Count": "N/A",
                        "Points Count": "N/A",
                        "Indexed Vectors": "N/A",
                        "Status": "Error"
                    })
            
            return pd.DataFrame(data) if data else pd.DataFrame({"Info": ["No collections found"]})
            
        except Exception as e:
            return pd.DataFrame({"Error": [str(e)]})
    
    def fetch_qdrant_collection_points(self, collection_name: str, limit: int = 50) -> pd.DataFrame:
        """Qdrantã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        if "qdrant" not in self.checker.connection_cache:
            return pd.DataFrame({"Error": ["Not connected"]})
        
        try:
            client = self.checker.connection_cache["qdrant"]
            
            # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦ãƒã‚¤ãƒ³ãƒˆã‚’å–å¾—
            points_result = client.scroll(
                collection_name=collection_name,
                limit=limit,
                with_payload=True,
                with_vectors=False
            )
            
            points = points_result[0]  # scrollã¯ (points, next_offset) ã®ã‚¿ãƒ—ãƒ«ã‚’è¿”ã™
            
            if not points:
                return pd.DataFrame({"Info": ["No points found in collection"]})
            
            # ãƒã‚¤ãƒ³ãƒˆã‚’DataFrameã«å¤‰æ›
            data = []
            for point in points:
                row = {"ID": point.id}
                
                # payloadã®å„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’åˆ—ã¨ã—ã¦è¿½åŠ 
                if point.payload:
                    for key, value in point.payload.items():
                        # é•·ã™ãã‚‹æ–‡å­—åˆ—ã¯åˆ‡ã‚Šè©°ã‚
                        if isinstance(value, str) and len(value) > 200:
                            row[key] = value[:200] + '...'
                        elif isinstance(value, (list, dict)):
                            row[key] = str(value)[:200] + '...' if len(str(value)) > 200 else str(value)
                        else:
                            row[key] = value
                
                data.append(row)
            
            return pd.DataFrame(data)
            
        except Exception as e:
            return pd.DataFrame({"Error": [str(e)]})
    
    def fetch_qdrant_collection_info(self, collection_name: str) -> Dict[str, Any]:
        """Qdrantã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®è©³ç´°æƒ…å ±ã‚’å–å¾—"""
        if "qdrant" not in self.checker.connection_cache:
            return {"error": "Not connected"}
        
        try:
            client = self.checker.connection_cache["qdrant"]
            collection_info = client.get_collection(collection_name)
            
            # configã®æ§‹é€ ã‚’å®‰å…¨ã«ã‚¢ã‚¯ã‚»ã‚¹
            vector_config = collection_info.config.params.vectors
            
            # vector_configã®å‹ã‚’åˆ¤å®šã—ã¦é©åˆ‡ã«å‡¦ç†
            if hasattr(vector_config, 'size'):
                # å˜ä¸€ãƒ™ã‚¯ãƒˆãƒ«è¨­å®š
                vector_size = vector_config.size
                distance = vector_config.distance
            elif hasattr(vector_config, '__iter__'):
                # Named vectorsè¨­å®šã®å ´åˆ
                vector_sizes = {}
                distances = {}
                for name, config in vector_config.items() if isinstance(vector_config, dict) else []:
                    vector_sizes[name] = config.size if hasattr(config, 'size') else 'N/A'
                    distances[name] = config.distance if hasattr(config, 'distance') else 'N/A'
                vector_size = vector_sizes if vector_sizes else 'N/A'
                distance = distances if distances else 'N/A'
            else:
                vector_size = 'N/A'
                distance = 'N/A'
            
            return {
                "vectors_count": collection_info.vectors_count,
                "points_count": collection_info.points_count,
                "indexed_vectors": collection_info.indexed_vectors_count,
                "status": collection_info.status,
                "config": {
                    "vector_size": vector_size,
                    "distance": distance,
                }
            }
        except Exception as e:
            return {"error": str(e)}
    
    def _format_bytes(self, size: int) -> str:
        """ãƒã‚¤ãƒˆã‚µã‚¤ã‚ºã‚’äººé–“ãŒèª­ã¿ã‚„ã™ã„å½¢å¼ã«å¤‰æ›"""
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if size < 1024.0:
                return f"{size:.2f} {unit}"
            size /= 1024.0
        return f"{size:.2f} PB"

# ===================================================================
# Streamlit UI
# ===================================================================
def main():
    st.set_page_config(
        page_title="DB Server Monitor",
        page_icon="ğŸ”",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ–
    if "debug_mode" not in st.session_state:
        st.session_state.debug_mode = False
    if "auto_refresh" not in st.session_state:
        st.session_state.auto_refresh = False
    if "refresh_interval" not in st.session_state:
        st.session_state.refresh_interval = 30
    
    # ã‚¿ã‚¤ãƒˆãƒ«
    st.title("ğŸ” ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µãƒ¼ãƒãƒ¼çµ±åˆç›£è¦–ãƒ„ãƒ¼ãƒ«")
    st.markdown("Redis, PostgreSQL, Elasticsearch, Qdrant ã®çŠ¶æ…‹ç›£è¦–ã¨ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º")
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼ˆå·¦ãƒšã‚¤ãƒ³ï¼‰
    with st.sidebar:
        st.header("âš™ï¸ ã‚µãƒ¼ãƒãƒ¼æ¥ç¶šçŠ¶æ…‹")
        
        # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰åˆ‡ã‚Šæ›¿ãˆ
        debug_mode = st.checkbox("ğŸ› ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰", value=st.session_state.debug_mode)
        st.session_state.debug_mode = debug_mode
        
        # è‡ªå‹•ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥è¨­å®š
        col1, col2 = st.columns(2)
        with col1:
            auto_refresh = st.checkbox("ğŸ”„ è‡ªå‹•æ›´æ–°", value=st.session_state.auto_refresh)
            st.session_state.auto_refresh = auto_refresh
        with col2:
            if auto_refresh:
                refresh_interval = st.number_input("é–“éš”(ç§’)", min_value=5, max_value=300, value=30)
                st.session_state.refresh_interval = refresh_interval
        
        # æ¥ç¶šãƒã‚§ãƒƒã‚¯å®Ÿè¡Œãƒœã‚¿ãƒ³
        check_button = st.button("ğŸ” æ¥ç¶šãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ", type="primary", use_container_width=True)
        
        # HealthCheckerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        checker = ServerHealthChecker(debug_mode=debug_mode)
        
        # æ¥ç¶šçŠ¶æ…‹è¡¨ç¤ºã‚¨ãƒªã‚¢
        status_container = st.container()
        
        # è‡ªå‹•ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ã¾ãŸã¯ãƒœã‚¿ãƒ³æŠ¼ä¸‹æ™‚ã«å®Ÿè¡Œ
        if check_button or (auto_refresh and time.time() % refresh_interval < 1):
            with status_container:
                with st.spinner("ãƒã‚§ãƒƒã‚¯ä¸­..."):
                    results = checker.check_all_servers()
                
                # å„ã‚µãƒ¼ãƒãƒ¼ã®çŠ¶æ…‹è¡¨ç¤º
                for server_key, config in SERVER_CONFIG.items():
                    is_connected, message, metrics = results[server_key]
                    
                    # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º
                    if is_connected:
                        st.success(f"{config['icon']} **{config['name']}**")
                        st.caption(f"âœ… {message}")
                        
                        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
                        if metrics and debug_mode:
                            with st.expander(f"è©³ç´°æƒ…å ±", expanded=False):
                                for key, value in metrics.items():
                                    st.text(f"{key}: {value}")
                    else:
                        st.error(f"{config['icon']} **{config['name']}**")
                        st.caption(f"âŒ {message}")
                        
                        # ã‚¨ãƒ©ãƒ¼è©³ç´°ï¼ˆãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ï¼‰
                        if debug_mode:
                            with st.expander("ã‚¨ãƒ©ãƒ¼è©³ç´°", expanded=False):
                                st.code(message)
                                st.caption(f"Host: {config.get('host')}:{config.get('port')}")
                                
                                # Dockerèµ·å‹•ã‚³ãƒãƒ³ãƒ‰è¡¨ç¤º
                                if "docker_image" in config:
                                    st.info("Dockerèµ·å‹•ã‚³ãƒãƒ³ãƒ‰:")
                                    if server_key == "redis":
                                        cmd = f"docker run -d -p {config['port']}:{config['port']} {config['docker_image']}"
                                    elif server_key == "postgresql":
                                        cmd = f"docker run -d -p {config['port']}:{config['port']} -e POSTGRES_PASSWORD=postgres {config['docker_image']}"
                                    elif server_key == "elasticsearch":
                                        cmd = f"docker run -d -p {config['port']}:{config['port']} -e discovery.type=single-node {config['docker_image']}"
                                    elif server_key == "qdrant":
                                        cmd = f"docker run -d -p {config['port']}:{config['port']} {config['docker_image']}"
                                    st.code(cmd, language="bash")
                
                st.divider()
                
                # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ï¼ˆãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ï¼‰
                if debug_mode and PSUTIL_AVAILABLE:
                    st.subheader("ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±")
                    cpu_percent = psutil.cpu_percent(interval=1)
                    memory = psutil.virtual_memory()
                    
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric("CPUä½¿ç”¨ç‡", f"{cpu_percent}%")
                    with col2:
                        st.metric("ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡", f"{memory.percent}%")
                    
                    st.caption(f"Platform: {platform.system()} {platform.release()}")
                    st.caption(f"Python: {platform.python_version()}")
                elif debug_mode:
                    st.subheader("ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±")
                    st.caption(f"Platform: {platform.system()} {platform.release()}")
                    st.caption(f"Python: {platform.python_version()}")
                    st.info("psutil not installed. Install with: pip install psutil")
        
        # ã‚µãƒ¼ãƒãƒ¼é¸æŠãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³
        st.divider()
        st.subheader("ğŸ“Š ãƒ‡ãƒ¼ã‚¿è¡¨ç¤ºã‚µãƒ¼ãƒãƒ¼é¸æŠ")
        server_options = {
            "redis": f"{SERVER_CONFIG['redis']['icon']} Redis",
            "postgresql": f"{SERVER_CONFIG['postgresql']['icon']} PostgreSQL",
            "elasticsearch": f"{SERVER_CONFIG['elasticsearch']['icon']} Elasticsearch",
            "qdrant": f"{SERVER_CONFIG['qdrant']['icon']} Qdrant"
        }
        selected_server = st.radio(
            "è¡¨ç¤ºã™ã‚‹ã‚µãƒ¼ãƒãƒ¼ã‚’é¸æŠ",
            options=list(server_options.keys()),
            format_func=lambda x: server_options[x],
            key="selected_server"
        )
    
    # ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ï¼ˆå³ãƒšã‚¤ãƒ³ï¼‰
    st.header(f"ğŸ“Š {server_options[selected_server]} ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º")
    
    # ãƒ‡ãƒ¼ã‚¿å–å¾—ç”¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    data_fetcher = ServerDataFetcher(checker)
    
    # é¸æŠã•ã‚ŒãŸã‚µãƒ¼ãƒãƒ¼ã«å¿œã˜ã¦è¡¨ç¤ºã‚’åˆ‡ã‚Šæ›¿ãˆ
    if selected_server == "redis":
        col1, col2, col3 = st.columns([2, 2, 1])
        with col1:
            pattern = st.text_input("æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³", value="*", key="redis_pattern")
        with col2:
            limit = st.number_input("è¡¨ç¤ºä»¶æ•°", min_value=1, max_value=1000, value=100, key="redis_limit")
        with col3:
            fetch_redis = st.button("ğŸ” ãƒ‡ãƒ¼ã‚¿å–å¾—", key="fetch_redis")
        
        if fetch_redis:
            with st.spinner("Redisãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­..."):
                try:
                    if not REDIS_AVAILABLE:
                        st.error("Redis clientãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
                        st.code("pip install redis", language="bash")
                    else:
                        # ç›´æ¥Redisã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆã—ã¦æ¥ç¶š
                        import redis
                        redis_config = SERVER_CONFIG["redis"]
                        redis_client = redis.Redis(
                            host=redis_config["host"],
                            port=redis_config["port"],
                            db=redis_config["default_db"],
                            socket_timeout=5,
                            socket_connect_timeout=5,
                            decode_responses=True  # æ–‡å­—åˆ—ã¨ã—ã¦è‡ªå‹•ãƒ‡ã‚³ãƒ¼ãƒ‰
                        )
                        
                        # æ¥ç¶šç¢ºèª
                        redis_client.ping()
                        
                        # ã‚­ãƒ¼ã‚’å–å¾—
                        keys = redis_client.keys(pattern)
                        
                        if not keys:
                            st.info(f"ãƒ‘ã‚¿ãƒ¼ãƒ³ '{pattern}' ã«ä¸€è‡´ã™ã‚‹ã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                        else:
                            # limitã‚’é©ç”¨
                            keys = keys[:limit]
                            
                            data = []
                            for key in keys:
                                try:
                                    key_type = redis_client.type(key)
                                    
                                    if key_type == "string":
                                        value = redis_client.get(key)
                                        if value and len(value) > 200:
                                            value = value[:200] + '...'
                                    elif key_type == "list":
                                        list_len = redis_client.llen(key)
                                        sample = redis_client.lrange(key, 0, 2)
                                        value = f"List[{list_len} items]: {sample[:3]}..."
                                    elif key_type == "set":
                                        set_size = redis_client.scard(key)
                                        sample = list(redis_client.smembers(key))[:3]
                                        value = f"Set[{set_size} items]: {sample}..."
                                    elif key_type == "hash":
                                        hash_len = redis_client.hlen(key)
                                        sample = dict(list(redis_client.hgetall(key).items())[:3])
                                        value = f"Hash[{hash_len} fields]: {sample}..."
                                    elif key_type == "zset":
                                        zset_size = redis_client.zcard(key)
                                        sample = redis_client.zrange(key, 0, 2, withscores=True)
                                        value = f"ZSet[{zset_size} items]: {sample[:3]}..."
                                    else:
                                        value = f"Type: {key_type}"
                                    
                                    ttl = redis_client.ttl(key)
                                    ttl_str = "æ°¸ç¶š" if ttl == -1 else f"{ttl}ç§’" if ttl > 0 else "æœŸé™åˆ‡ã‚Œ"
                                    
                                    data.append({
                                        "Key": key,
                                        "Type": key_type,
                                        "Value": value,
                                        "TTL": ttl_str
                                    })
                                except Exception as e:
                                    data.append({
                                        "Key": key,
                                        "Type": "Error",
                                        "Value": str(e)[:100],
                                        "TTL": "N/A"
                                    })
                            
                            if data:
                                df = pd.DataFrame(data)
                                st.write(f"**æ¤œç´¢çµæœ: {len(data)} ä»¶ã®ã‚­ãƒ¼**")
                                st.dataframe(df, use_container_width=True)
                                
                                # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½
                                col1, col2 = st.columns(2)
                                with col1:
                                    csv = df.to_csv(index=False)
                                    st.download_button(
                                        label="ğŸ“¥ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                                        data=csv,
                                        file_name=f"redis_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                                        mime="text/csv"
                                    )
                                with col2:
                                    json_str = df.to_json(orient="records", indent=2)
                                    st.download_button(
                                        label="ğŸ“¥ JSONãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                                        data=json_str,
                                        file_name=f"redis_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                                        mime="application/json"
                                    )
                            else:
                                st.info("ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                                
                except redis.ConnectionError as e:
                    st.error(f"Redisæ¥ç¶šã‚¨ãƒ©ãƒ¼: {str(e)}")
                    st.info("Redisã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
                    st.code("docker run -d -p 6379:6379 redis:latest", language="bash")
                except Exception as e:
                    st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                    import traceback
                    if debug_mode:
                        st.code(traceback.format_exc())
    
    elif selected_server == "postgresql":
        # ã‚¿ãƒ–ã‚’ä½œæˆ
        tab1, tab2, tab3 = st.tabs(["ğŸ“Š ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§", "ğŸ“ SQLã‚¯ã‚¨ãƒª", "ğŸ” ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º"])
        
        with tab1:
            st.subheader("ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§")
            if st.button("ğŸ”„ ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ã‚’å–å¾—", key="fetch_tables"):
                with st.spinner("ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ã‚’å–å¾—ä¸­..."):
                    # PostgreSQLæ¥ç¶šã‚’æ–°è¦ä½œæˆ
                    pg_checker = ServerHealthChecker(debug_mode=debug_mode)
                    pg_result = pg_checker.check_postgresql()
                    
                    if pg_result[0]:
                        pg_fetcher = ServerDataFetcher(pg_checker)
                        # ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ã‚’å–å¾—
                        table_query = """
                            SELECT 
                                schemaname as "Schema",
                                tablename as "Table",
                                pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as "Size"
                            FROM pg_tables 
                            WHERE schemaname NOT IN ('pg_catalog', 'information_schema')
                            ORDER BY schemaname, tablename
                        """
                        df_tables = pg_fetcher.fetch_postgresql_data(query=table_query)
                        
                        if not df_tables.empty and "Table" in df_tables.columns:
                            st.success(f"âœ… {df_tables.shape[0]} å€‹ã®ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
                            st.dataframe(df_tables, use_container_width=True)
                            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«ãƒ†ãƒ¼ãƒ–ãƒ«ãƒªã‚¹ãƒˆã‚’ä¿å­˜
                            st.session_state['pg_tables'] = df_tables[['Schema', 'Table']].values.tolist()
                        else:
                            st.info("ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    else:
                        st.error(f"PostgreSQLæ¥ç¶šã‚¨ãƒ©ãƒ¼: {pg_result[1]}")
        
        with tab2:
            st.subheader("SQLã‚¯ã‚¨ãƒª")
            
            # ã¾ãšãƒ†ãƒ¼ãƒ–ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—
            if 'pg_tables_for_query' not in st.session_state:
                if st.button("ğŸ“‹ ãƒ†ãƒ¼ãƒ–ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—", key="get_tables_for_query"):
                    with st.spinner("ãƒ†ãƒ¼ãƒ–ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—ä¸­..."):
                        pg_checker = ServerHealthChecker(debug_mode=debug_mode)
                        pg_result = pg_checker.check_postgresql()
                        
                        if pg_result[0]:
                            pg_fetcher = ServerDataFetcher(pg_checker)
                            table_query = """
                                SELECT schemaname, tablename 
                                FROM pg_tables 
                                WHERE schemaname NOT IN ('pg_catalog', 'information_schema')
                                ORDER BY schemaname, tablename
                            """
                            df_tables = pg_fetcher.fetch_postgresql_data(query=table_query)
                            if not df_tables.empty:
                                st.session_state['pg_tables_for_query'] = df_tables.values.tolist()
                                st.rerun()
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«é¸æŠã¨ã‚¯ã‚¨ãƒªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
            if 'pg_tables_for_query' in st.session_state and st.session_state['pg_tables_for_query']:
                col1, col2 = st.columns([2, 1])
                
                with col1:
                    # ãƒ†ãƒ¼ãƒ–ãƒ«é¸æŠ
                    table_options = ["ã‚«ã‚¹ã‚¿ãƒ ã‚¯ã‚¨ãƒª"] + [f"{schema}.{table}" for schema, table in st.session_state['pg_tables_for_query']]
                    selected_table_for_query = st.selectbox(
                        "ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’é¸æŠï¼ˆã¾ãŸã¯ã‚«ã‚¹ã‚¿ãƒ ã‚¯ã‚¨ãƒªï¼‰",
                        options=table_options,
                        key="selected_table_for_query"
                    )
                
                with col2:
                    # ã‚¯ã‚¨ãƒªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé¸æŠ
                    if selected_table_for_query != "ã‚«ã‚¹ã‚¿ãƒ ã‚¯ã‚¨ãƒª":
                        query_templates = {
                            "ã™ã¹ã¦é¸æŠ": f"SELECT * FROM {selected_table_for_query}",
                            "å…ˆé ­10è¡Œ": f"SELECT * FROM {selected_table_for_query} LIMIT 10",
                            "å…ˆé ­100è¡Œ": f"SELECT * FROM {selected_table_for_query} LIMIT 100",
                            "è¡Œæ•°ã‚«ã‚¦ãƒ³ãƒˆ": f"SELECT COUNT(*) as total_rows FROM {selected_table_for_query}",
                            "ã‚«ãƒ©ãƒ æƒ…å ±": f"SELECT column_name, data_type, is_nullable FROM information_schema.columns WHERE table_schema || '.' || table_name = '{selected_table_for_query}'",
                            "æœ€è¿‘è¿½åŠ ã•ã‚ŒãŸãƒ¬ã‚³ãƒ¼ãƒ‰": f"SELECT * FROM {selected_table_for_query} ORDER BY id DESC LIMIT 10",
                            "ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒ«": f"SELECT * FROM {selected_table_for_query} ORDER BY RANDOM() LIMIT 10",
                        }
                        
                        template = st.selectbox(
                            "ã‚¯ã‚¨ãƒªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ",
                            options=list(query_templates.keys()),
                            key="query_template"
                        )
                        
                        # é¸æŠã•ã‚ŒãŸãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ã‚¯ã‚¨ãƒªã‚’è¨­å®š
                        default_query = query_templates[template]
                    else:
                        default_query = "SELECT * FROM test_users LIMIT 10"
                
                # ã‚¯ã‚¨ãƒªã‚¨ãƒ‡ã‚£ã‚¿
                query = st.text_area(
                    "SQLã‚¯ã‚¨ãƒªï¼ˆç·¨é›†å¯èƒ½ï¼‰",
                    value=default_query if 'pg_tables_for_query' in st.session_state else "SELECT * FROM test_users LIMIT 10",
                    height=150,
                    key="pg_query_editor"
                )
                
                # è¡Œæ•°åˆ¶é™ã‚ªãƒ—ã‚·ãƒ§ãƒ³
                col1, col2, col3 = st.columns([1, 1, 2])
                with col1:
                    add_limit = st.checkbox("è¡Œæ•°åˆ¶é™ã‚’è¿½åŠ ", key="add_limit")
                with col2:
                    if add_limit:
                        limit_value = st.number_input("åˆ¶é™è¡Œæ•°", min_value=1, max_value=10000, value=100, key="limit_value")
                        if "LIMIT" not in query.upper():
                            query = f"{query.rstrip(';')} LIMIT {limit_value}"
                
                # ã‚¯ã‚¨ãƒªå®Ÿè¡Œãƒœã‚¿ãƒ³
                fetch_pg = st.button("ğŸ” ã‚¯ã‚¨ãƒªå®Ÿè¡Œ", key="fetch_pg", type="primary")
                
                if fetch_pg:
                    with st.spinner("ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œä¸­..."):
                        # PostgreSQLæ¥ç¶šã‚’æ–°è¦ä½œæˆ
                        pg_checker = ServerHealthChecker(debug_mode=debug_mode)
                        pg_result = pg_checker.check_postgresql()
                        
                        if pg_result[0]:
                            # æ¥ç¶šæˆåŠŸæ™‚ã«ãƒ‡ãƒ¼ã‚¿å–å¾—
                            pg_fetcher = ServerDataFetcher(pg_checker)
                            
                            # ã‚¯ã‚¨ãƒªã®å®Ÿè¡Œæ™‚é–“ã‚’è¨ˆæ¸¬
                            import time
                            start_time = time.time()
                            df = pg_fetcher.fetch_postgresql_data(query=query)
                            execution_time = time.time() - start_time
                            
                            if not df.empty and "Error" not in df.columns and "Info" not in df.columns:
                                # å®Ÿè¡Œçµæœã®çµ±è¨ˆ
                                col1, col2, col3, col4 = st.columns(4)
                                with col1:
                                    st.metric("å–å¾—è¡Œæ•°", f"{df.shape[0]:,}")
                                with col2:
                                    st.metric("åˆ—æ•°", f"{df.shape[1]:,}")
                                with col3:
                                    st.metric("å®Ÿè¡Œæ™‚é–“", f"{execution_time:.3f}ç§’")
                                with col4:
                                    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆæ¦‚ç®—ï¼‰
                                    memory_usage = df.memory_usage(deep=True).sum() / 1024 / 1024
                                    st.metric("ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º", f"{memory_usage:.2f} MB")
                                
                                # ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º
                                st.dataframe(df, use_container_width=True, height=400)
                                
                                # ãƒ‡ãƒ¼ã‚¿åˆ†æã‚»ã‚¯ã‚·ãƒ§ãƒ³
                                with st.expander("ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†æ", expanded=False):
                                    # æ•°å€¤åˆ—ã®çµ±è¨ˆ
                                    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns
                                    if len(numeric_cols) > 0:
                                        st.write("**æ•°å€¤åˆ—ã®çµ±è¨ˆ:**")
                                        st.dataframe(df[numeric_cols].describe(), use_container_width=True)
                                    
                                    # ã‚«ãƒ©ãƒ æƒ…å ±
                                    col_info = pd.DataFrame({
                                        'ã‚«ãƒ©ãƒ å': df.columns,
                                        'ãƒ‡ãƒ¼ã‚¿å‹': df.dtypes.astype(str),
                                        'NULLæ•°': df.isnull().sum(),
                                        'ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤æ•°': [df[col].nunique() for col in df.columns],
                                        'ã‚µãƒ³ãƒ—ãƒ«å€¤': [str(df[col].iloc[0]) if len(df) > 0 else 'N/A' for col in df.columns]
                                    })
                                    st.write("**ã‚«ãƒ©ãƒ æƒ…å ±:**")
                                    st.dataframe(col_info, use_container_width=True)
                                
                                # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½
                                st.divider()
                                col1, col2, col3 = st.columns(3)
                                with col1:
                                    csv = df.to_csv(index=False)
                                    st.download_button(
                                        label="ğŸ“¥ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                                        data=csv,
                                        file_name=f"query_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                                        mime="text/csv"
                                    )
                                with col2:
                                    json_str = df.to_json(orient="records", indent=2)
                                    st.download_button(
                                        label="ğŸ“¥ JSONãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                                        data=json_str,
                                        file_name=f"query_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                                        mime="application/json"
                                    )
                                with col3:
                                    # ã‚¯ã‚¨ãƒªè‡ªä½“ã‚‚ä¿å­˜
                                    st.download_button(
                                        label="ğŸ“¥ SQLã‚¯ã‚¨ãƒªä¿å­˜",
                                        data=query,
                                        file_name=f"query_{datetime.now().strftime('%Y%m%d_%H%M%S')}.sql",
                                        mime="text/plain"
                                    )
                                    
                            elif "Info" in df.columns:
                                st.info(df.iloc[0]["Info"])
                            elif "Error" in df.columns:
                                st.error(f"ã‚¯ã‚¨ãƒªå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {df.iloc[0]['Error']}")
                                st.code(query, language="sql")
                            else:
                                st.warning("ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                        else:
                            st.error(f"PostgreSQLæ¥ç¶šã‚¨ãƒ©ãƒ¼: {pg_result[1]}")
                            st.info("PostgreSQLã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
                            st.code("docker run -d -p 5432:5432 -e POSTGRES_PASSWORD=postgres postgres:latest", language="bash")
            else:
                st.info("ã¾ãšã€Œãƒ†ãƒ¼ãƒ–ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„")
        
        with tab3:
            st.subheader("ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º")
            
            # ã¾ãšãƒ†ãƒ¼ãƒ–ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—
            if 'pg_tables' not in st.session_state:
                if st.button("ğŸ“‹ ãƒ†ãƒ¼ãƒ–ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—", key="get_table_list"):
                    with st.spinner("ãƒ†ãƒ¼ãƒ–ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—ä¸­..."):
                        pg_checker = ServerHealthChecker(debug_mode=debug_mode)
                        pg_result = pg_checker.check_postgresql()
                        
                        if pg_result[0]:
                            pg_fetcher = ServerDataFetcher(pg_checker)
                            table_query = """
                                SELECT schemaname, tablename 
                                FROM pg_tables 
                                WHERE schemaname NOT IN ('pg_catalog', 'information_schema')
                                ORDER BY schemaname, tablename
                            """
                            df_tables = pg_fetcher.fetch_postgresql_data(query=table_query)
                            if not df_tables.empty:
                                st.session_state['pg_tables'] = df_tables.values.tolist()
                                st.rerun()
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«é¸æŠã¨ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º
            if 'pg_tables' in st.session_state and st.session_state['pg_tables']:
                col1, col2, col3 = st.columns([2, 2, 1])
                
                with col1:
                    # ãƒ†ãƒ¼ãƒ–ãƒ«é¸æŠ
                    table_options = [f"{schema}.{table}" for schema, table in st.session_state['pg_tables']]
                    selected_table = st.selectbox(
                        "ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’é¸æŠ",
                        options=table_options,
                        key="selected_pg_table"
                    )
                
                with col2:
                    # è¡Œæ•°åˆ¶é™
                    row_limit = st.number_input(
                        "è¡¨ç¤ºè¡Œæ•°",
                        min_value=1,
                        max_value=10000,
                        value=100,
                        key="pg_row_limit"
                    )
                
                with col3:
                    # ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒœã‚¿ãƒ³
                    fetch_table_data = st.button("ğŸ“Š ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º", key="fetch_table_data")
                
                if fetch_table_data and selected_table:
                    with st.spinner(f"{selected_table} ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­..."):
                        # PostgreSQLæ¥ç¶šã‚’æ–°è¦ä½œæˆ
                        pg_checker = ServerHealthChecker(debug_mode=debug_mode)
                        pg_result = pg_checker.check_postgresql()
                        
                        if pg_result[0]:
                            pg_fetcher = ServerDataFetcher(pg_checker)
                            
                            # é¸æŠã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                            data_query = f"SELECT * FROM {selected_table} LIMIT {row_limit}"
                            df_data = pg_fetcher.fetch_postgresql_data(query=data_query)
                            
                            if not df_data.empty and "Error" not in df_data.columns:
                                # ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’è¡¨ç¤º
                                st.success(f"âœ… {selected_table} ã‹ã‚‰ {df_data.shape[0]} è¡Œã‚’å–å¾—")
                                
                                # ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆæƒ…å ±
                                col1, col2, col3 = st.columns(3)
                                with col1:
                                    st.metric("è¡Œæ•°", df_data.shape[0])
                                with col2:
                                    st.metric("åˆ—æ•°", df_data.shape[1])
                                with col3:
                                    # ç·è¡Œæ•°ã‚’å–å¾—
                                    count_query = f"SELECT COUNT(*) as total FROM {selected_table}"
                                    df_count = pg_fetcher.fetch_postgresql_data(query=count_query)
                                    if not df_count.empty:
                                        total_rows = df_count.iloc[0]['total']
                                        st.metric("ç·è¡Œæ•°", total_rows)
                                
                                # ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º
                                st.dataframe(df_data, use_container_width=True)
                                
                                # ã‚«ãƒ©ãƒ æƒ…å ±ã‚’è¡¨ç¤º
                                with st.expander("ğŸ“‹ ã‚«ãƒ©ãƒ æƒ…å ±"):
                                    col_info = pd.DataFrame({
                                        'ã‚«ãƒ©ãƒ å': df_data.columns,
                                        'ãƒ‡ãƒ¼ã‚¿å‹': df_data.dtypes.astype(str),
                                        'NULLä»¥å¤–ã®å€¤': df_data.count(),
                                        'ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤æ•°': [df_data[col].nunique() for col in df_data.columns]
                                    })
                                    st.dataframe(col_info, use_container_width=True)
                                
                                # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½
                                st.divider()
                                col1, col2 = st.columns(2)
                                with col1:
                                    csv = df_data.to_csv(index=False)
                                    st.download_button(
                                        label=f"ğŸ“¥ {selected_table} CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                                        data=csv,
                                        file_name=f"{selected_table.replace('.', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                                        mime="text/csv"
                                    )
                                with col2:
                                    json_str = df_data.to_json(orient="records", indent=2)
                                    st.download_button(
                                        label=f"ğŸ“¥ {selected_table} JSONãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                                        data=json_str,
                                        file_name=f"{selected_table.replace('.', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                                        mime="application/json"
                                    )
                            elif "Error" in df_data.columns:
                                st.error(f"ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {df_data.iloc[0]['Error']}")
                            else:
                                st.warning("ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                        else:
                            st.error(f"PostgreSQLæ¥ç¶šã‚¨ãƒ©ãƒ¼: {pg_result[1]}")
            else:
                st.info("ã¾ãšã€Œãƒ†ãƒ¼ãƒ–ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„")
    
    elif selected_server == "elasticsearch":
        # ã‚¿ãƒ–ã‚’ä½œæˆ
        tab1, tab2, tab3, tab4 = st.tabs([
            "ğŸ“Š ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸€è¦§", 
            "ğŸ” ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢",
            "ğŸ“ˆ ã‚¯ãƒ©ã‚¹ã‚¿çµ±è¨ˆ",
            "ğŸ”§ è©³ç´°åˆ†æ"
        ])
        
        with tab1:
            st.subheader("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸€è¦§ã¨è©³ç´°")
            
            col1, col2, col3 = st.columns([2, 1, 1])
            with col1:
                index_pattern = st.text_input("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³", value="*", key="es_index_pattern")
            with col2:
                show_system = st.checkbox("ã‚·ã‚¹ãƒ†ãƒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å«ã‚€", key="show_system_indices")
            with col3:
                fetch_indices = st.button("ğŸ”„ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å–å¾—", key="fetch_indices")
            
            if fetch_indices:
                with st.spinner("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æƒ…å ±ã‚’å–å¾—ä¸­..."):
                    es_checker = ServerHealthChecker(debug_mode=debug_mode)
                    es_result = es_checker.check_elasticsearch()
                    
                    if es_result[0]:
                        es = es_checker.connection_cache["elasticsearch"]
                        
                        try:
                            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸€è¦§ã‚’å–å¾—
                            indices = es.indices.get_alias(index=index_pattern)
                            
                            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æƒ…å ±ã‚’æ•´ç†
                            index_data = []
                            for idx_name, idx_info in indices.items():
                                # ã‚·ã‚¹ãƒ†ãƒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                                if not show_system and (idx_name.startswith('.') or idx_name.startswith('_')):
                                    continue
                                
                                # çµ±è¨ˆæƒ…å ±ã‚’å–å¾—
                                stats = es.indices.stats(index=idx_name)
                                idx_stats = stats["indices"][idx_name]["total"]
                                
                                # ãƒãƒƒãƒ”ãƒ³ã‚°æƒ…å ±ã‚’å–å¾—
                                mappings = es.indices.get_mapping(index=idx_name)
                                field_count = 0
                                if idx_name in mappings and "mappings" in mappings[idx_name]:
                                    properties = mappings[idx_name]["mappings"].get("properties", {})
                                    field_count = len(properties)
                                
                                # è¨­å®šæƒ…å ±ã‚’å–å¾—
                                settings = es.indices.get_settings(index=idx_name)
                                shards = settings[idx_name]["settings"]["index"].get("number_of_shards", "N/A")
                                replicas = settings[idx_name]["settings"]["index"].get("number_of_replicas", "N/A")
                                
                                index_data.append({
                                    "ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å": idx_name,
                                    "ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°": f"{idx_stats['docs']['count']:,}",
                                    "å‰Šé™¤æ¸ˆã¿": f"{idx_stats['docs'].get('deleted', 0):,}",
                                    "ã‚µã‚¤ã‚º": f"{idx_stats['store']['size_in_bytes'] / 1024 / 1024:.2f} MB",
                                    "ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ•°": field_count,
                                    "ã‚·ãƒ£ãƒ¼ãƒ‰æ•°": shards,
                                    "ãƒ¬ãƒ—ãƒªã‚«æ•°": replicas,
                                    "ã‚¨ã‚¤ãƒªã‚¢ã‚¹": ", ".join(idx_info.get("aliases", {}).keys()) or "ãªã—"
                                })
                            
                            if index_data:
                                df_indices = pd.DataFrame(index_data)
                                st.success(f"âœ… {len(df_indices)} å€‹ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
                                
                                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸€è¦§ã‚’è¡¨ç¤º
                                st.dataframe(df_indices, use_container_width=True, height=400)
                                
                                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒªã‚¹ãƒˆã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«ä¿å­˜
                                st.session_state['es_indices'] = df_indices["ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å"].tolist()
                                
                                # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½
                                col1, col2 = st.columns(2)
                                with col1:
                                    csv = df_indices.to_csv(index=False)
                                    st.download_button(
                                        label="ğŸ“¥ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                                        data=csv,
                                        file_name=f"es_indices_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                                        mime="text/csv"
                                    )
                                with col2:
                                    json_str = df_indices.to_json(orient="records", indent=2)
                                    st.download_button(
                                        label="ğŸ“¥ JSONãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                                        data=json_str,
                                        file_name=f"es_indices_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                                        mime="application/json"
                                    )
                            else:
                                st.info("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                        except Exception as e:
                            st.error(f"ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    else:
                        st.error(f"Elasticsearchæ¥ç¶šã‚¨ãƒ©ãƒ¼: {es_result[1]}")
        
        with tab2:
            st.subheader("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢ã¨è¡¨ç¤º")
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é¸æŠ
            if 'es_indices' not in st.session_state:
                st.info("ã¾ãšã€Œã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸€è¦§ã€ã‚¿ãƒ–ã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—ã—ã¦ãã ã•ã„")
            else:
                col1, col2 = st.columns([2, 1])
                
                with col1:
                    selected_index = st.selectbox(
                        "ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’é¸æŠ",
                        options=st.session_state['es_indices'],
                        key="selected_es_index"
                    )
                
                with col2:
                    doc_limit = st.number_input(
                        "å–å¾—ä»¶æ•°",
                        min_value=1,
                        max_value=1000,
                        value=10,
                        key="es_doc_limit"
                    )
                
                # æ¤œç´¢ã‚¯ã‚¨ãƒª
                search_query = st.text_area(
                    "æ¤œç´¢ã‚¯ã‚¨ãƒªï¼ˆJSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼‰â€»ç©ºç™½ã®å ´åˆã¯å…¨ä»¶å–å¾—",
                    value='{\n  "match_all": {}\n}',
                    height=100,
                    key="es_search_query"
                )
                
                # æ¤œç´¢å®Ÿè¡Œ
                if st.button("ğŸ” ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢", key="search_docs"):
                    with st.spinner("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¤œç´¢ä¸­..."):
                        es_checker = ServerHealthChecker(debug_mode=debug_mode)
                        es_result = es_checker.check_elasticsearch()
                        
                        if es_result[0]:
                            es = es_checker.connection_cache["elasticsearch"]
                            
                            try:
                                # ã‚¯ã‚¨ãƒªã‚’ãƒ‘ãƒ¼ã‚¹
                                import json
                                if search_query.strip():
                                    query = json.loads(search_query)
                                else:
                                    query = {"match_all": {}}
                                
                                # æ¤œç´¢å®Ÿè¡Œ
                                response = es.search(
                                    index=selected_index,
                                    query=query,
                                    size=doc_limit
                                )
                                
                                # çµæœã‚’å‡¦ç†
                                hits = response["hits"]["hits"]
                                total = response["hits"]["total"]["value"]
                                
                                st.success(f"âœ… {total:,} ä»¶ä¸­ {len(hits)} ä»¶ã‚’è¡¨ç¤º")
                                
                                # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¡¨å½¢å¼ã§è¡¨ç¤º
                                if hits:
                                    # ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
                                    docs_data = []
                                    for hit in hits:
                                        doc = {
                                            "_id": hit["_id"],
                                            "_score": hit.get("_score", "N/A")
                                        }
                                        # _sourceãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ãƒ•ãƒ©ãƒƒãƒˆåŒ–
                                        source = hit.get("_source", {})
                                        for key, value in source.items():
                                            # é•·ã„æ–‡å­—åˆ—ã¯åˆ‡ã‚Šè©°ã‚
                                            if isinstance(value, str) and len(value) > 100:
                                                doc[key] = value[:100] + "..."
                                            elif isinstance(value, (dict, list)):
                                                doc[key] = json.dumps(value, ensure_ascii=False)[:100] + "..."
                                            else:
                                                doc[key] = value
                                        docs_data.append(doc)
                                    
                                    df_docs = pd.DataFrame(docs_data)
                                    st.dataframe(df_docs, use_container_width=True, height=400)
                                    
                                    # è©³ç´°è¡¨ç¤ºï¼ˆã‚¨ã‚­ã‚¹ãƒ‘ãƒ³ãƒ€ãƒ¼ï¼‰
                                    with st.expander("ğŸ“ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè©³ç´°ï¼ˆJSONå½¢å¼ï¼‰"):
                                        for i, hit in enumerate(hits[:5], 1):  # æœ€åˆã®5ä»¶ã®ã¿
                                            st.write(f"**ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ {i} (ID: {hit['_id']})**")
                                            st.json(hit["_source"])
                                            if i < min(5, len(hits)):
                                                st.divider()
                                    
                                    # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
                                    col1, col2 = st.columns(2)
                                    with col1:
                                        csv = df_docs.to_csv(index=False)
                                        st.download_button(
                                            label="ğŸ“¥ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                                            data=csv,
                                            file_name=f"{selected_index}_docs_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                                            mime="text/csv"
                                        )
                                    with col2:
                                        json_str = json.dumps(hits, ensure_ascii=False, indent=2)
                                        st.download_button(
                                            label="ğŸ“¥ JSONãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                                            data=json_str,
                                            file_name=f"{selected_index}_docs_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                                            mime="application/json"
                                        )
                                else:
                                    st.info("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                                    
                            except json.JSONDecodeError:
                                st.error("æ¤œç´¢ã‚¯ã‚¨ãƒªã®JSONå½¢å¼ãŒä¸æ­£ã§ã™")
                            except Exception as e:
                                st.error(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
                        else:
                            st.error(f"Elasticsearchæ¥ç¶šã‚¨ãƒ©ãƒ¼: {es_result[1]}")
        
        with tab3:
            st.subheader("ã‚¯ãƒ©ã‚¹ã‚¿çµ±è¨ˆæƒ…å ±")
            
            if st.button("ğŸ“Š çµ±è¨ˆæƒ…å ±ã‚’å–å¾—", key="get_cluster_stats"):
                with st.spinner("ã‚¯ãƒ©ã‚¹ã‚¿æƒ…å ±ã‚’å–å¾—ä¸­..."):
                    es_checker = ServerHealthChecker(debug_mode=debug_mode)
                    es_result = es_checker.check_elasticsearch()
                    
                    if es_result[0]:
                        es = es_checker.connection_cache["elasticsearch"]
                        
                        try:
                            # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ˜ãƒ«ã‚¹
                            health = es.cluster.health()
                            
                            # ã‚¯ãƒ©ã‚¹ã‚¿çµ±è¨ˆ
                            stats = es.cluster.stats()
                            
                            # ãƒãƒ¼ãƒ‰æƒ…å ±
                            nodes = es.nodes.info()
                            
                            # ã‚¯ãƒ©ã‚¹ã‚¿çŠ¶æ…‹ã‚’è¡¨ç¤º
                            col1, col2, col3, col4 = st.columns(4)
                            
                            with col1:
                                status_color = {"green": "ğŸŸ¢", "yellow": "ğŸŸ¡", "red": "ğŸ”´"}
                                st.metric("ã‚¯ãƒ©ã‚¹ã‚¿çŠ¶æ…‹", f"{status_color.get(health['status'], 'âšª')} {health['status'].upper()}")
                            
                            with col2:
                                st.metric("ãƒãƒ¼ãƒ‰æ•°", f"{health['number_of_nodes']:,}")
                            
                            with col3:
                                st.metric("ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ãƒ‰æ•°", f"{health['number_of_data_nodes']:,}")
                            
                            with col4:
                                st.metric("ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚·ãƒ£ãƒ¼ãƒ‰", f"{health['active_shards']:,}")
                            
                            # è©³ç´°çµ±è¨ˆ
                            st.divider()
                            
                            col1, col2 = st.columns(2)
                            
                            with col1:
                                st.write("**ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çµ±è¨ˆ:**")
                                st.write(f"â€¢ ç·ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ•°: {stats['indices']['count']:,}")
                                st.write(f"â€¢ ç·ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {stats['indices']['docs']['count']:,}")
                                st.write(f"â€¢ ç·ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {stats['indices']['store']['size_in_bytes'] / 1024 / 1024 / 1024:.2f} GB")
                                st.write(f"â€¢ ç·ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ•°: {stats['indices']['mappings']['field_types'][0]['count'] if stats['indices']['mappings']['field_types'] else 0:,}")
                            
                            with col2:
                                st.write("**ãƒãƒ¼ãƒ‰æƒ…å ±:**")
                                for node_id, node_info in nodes['nodes'].items():
                                    st.write(f"â€¢ ãƒãƒ¼ãƒ‰å: {node_info['name']}")
                                    st.write(f"  - ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {node_info['version']}")
                                    st.write(f"  - ãƒ­ãƒ¼ãƒ«: {', '.join(node_info.get('roles', []))}")
                                    break  # æœ€åˆã®ãƒãƒ¼ãƒ‰ã®ã¿è¡¨ç¤º
                            
                            # JVMãƒ¡ãƒ¢ãƒªæƒ…å ±
                            if 'jvm' in stats['nodes']:
                                st.divider()
                                st.write("**JVMãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³:**")
                                jvm = stats['nodes']['jvm']['mem']
                                heap_used = jvm['heap_used_in_bytes'] / 1024 / 1024
                                heap_max = jvm['heap_max_in_bytes'] / 1024 / 1024
                                st.progress(heap_used / heap_max, text=f"ãƒ’ãƒ¼ãƒ—ä½¿ç”¨ç‡: {heap_used:.1f} MB / {heap_max:.1f} MB ({heap_used/heap_max*100:.1f}%)")
                            
                        except Exception as e:
                            st.error(f"çµ±è¨ˆæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    else:
                        st.error(f"Elasticsearchæ¥ç¶šã‚¨ãƒ©ãƒ¼: {es_result[1]}")
        
        with tab4:
            st.subheader("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è©³ç´°åˆ†æ")
            
            if 'es_indices' not in st.session_state:
                st.info("ã¾ãšã€Œã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸€è¦§ã€ã‚¿ãƒ–ã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—ã—ã¦ãã ã•ã„")
            else:
                selected_index_detail = st.selectbox(
                    "åˆ†æã™ã‚‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’é¸æŠ",
                    options=st.session_state['es_indices'],
                    key="selected_index_detail"
                )
                
                if st.button("ğŸ”¬ è©³ç´°åˆ†æ", key="analyze_index"):
                    with st.spinner(f"{selected_index_detail} ã‚’åˆ†æä¸­..."):
                        es_checker = ServerHealthChecker(debug_mode=debug_mode)
                        es_result = es_checker.check_elasticsearch()
                        
                        if es_result[0]:
                            es = es_checker.connection_cache["elasticsearch"]
                            
                            try:
                                # ãƒãƒƒãƒ”ãƒ³ã‚°æƒ…å ±
                                mappings = es.indices.get_mapping(index=selected_index_detail)
                                
                                # è¨­å®šæƒ…å ±
                                settings = es.indices.get_settings(index=selected_index_detail)
                                
                                # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰çµ±è¨ˆ
                                field_caps = es.field_caps(index=selected_index_detail, fields="*")
                                
                                # ãƒãƒƒãƒ”ãƒ³ã‚°è¡¨ç¤º
                                st.write("**ğŸ“‹ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°:**")
                                if selected_index_detail in mappings:
                                    properties = mappings[selected_index_detail]["mappings"].get("properties", {})
                                    
                                    field_data = []
                                    for field_name, field_info in properties.items():
                                        field_data.append({
                                            "ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å": field_name,
                                            "ã‚¿ã‚¤ãƒ—": field_info.get("type", "N/A"),
                                            "ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹": field_info.get("index", True),
                                            "ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼": field_info.get("analyzer", "standard")
                                        })
                                    
                                    if field_data:
                                        df_fields = pd.DataFrame(field_data)
                                        st.dataframe(df_fields, use_container_width=True)
                                
                                # è¨­å®šè¡¨ç¤º
                                with st.expander("âš™ï¸ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨­å®š"):
                                    index_settings = settings[selected_index_detail]["settings"]["index"]
                                    st.json({
                                        "ä½œæˆæ—¥": index_settings.get("creation_date", "N/A"),
                                        "UUID": index_settings.get("uuid", "N/A"),
                                        "ã‚·ãƒ£ãƒ¼ãƒ‰æ•°": index_settings.get("number_of_shards", "N/A"),
                                        "ãƒ¬ãƒ—ãƒªã‚«æ•°": index_settings.get("number_of_replicas", "N/A"),
                                        "ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥é–“éš”": index_settings.get("refresh_interval", "N/A")
                                    })
                                
                                # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿å–å¾—
                                st.divider()
                                st.write("**ğŸ“ ã‚µãƒ³ãƒ—ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ (æœ€æ–°5ä»¶):**")
                                
                                sample_response = es.search(
                                    index=selected_index_detail,
                                    size=5,
                                    sort="_doc"
                                )
                                
                                for i, hit in enumerate(sample_response["hits"]["hits"], 1):
                                    with st.expander(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ {i} (ID: {hit['_id']})"):
                                        st.json(hit["_source"])
                                
                            except Exception as e:
                                st.error(f"åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    elif selected_server == "qdrant":
        # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æ¦‚è¦è¡¨ç¤º
        st.subheader("ğŸ“š ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä¸€è¦§")
        
        # Qdrantã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®å†æ¥ç¶šï¼ˆãƒ‡ãƒ¼ã‚¿å–å¾—ã®ãŸã‚ï¼‰
        try:
            if not QDRANT_AVAILABLE:
                st.warning("Qdrant clientãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
                st.code("pip install qdrant-client", language="bash")
            else:
                # ç›´æ¥Qdrantã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆã—ã¦æ¥ç¶š
                from qdrant_client import QdrantClient
                qdrant_config = SERVER_CONFIG["qdrant"]
                client = QdrantClient(url=qdrant_config["url"], timeout=5)
                
                # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä¸€è¦§ã‚’è‡ªå‹•å–å¾—
                try:
                    collections = client.get_collections()
                    if collections.collections:
                        collection_names = [c.name for c in collections.collections]
                        st.session_state['qdrant_collections'] = collection_names
                        
                        # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’DataFrameåŒ–
                        data = []
                        for collection in collections.collections:
                            try:
                                info = client.get_collection(collection.name)
                                data.append({
                                    "Collection": collection.name,
                                    "Vectors Count": info.vectors_count,
                                    "Points Count": info.points_count,
                                    "Indexed Vectors": info.indexed_vectors_count,
                                    "Status": info.status
                                })
                            except Exception as e:
                                data.append({
                                    "Collection": collection.name,
                                    "Vectors Count": "N/A",
                                    "Points Count": "N/A",
                                    "Indexed Vectors": "N/A",
                                    "Status": f"Error: {str(e)[:50]}"
                                })
                        
                        if data:
                            df = pd.DataFrame(data)
                            st.dataframe(df, use_container_width=True)
                            
                            # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½
                            col1, col2 = st.columns(2)
                            with col1:
                                csv = df.to_csv(index=False)
                                st.download_button(
                                    label="ğŸ“¥ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                                    data=csv,
                                    file_name=f"qdrant_collections_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                                    mime="text/csv"
                                )
                            with col2:
                                json_str = df.to_json(orient="records", indent=2)
                                st.download_button(
                                    label="ğŸ“¥ JSONãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                                    data=json_str,
                                    file_name=f"qdrant_collections_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                                    mime="application/json"
                                )
                    else:
                        st.info("ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                        
                except Exception as e:
                    st.error(f"ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä¸€è¦§ã®å–å¾—ã«å¤±æ•—: {str(e)}")
                    st.info("Qdrantã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
                    st.code("docker run -p 6333:6333 qdrant/qdrant", language="bash")
                    
        except Exception as e:
            st.error(f"Qdrantæ¥ç¶šã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³è©³ç´°è¡¨ç¤º
        st.divider()
        st.subheader("ğŸ” ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³è©³ç´°ãƒ‡ãƒ¼ã‚¿")
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã‹ã‚‰ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä¸€è¦§ã‚’å–å¾—
        if 'qdrant_collections' in st.session_state and st.session_state['qdrant_collections']:
            selected_collection = st.selectbox(
                "è©³ç´°ã‚’è¡¨ç¤ºã™ã‚‹ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠ",
                options=st.session_state['qdrant_collections'],
                key="selected_collection"
            )
            
            col1, col2, col3 = st.columns([1, 1, 2])
            with col1:
                limit = st.number_input("è¡¨ç¤ºä»¶æ•°", min_value=1, max_value=500, value=50, key="qdrant_limit")
            with col2:
                show_details = st.button("ğŸ“Š è©³ç´°æƒ…å ±ã‚’è¡¨ç¤º", key="show_collection_details")
            with col3:
                fetch_points = st.button("ğŸ” ãƒã‚¤ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—", key="fetch_collection_points")
            
            # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³è©³ç´°æƒ…å ±ã®è¡¨ç¤º
            if show_details and QDRANT_AVAILABLE:
                with st.spinner(f"{selected_collection} ã®è©³ç´°æƒ…å ±ã‚’å–å¾—ä¸­..."):
                    try:
                        # ç›´æ¥ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨
                        info = client.get_collection(selected_collection)
                        
                        col1, col2, col3, col4 = st.columns(4)
                        with col1:
                            st.metric("ãƒ™ã‚¯ãƒˆãƒ«æ•°", info.vectors_count)
                        with col2:
                            st.metric("ãƒã‚¤ãƒ³ãƒˆæ•°", info.points_count)
                        with col3:
                            st.metric("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ¸ˆã¿", info.indexed_vectors_count)
                        with col4:
                            st.metric("ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹", info.status)
                        
                        # è¨­å®šæƒ…å ±
                        st.write("**ãƒ™ã‚¯ãƒˆãƒ«è¨­å®š:**")
                        try:
                            vector_config = info.config.params.vectors
                            if hasattr(vector_config, 'size'):
                                st.write(f"  â€¢ ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒ: {vector_config.size}")
                                st.write(f"  â€¢ è·é›¢è¨ˆç®—: {vector_config.distance}")
                            else:
                                st.write(f"  â€¢ è¨­å®š: {vector_config}")
                        except:
                            st.write("  â€¢ ãƒ™ã‚¯ãƒˆãƒ«è¨­å®šæƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“")
                            
                    except Exception as e:
                        st.error(f"ã‚¨ãƒ©ãƒ¼: {str(e)}")
            
            # ãƒã‚¤ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
            if fetch_points and QDRANT_AVAILABLE:
                with st.spinner(f"{selected_collection} ã®ãƒã‚¤ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­..."):
                    try:
                        # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦ãƒã‚¤ãƒ³ãƒˆã‚’å–å¾—
                        points_result = client.scroll(
                            collection_name=selected_collection,
                            limit=limit,
                            with_payload=True,
                            with_vectors=False
                        )
                        
                        points = points_result[0]  # scrollã¯ (points, next_offset) ã®ã‚¿ãƒ—ãƒ«ã‚’è¿”ã™
                        
                        if points:
                            # ãƒã‚¤ãƒ³ãƒˆã‚’DataFrameã«å¤‰æ›
                            data = []
                            for point in points:
                                row = {"ID": point.id}
                                
                                # payloadã®å„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’åˆ—ã¨ã—ã¦è¿½åŠ 
                                if point.payload:
                                    for key, value in point.payload.items():
                                        # é•·ã™ãã‚‹æ–‡å­—åˆ—ã¯åˆ‡ã‚Šè©°ã‚
                                        if isinstance(value, str) and len(value) > 200:
                                            row[key] = value[:200] + '...'
                                        elif isinstance(value, (list, dict)):
                                            row[key] = str(value)[:200] + '...' if len(str(value)) > 200 else str(value)
                                        else:
                                            row[key] = value
                                
                                data.append(row)
                            
                            if data:
                                df_points = pd.DataFrame(data)
                                st.write(f"**{selected_collection} ã®ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ« ({len(df_points)} ä»¶):**")
                                st.dataframe(df_points, use_container_width=True)
                                
                                # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½
                                col1, col2 = st.columns(2)
                                with col1:
                                    csv = df_points.to_csv(index=False)
                                    st.download_button(
                                        label="ğŸ“¥ ãƒã‚¤ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                                        data=csv,
                                        file_name=f"{selected_collection}_points_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                                        mime="text/csv"
                                    )
                                with col2:
                                    json_str = df_points.to_json(orient="records", indent=2)
                                    st.download_button(
                                        label="ğŸ“¥ ãƒã‚¤ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ JSONãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                                        data=json_str,
                                        file_name=f"{selected_collection}_points_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                                        mime="application/json"
                                    )
                        else:
                            st.info("ãƒã‚¤ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                            
                    except Exception as e:
                        st.error(f"ãƒã‚¤ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        else:
            st.info("ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚Qdrantã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    
    # ãƒ•ãƒƒã‚¿ãƒ¼
    st.divider()
    st.caption(f"æœ€çµ‚æ›´æ–°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±è¡¨ç¤º
    if debug_mode:
        with st.expander("ğŸ› ãƒ‡ãƒãƒƒã‚°æƒ…å ±", expanded=False):
            st.subheader("ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ")
            st.write({
                "Redis": "âœ…" if REDIS_AVAILABLE else "âŒ",
                "PostgreSQL": "âœ…" if POSTGRESQL_AVAILABLE else "âŒ",
                "Elasticsearch": "âœ…" if ELASTICSEARCH_AVAILABLE else "âŒ",
                "Qdrant": "âœ…" if QDRANT_AVAILABLE else "âŒ",
                "psutil": "âœ…" if PSUTIL_AVAILABLE else "âŒ"
            })
            
            st.subheader("ã‚µãƒ¼ãƒãƒ¼è¨­å®š")
            st.json(SERVER_CONFIG)
            
            st.subheader("æ¥ç¶šã‚­ãƒ£ãƒƒã‚·ãƒ¥")
            st.write(list(checker.connection_cache.keys()) if 'checker' in locals() else [])

if __name__ == "__main__":
    main()