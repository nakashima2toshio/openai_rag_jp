# RAG (Retrieval-Augmented Generation) の実践ガイド

## 1. 「文書チャンク」とは？

### 文書を直接ベクトル化

RAGの基本的な仕組みは、ユーザーのクエリに対して、文書を「チャンク」に分割し、これらをベクトル化して類似検索を実施することです。OpenAI Cookbookでは、PDFのページ、スライド、画像付きリポートなど、多様なコンテンツを直接ベクトル化する方法が紹介されています ([OpenAI Cookbook](https://cookbook.openai.com/))。

### チャンクの設計

チャンクは一般的に200–500トークン程度のサイズで作成され、隣接するチャンク間には20–50トークン程度の重複を設けるのが推奨されています ([Medium](https://medium.com/), [sagacify.com](https://sagacify.com/))。

チャンクの分割方法（文字数、段落単位、ヘッダ単位、意味的分割など）は、最終的な回答精度や検索コストに直接影響します ([Medium](https://medium.com/), [Vectara](https://vectara.com/))。

### メタデータの活用

元の文書ファイル名やURL、章番号などをメタデータとして保存することで、検索結果の出典をユーザーに提示しやすくなります ([sagacify.com](https://sagacify.com/))。

---

## 2. Q&Aペアが役立つケースと扱い方


| 適した場面              | 具体例                                    | 実装ポイント                                                                       |
| ----------------------- | ----------------------------------------- | ---------------------------------------------------------------------------------- |
| FAQボット／ヘルプデスク | 「返品の締切は？」「送料はいくら？」      | 質問と回答をそれぞれ別のベクトルにするか、質問と回答を1つのチャンクにまとめる      |
| 合成QAでリコール向上    | ドメイン文書からLLMに自動生成させた疑問文 | 生成した質問文をコーパスに追加すると、ユーザ質問との距離が縮まりヒット率が向上する |

Label StudioのRAGチュートリアルでは、「まず既存文書からQAを作成して精度を底上げする」手法が紹介されています ([Label Studio](https://labelstud.io/))。

また、PineconeやWeaviateでは、「質問フィールドだけをベクトル検索し、回答をコンテキストとして追加する」方法が一般的です ([Reddit](https://reddit.com/), [Pinecone](https://pinecone.io/))。

さらに、embeddingモデル訓練時に、質問-回答やポジティブ-ネガティブのペアを利用することで、リトリーバ性能が向上することも多くのベンチマークで報告されています ([Unstructured](https://unstructured.io/))。

---

## 3. 「文書チャンク」か「Q&Aペア」かを選ぶ指針


| 観点               | 文書チャンク中心の設計                             | Q&Aペアを追加した設計                          |
| ------------------ | -------------------------------------------------- | ---------------------------------------------- |
| セットアップの手間 | 既存文書を分割するだけで比較的容易                 | QAを手作業または自動生成で用意する必要がある   |
| 網羅性             | 原文を直接使うため情報の漏れが少ない               | 生成しきれない質問は拾えない                   |
| 検索マッチ         | ユーザ質問との表現の差が大きいと距離が伸びる可能性 | 質問文が予め準備されているためマッチ精度が高い |
| 拡張性             | 長文引用や部分的要約に強い                         | FAQ形式で即答させたい場合に有効                |

実運用では、基本的に「文書チャンク」を中心に、頻繁に尋ねられる質問や自動生成したQ&Aペアを補助的なインデックスとして併用する構成が広く採用されています ([Stack Overflow Blog](https://stackoverflow.blog/), [IBM](https://ibm.com/))。

---

## 4. OpenAI API + ベクターストア実装フロー

### 前処理・チャンク化

`tiktoken`などを用いて、文書をトークン数で分割します。

### 埋め込み作成（Embedding）

```python
result = client.embeddings.create(
    model="text-embedding-3-small",
    input=[chunk["text"] for chunk in chunks]
)
```

### ベクターストアへアップサート（Upsert）

OpenAI Cookbookでは、Pinecone、Qdrant、Weaviate、Redis、BigQuery Vector Searchなど、多数のベクターストアが利用可能な例として挙げられています ([OpenAI Cookbook](https://cookbook.openai.com/))。

### クエリ時のフロー

```
ユーザー質問 → Embedding → kNN検索 → 上位チャンク／回答をシステムプロンプトに添付 → chat/completions
```

### (任意) FAQペアの取り込み

* `question_text`フィールドを検索可能なベクトルとして登録
* `answer_text`はメタデータまたは別フィールドとして保持

---

## 5. まとめとベストプラクティス

* 必ずしもQ&A形式である必要はなく、「検索しやすく分割された知識チャンク」が基本。
* Q&AペアはFAQ用途やクリック誘導の補強策として非常に有効。
* チャンクサイズの調整とメタデータ設計が精度向上とコスト削減の鍵。
* OpenAI Embeddingsは、任意のテキストを内容に依存せずベクトル化可能。文書・コード・PDF・スライド・画像入りページなど、多種多様なコンテンツを扱えます ([OpenAI Cookbook](https://cookbook.openai.com/))。

上記の指針を踏まえ、ユースケース（FAQボットか汎用QAか）に適した「文書チャンク＋Q&Aペアのハイブリッド設計」を採用することで、RAGシステムの再現率と精度の向上が期待できます。
