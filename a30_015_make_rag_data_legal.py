# a30_015_make_rag_data_legal.py
# æ³•å¾‹ãƒ»åˆ¤ä¾‹QAãƒ‡ãƒ¼ã‚¿ã®RAGå‰å‡¦ç†ï¼ˆhelper_rag.pyåˆ©ç”¨ç‰ˆï¼‰
# streamlit run a30_015_make_rag_data_legal.py --server.port=8505

import streamlit as st
import pandas as pd
import logging
import re
from typing import List
from pathlib import Path

# å…±é€šæ©Ÿèƒ½ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from helper_rag import (
    AppConfig, RAGConfig, TokenManager, safe_execute,
    select_model, show_model_info, estimate_token_usage,
    validate_data, load_dataset, process_rag_data,
    create_download_data, display_statistics, save_files_to_output,
    show_usage_instructions, setup_page_config, setup_page_header, setup_sidebar_header
)

# ===================================================================
# helper_rag.py ã‚’åˆ©ç”¨ã—ãŸæ”¹ä¿®ç‰ˆ
# ===================================================================

# åŸºæœ¬ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ==================================================
# æ³•å¾‹ãƒ»åˆ¤ä¾‹QAç‰¹æœ‰ã®å‡¦ç†é–¢æ•°
# ==================================================
def validate_legal_data_specific(df) -> List[str]:
    """æ³•å¾‹ãƒ»åˆ¤ä¾‹QAãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®æ¤œè¨¼"""
    legal_issues = []

    # æ³•å¾‹é–¢é€£ç”¨èªã®å­˜åœ¨ç¢ºèª
    legal_keywords = [
        'æ³•å¾‹', 'æ¡æ–‡', 'åˆ¤ä¾‹', 'è£åˆ¤', 'æœ€é«˜è£', 'åœ°è£', 'é«˜è£', 'æ°‘æ³•', 'åˆ‘æ³•', 'å•†æ³•', 'æ†²æ³•',
        'å¥‘ç´„', 'æå®³è³ å„Ÿ', 'é•æ³•', 'åˆæ³•', 'æ¨©åˆ©', 'ç¾©å‹™', 'è²¬ä»»',
        'law', 'legal', 'court', 'judge', 'civil', 'criminal', 'contract', 'liability'
    ]

    # å¤§æ–‡å­—å°æ–‡å­—ã‚’è€ƒæ…®ã—ãŸåˆ—åæ¤œç´¢
    question_col = None
    for col in df.columns:
        if 'question' in col.lower():
            question_col = col
            break

    if question_col is not None:
        questions_with_legal_terms = 0
        for _, row in df.iterrows():
            question_text = str(row.get(question_col, '')).lower()
            if any(keyword in question_text for keyword in legal_keywords):
                questions_with_legal_terms += 1

        legal_ratio = (questions_with_legal_terms / len(df)) * 100
        legal_issues.append(f"æ³•å¾‹é–¢é€£ç”¨èªã‚’å«ã‚€è³ªå•: {questions_with_legal_terms:,}ä»¶ ({legal_ratio:.1f}%)")

    # å›ç­”ã®é•·ã•åˆ†æï¼ˆæ³•å¾‹å›ç­”ã¯é€šå¸¸è©³ç´°ã§é•·ã„ï¼‰
    answer_col = None
    for col in df.columns:
        if 'answer' in col.lower():
            answer_col = col
            break

    if answer_col is not None:
        answer_lengths = df[answer_col].astype(str).str.len()
        avg_answer_length = answer_lengths.mean()
        if avg_answer_length < 100:
            legal_issues.append(f"âš ï¸ å¹³å‡å›ç­”é•·ãŒçŸ­ã„å¯èƒ½æ€§: {avg_answer_length:.0f}æ–‡å­—")
        else:
            legal_issues.append(f"âœ… é©åˆ‡ãªå›ç­”é•·: å¹³å‡{avg_answer_length:.0f}æ–‡å­—")

        # è©³ç´°ãªå›ç­”é•·åˆ†æ
        short_answers = (answer_lengths <= 100).sum()
        medium_answers = ((answer_lengths > 100) & (answer_lengths <= 500)).sum()
        long_answers = (answer_lengths > 500).sum()

        legal_issues.append(f"çŸ­ã„å›ç­”ï¼ˆâ‰¤100æ–‡å­—ï¼‰: {short_answers:,}ä»¶")
        legal_issues.append(f"ä¸­ç¨‹åº¦ã®å›ç­”ï¼ˆ101-500æ–‡å­—ï¼‰: {medium_answers:,}ä»¶")
        legal_issues.append(f"é•·ã„å›ç­”ï¼ˆ>500æ–‡å­—ï¼‰: {long_answers:,}ä»¶")

    # åˆ¤ä¾‹ç•ªå·ã‚„æ¡æ–‡å‚ç…§ã®ç¢ºèªï¼ˆè©³ç´°ç‰ˆï¼‰
    if answer_col is not None:
        reference_patterns = {
            'æ¡æ–‡å‚ç…§': r'ç¬¬\d+æ¡',
            'æ°‘æ³•æ¡æ–‡': r'æ°‘æ³•ç¬¬?\d+æ¡',
            'åˆ‘æ³•æ¡æ–‡': r'åˆ‘æ³•ç¬¬?\d+æ¡',
            'æ†²æ³•æ¡æ–‡': r'æ†²æ³•ç¬¬?\d+æ¡',
            'åˆ¤ä¾‹'    : r'åˆ¤ä¾‹|æœ€åˆ¤|æ±äº¬åœ°åˆ¤|å¤§é˜ªé«˜åˆ¤',
            'å¹´æœˆæ—¥'  : r'\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥',
            'äº‹ä»¶ç•ªå·': r'(å¹³æˆ|ä»¤å’Œ)\d+å¹´'
        }

        reference_analysis = {}
        for pattern_name, pattern in reference_patterns.items():
            try:
                count = df[answer_col].str.contains(pattern, regex=True, na=False).sum()
                if count > 0:
                    percentage = (count / len(df)) * 100
                    reference_analysis[pattern_name] = f"{count:,}ä»¶ ({percentage:.1f}%)"
            except Exception:
                reference_analysis[pattern_name] = "æ¤œç´¢ã‚¨ãƒ©ãƒ¼"

        if reference_analysis:
            legal_issues.append("**æ³•çš„æ ¹æ‹ ã®å‚ç…§åˆ†æ:**")
            for ref_type, count_info in reference_analysis.items():
                legal_issues.append(f"  - {ref_type}: {count_info}")

    return legal_issues


# ==================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†é–¢æ•°
# ==================================================
def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†é–¢æ•°"""

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—ã®è¨­å®š
    DATASET_TYPE = "legal_qa"

    # ãƒšãƒ¼ã‚¸è¨­å®šï¼ˆå…±é€šé–¢æ•°åˆ©ç”¨ï¼‰
    setup_page_config(DATASET_TYPE)

    # ãƒ˜ãƒƒãƒ€ãƒ¼ï¼ˆå…±é€šé–¢æ•°åˆ©ç”¨ï¼‰
    setup_page_header(DATASET_TYPE)

    # =================================================
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼: ãƒ¢ãƒ‡ãƒ«é¸æŠæ©Ÿèƒ½ï¼ˆå…±é€šé–¢æ•°åˆ©ç”¨ï¼‰
    # =================================================
    setup_sidebar_header(DATASET_TYPE)

    # ãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆå…±é€šé–¢æ•°åˆ©ç”¨ï¼‰
    selected_model = select_model(key="legal_model_selection")

    # é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’è¡¨ç¤ºï¼ˆå…±é€šé–¢æ•°åˆ©ç”¨ï¼‰
    show_model_info(selected_model)

    st.sidebar.markdown("---")

    # å‰å‡¦ç†è¨­å®š
    st.sidebar.header("âš™ï¸ å‰å‡¦ç†è¨­å®š")
    combine_columns_option = st.sidebar.checkbox(
        "è¤‡æ•°åˆ—ã‚’çµåˆã™ã‚‹ï¼ˆVector Storeç”¨ï¼‰",
        value=True,
        help="è¤‡æ•°åˆ—ã‚’çµåˆã—ã¦RAGç”¨ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ"
    )
    show_validation = st.sidebar.checkbox(
        "ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚’è¡¨ç¤º",
        value=True,
        help="ãƒ‡ãƒ¼ã‚¿ã®å“è³ªæ¤œè¨¼çµæœã‚’è¡¨ç¤º"
    )

    # æ³•å¾‹ãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®è¨­å®š
    with st.sidebar.expander("âš–ï¸ æ³•å¾‹ãƒ‡ãƒ¼ã‚¿è¨­å®š", expanded=False):
        preserve_legal_terms = st.checkbox(
            "æ³•å¾‹ç”¨èªã‚’ä¿è­·",
            value=True,
            help="æ³•å¾‹å°‚é–€ç”¨èªã®éåº¦ãªæ­£è¦åŒ–ã‚’é˜²ã"
        )
        preserve_references = st.checkbox(
            "æ³•çš„æ ¹æ‹ ã‚’ä¿è­·",
            value=True,
            help="æ¡æ–‡ç•ªå·ã‚„åˆ¤ä¾‹ç•ªå·ãªã©ã®æ³•çš„æ ¹æ‹ ã‚’ä¿è­·"
        )
        normalize_case_names = st.checkbox(
            "åˆ¤ä¾‹åã‚’æ­£è¦åŒ–",
            value=False,
            help="åˆ¤ä¾‹åã®è¡¨è¨˜ã‚†ã‚Œã‚’çµ±ä¸€"
        )

    # =================================================
    # ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢: ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
    # =================================================

    # ç¾åœ¨ã®é¸æŠãƒ¢ãƒ‡ãƒ«æƒ…å ±è¡¨ç¤º
    with st.expander("ğŸ“Š é¸æŠä¸­ã®ãƒ¢ãƒ‡ãƒ«æƒ…å ±", expanded=False):
        col1, col2 = st.columns(2)
        with col1:
            st.info(f"ğŸ¤– é¸æŠãƒ¢ãƒ‡ãƒ«: **{selected_model}**")
        with col2:
            limits = AppConfig.get_model_limits(selected_model)
            st.info(f"ğŸ“ æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³: **{limits['max_tokens']:,}**")

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    st.subheader("ğŸ“ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    uploaded_file = st.file_uploader(
        "æ³•å¾‹ãƒ»åˆ¤ä¾‹QAãƒ‡ãƒ¼ã‚¿ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„",
        type=['csv'],
        help="question, answer ã®2åˆ—ã‚’å«ã‚€CSVãƒ•ã‚¡ã‚¤ãƒ«"
    )

    if uploaded_file is not None:
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã®ç¢ºèª
            st.info(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«: **{uploaded_file.name}** ({uploaded_file.size:,} bytes)")

            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã§ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†çŠ¶æ³ã‚’ç®¡ç†
            file_key = f"file_{uploaded_file.name}_{uploaded_file.size}"

            # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤‰æ›´ã•ã‚ŒãŸå ´åˆã¯å†èª­ã¿è¾¼ã¿ï¼ˆå…±é€šé–¢æ•°åˆ©ç”¨ï¼‰
            if st.session_state.get('current_file_key') != file_key:
                with st.spinner("ğŸ“– ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
                    df, validation_results = load_dataset(uploaded_file, DATASET_TYPE)

                # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
                st.session_state['current_file_key'] = file_key
                st.session_state['original_df'] = df
                st.session_state['validation_results'] = validation_results
                st.session_state['original_rows'] = len(df)
                st.session_state['file_processed'] = False

                logger.info(f"æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿: {len(df):,}è¡Œ")
            else:
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‹ã‚‰å–å¾—
                df = st.session_state['original_df']
                validation_results = st.session_state['validation_results']
                logger.info(f"ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—: {len(df):,}è¡Œ")

            st.success(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£å¸¸ã«èª­ã¿è¾¼ã¾ã‚Œã¾ã—ãŸã€‚è¡Œæ•°: **{len(df):,}**")

            # å…ƒãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
            st.subheader("ğŸ“‹ å…ƒãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
            st.dataframe(df.head(10), use_container_width=True)

            # ã‚«ãƒ©ãƒ æƒ…å ±ã®è¡¨ç¤º
            st.subheader("ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ§‹é€ æƒ…å ±")
            col1, col2 = st.columns(2)
            with col1:
                st.write("**æ¤œå‡ºã•ã‚ŒãŸã‚«ãƒ©ãƒ :**")
                for col in df.columns:
                    st.write(f"- {col}")
            with col2:
                st.write("**ãƒ‡ãƒ¼ã‚¿å‹:**")
                for col, dtype in df.dtypes.items():
                    st.write(f"- {col}: {dtype}")

            # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼çµæœã®è¡¨ç¤º
            if show_validation:
                st.subheader("ğŸ” ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼")

                # åŸºæœ¬æ¤œè¨¼çµæœï¼ˆå…±é€šé–¢æ•°ã®çµæœï¼‰
                col1, col2 = st.columns(2)
                with col1:
                    st.write("**åŸºæœ¬çµ±è¨ˆ:**")
                    for issue in validation_results:
                        st.info(issue)

                with col2:
                    # æ³•å¾‹ãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®æ¤œè¨¼
                    legal_issues = validate_legal_data_specific(df)
                    if legal_issues:
                        st.write("**æ³•å¾‹ãƒ»åˆ¤ä¾‹QAãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®åˆ†æ:**")
                        for issue in legal_issues:
                            if issue.startswith("**"):
                                st.write(issue)
                            elif issue.startswith("  - "):
                                st.caption(issue)
                            else:
                                st.info(issue)

            # å‰å‡¦ç†å®Ÿè¡Œ
            st.subheader("âš™ï¸ å‰å‡¦ç†å®Ÿè¡Œ")

            col1, col2 = st.columns([3, 1])
            with col1:
                st.write("å‰å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ãƒ»æ­£è¦åŒ–ãƒ»çµåˆãŒè¡Œã‚ã‚Œã¾ã™ã€‚")
            with col2:
                process_button = st.button("ğŸš€ å‰å‡¦ç†ã‚’å®Ÿè¡Œ", type="primary", key="process_button",
                                           use_container_width=True)

            if process_button:
                try:
                    with st.spinner("âš™ï¸ å‰å‡¦ç†ä¸­..."):
                        # RAGãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ï¼ˆå…±é€šé–¢æ•°åˆ©ç”¨ï¼‰
                        df_processed = process_rag_data(
                            df.copy(),
                            DATASET_TYPE,
                            combine_columns_option
                        )

                    st.success("âœ… å‰å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")

                    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                    st.session_state['processed_df'] = df_processed
                    st.session_state['file_processed'] = True

                    # å‰å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º
                    st.subheader("âœ… å‰å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
                    st.dataframe(df_processed.head(10), use_container_width=True)

                    # çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤ºï¼ˆå…±é€šé–¢æ•°åˆ©ç”¨ï¼‰
                    display_statistics(df, df_processed, DATASET_TYPE)

                    # é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§ã®ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡æ¨å®šï¼ˆå…±é€šé–¢æ•°åˆ©ç”¨ï¼‰
                    estimate_token_usage(df_processed, selected_model)

                    # æ³•å¾‹ãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®å¾Œå‡¦ç†åˆ†æ
                    if 'Combined_Text' in df_processed.columns:
                        st.subheader("âš–ï¸ æ³•å¾‹ãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®åˆ†æ")

                        col1, col2 = st.columns(2)

                        with col1:
                            # çµåˆãƒ†ã‚­ã‚¹ãƒˆã®æ³•å¾‹ç”¨èªåˆ†æ
                            combined_texts = df_processed['Combined_Text']
                            legal_keywords = ['æ³•å¾‹', 'æ¡æ–‡', 'åˆ¤ä¾‹', 'æ°‘æ³•', 'åˆ‘æ³•', 'å¥‘ç´„', 'è²¬ä»»']

                            keyword_counts = {}
                            for keyword in legal_keywords:
                                count = combined_texts.str.contains(keyword, case=False, na=False).sum()
                                keyword_counts[keyword] = count

                            if keyword_counts:
                                st.write("**æ³•å¾‹ç”¨èªã®å‡ºç¾é »åº¦:**")
                                for keyword, count in keyword_counts.items():
                                    percentage = (count / len(df_processed)) * 100
                                    st.write(f"- {keyword}: {count:,}ä»¶ ({percentage:.1f}%)")

                        with col2:
                            # è³ªå•ã®è¤‡é›‘åº¦åˆ†æ
                            question_col = None
                            for col in df_processed.columns:
                                if 'question' in col.lower():
                                    question_col = col
                                    break

                            if question_col is not None:
                                question_lengths = df_processed[question_col].str.len()
                                st.write("**è³ªå•ã®è¤‡é›‘åº¦çµ±è¨ˆ:**")
                                st.metric("å¹³å‡è³ªå•é•·", f"{question_lengths.mean():.0f}æ–‡å­—")
                                st.metric("æœ€é•·è³ªå•", f"{question_lengths.max():,}æ–‡å­—")
                                st.metric("æœ€çŸ­è³ªå•", f"{question_lengths.min():,}æ–‡å­—")

                        # æ³•çš„æ ¹æ‹ ã®è©³ç´°åˆ†æ
                        answer_col = None
                        for col in df_processed.columns:
                            if 'answer' in col.lower():
                                answer_col = col
                                break

                        if answer_col is not None:
                            st.write("**æ³•çš„æ ¹æ‹ ã®å‚ç…§åˆ†æ:**")

                            # ã‚ˆã‚Šè©³ç´°ãªæ³•çš„å‚ç…§ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ
                            detailed_patterns = {
                                'æ¡æ–‡å‚ç…§'  : r'ç¬¬\d+æ¡',
                                'æ°‘æ³•'      : r'æ°‘æ³•',
                                'åˆ‘æ³•'      : r'åˆ‘æ³•',
                                'æ†²æ³•'      : r'æ†²æ³•',
                                'åˆ¤ä¾‹'      : r'åˆ¤ä¾‹',
                                'æœ€é«˜è£åˆ¤ä¾‹': r'æœ€åˆ¤',
                                'åœ°è£åˆ¤ä¾‹'  : r'åœ°åˆ¤',
                                'é«˜è£åˆ¤ä¾‹'  : r'é«˜åˆ¤'
                            }

                            reference_stats = {}
                            total_with_references = 0

                            for pattern_name, pattern in detailed_patterns.items():
                                try:
                                    count = df_processed[answer_col].str.contains(pattern, regex=True, na=False).sum()
                                    if count > 0:
                                        percentage = (count / len(df_processed)) * 100
                                        reference_stats[pattern_name] = (count, percentage)
                                        if pattern_name == 'æ¡æ–‡å‚ç…§':  # åŸºæœ¬çš„ãªæ³•çš„æ ¹æ‹ ã¨ã—ã¦æ¡æ–‡å‚ç…§ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                                            total_with_references = count
                                except Exception:
                                    reference_stats[pattern_name] = (0, 0)

                            if reference_stats:
                                col1, col2, col3 = st.columns(3)

                                # ä¸»è¦çµ±è¨ˆã®è¡¨ç¤º
                                with col1:
                                    total_refs = sum(count for count, _ in reference_stats.values())
                                    st.metric("æ³•çš„å‚ç…§ç·æ•°", f"{total_refs:,}")

                                with col2:
                                    if total_with_references > 0:
                                        ref_ratio = (total_with_references / len(df_processed)) * 100
                                        st.metric("æ ¹æ‹ å«æœ‰ç‡", f"{ref_ratio:.1f}%")
                                    else:
                                        st.metric("æ ¹æ‹ å«æœ‰ç‡", "0%")

                                with col3:
                                    if reference_stats:
                                        avg_refs_per_answer = total_refs / len(df_processed)
                                        st.metric("å¹³å‡å‚ç…§æ•°/å›ç­”", f"{avg_refs_per_answer:.1f}")
                                    else:
                                        st.metric("å¹³å‡å‚ç…§æ•°/å›ç­”", "0")

                                # è©³ç´°ãªå‚ç…§ã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆ
                                st.write("**å‚ç…§ã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆ:**")
                                for ref_type, (count, percentage) in reference_stats.items():
                                    if count > 0:
                                        st.write(f"- {ref_type}: {count:,}ä»¶ ({percentage:.1f}%)")

                        # å›ç­”é•·ã®è©³ç´°åˆ†æ
                        if answer_col is not None:
                            st.write("**å›ç­”é•·ã®è©³ç´°åˆ†æ:**")
                            answer_lengths = df_processed[answer_col].astype(str).str.len()

                            # å›ç­”é•·ã®ã‚«ãƒ†ã‚´ãƒªåˆ†æ
                            very_short = (answer_lengths <= 50).sum()
                            short = ((answer_lengths > 50) & (answer_lengths <= 100)).sum()
                            medium = ((answer_lengths > 100) & (answer_lengths <= 300)).sum()
                            long = ((answer_lengths > 300) & (answer_lengths <= 500)).sum()
                            very_long = (answer_lengths > 500).sum()

                            col1, col2, col3, col4, col5 = st.columns(5)
                            with col1:
                                st.metric("æ¥µçŸ­", f"{very_short}")
                                st.caption("â‰¤50æ–‡å­—")
                            with col2:
                                st.metric("çŸ­", f"{short}")
                                st.caption("51-100æ–‡å­—")
                            with col3:
                                st.metric("ä¸­", f"{medium}")
                                st.caption("101-300æ–‡å­—")
                            with col4:
                                st.metric("é•·", f"{long}")
                                st.caption("301-500æ–‡å­—")
                            with col5:
                                st.metric("æ¥µé•·", f"{very_long}")
                                st.caption(">500æ–‡å­—")

                    logger.info(f"æ³•å¾‹ãƒ»åˆ¤ä¾‹QAãƒ‡ãƒ¼ã‚¿å‡¦ç†å®Œäº†: {len(df):,} â†’ {len(df_processed):,}è¡Œ")

                except Exception as process_error:
                    st.error(f"âŒ å‰å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(process_error)}")
                    logger.error(f"å‰å‡¦ç†ã‚¨ãƒ©ãƒ¼: {process_error}")
                    with st.expander("ğŸ”§ è©³ç´°ã‚¨ãƒ©ãƒ¼æƒ…å ±", expanded=False):
                        st.code(str(process_error))

            # å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®ã¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ä¿å­˜ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¡¨ç¤º
            if st.session_state.get('file_processed', False) and 'processed_df' in st.session_state:
                df_processed = st.session_state['processed_df']

                # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ä¿å­˜ã‚»ã‚¯ã‚·ãƒ§ãƒ³
                st.subheader("ğŸ’¾ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ä¿å­˜")

                # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰ï¼ˆå…±é€šé–¢æ•°åˆ©ç”¨ï¼‰
                if 'download_data' not in st.session_state or st.session_state.get('download_data_key') != file_key:
                    with st.spinner("ğŸ“¦ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ä¸­..."):
                        csv_data, text_data = create_download_data(
                            df_processed,
                            combine_columns_option,
                            DATASET_TYPE
                        )
                        st.session_state['download_data'] = (csv_data, text_data)
                        st.session_state['download_data_key'] = file_key
                else:
                    csv_data, text_data = st.session_state['download_data']

                # ãƒ–ãƒ©ã‚¦ã‚¶ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                st.write("**ğŸ“¥ ãƒ–ãƒ©ã‚¦ã‚¶ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰**")
                col1, col2 = st.columns(2)

                with col1:
                    st.download_button(
                        label="ğŸ“Š CSVå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=csv_data,
                        file_name=f"preprocessed_{DATASET_TYPE}_{len(df_processed)}rows.csv",
                        mime="text/csv",
                        help="å‰å‡¦ç†æ¸ˆã¿ã®æ³•å¾‹ãƒ»åˆ¤ä¾‹QAãƒ‡ãƒ¼ã‚¿ã‚’CSVå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        use_container_width=True
                    )

                with col2:
                    if text_data:
                        st.download_button(
                            label="ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                            data=text_data,
                            file_name=f"legal_qa.txt",
                            mime="text/plain",
                            help="Vector Store/RAGç”¨ã«æœ€é©åŒ–ã•ã‚ŒãŸçµåˆãƒ†ã‚­ã‚¹ãƒˆ",
                            use_container_width=True
                        )
                    else:
                        st.info("çµåˆãƒ†ã‚­ã‚¹ãƒˆãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")

                # ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜ï¼ˆå…±é€šé–¢æ•°åˆ©ç”¨ï¼‰
                st.write("**ğŸ’¾ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ï¼ˆOUTPUTãƒ•ã‚©ãƒ«ãƒ€ï¼‰**")

                col1, col2 = st.columns([3, 1])
                with col1:
                    st.write("å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ OUTPUTãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã—ã¾ã™ã€‚")
                with col2:
                    save_button = st.button("ğŸ”„ OUTPUTãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜", type="secondary", key="save_button",
                                            use_container_width=True)

                if save_button:
                    try:
                        with st.spinner("ğŸ’¾ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ä¸­..."):
                            saved_files = save_files_to_output(
                                df_processed,
                                DATASET_TYPE,
                                csv_data,
                                text_data
                            )

                        if saved_files:
                            st.success("âœ… ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†ï¼")

                            # ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º
                            with st.expander("ğŸ“‚ ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§", expanded=True):
                                for file_type, file_path in saved_files.items():
                                    if Path(file_path).exists():
                                        file_size = Path(file_path).stat().st_size
                                        st.success(f"**{file_type.upper()}**: `{file_path}` ({file_size:,} bytes)")
                                    else:
                                        st.error(f"**{file_type.upper()}**: `{file_path}` âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

                                # OUTPUTãƒ•ã‚©ãƒ«ãƒ€ã®å ´æ‰€ã‚’è¡¨ç¤º
                                output_path = Path("OUTPUT").resolve()
                                st.info(f"**ä¿å­˜å ´æ‰€**: `{output_path}`")
                                try:
                                    file_count = len(list(output_path.glob("*")))
                                    st.info(f"**ãƒ•ã‚©ãƒ«ãƒ€å†…ãƒ•ã‚¡ã‚¤ãƒ«æ•°**: {file_count:,}å€‹")
                                except:
                                    st.info("ãƒ•ã‚©ãƒ«ãƒ€æƒ…å ±å–å¾—ä¸­...")
                        else:
                            st.error("âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")

                    except Exception as save_error:
                        st.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(save_error)}")
                        logger.error(f"ä¿å­˜ã‚¨ãƒ©ãƒ¼: {save_error}")

        except Exception as e:
            st.error(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

            with st.expander("ğŸ”§ è©³ç´°ã‚¨ãƒ©ãƒ¼æƒ…å ±", expanded=False):
                st.code(str(e))

                # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã®è©³ç´°ç¢ºèª
                if uploaded_file is not None:
                    st.write("**ãƒ•ã‚¡ã‚¤ãƒ«è¨ºæ–­:**")
                    st.write(f"- ãƒ•ã‚¡ã‚¤ãƒ«å: {uploaded_file.name}")
                    st.write(f"- ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {uploaded_file.size:,} bytes")
                    st.write(f"- ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—: {uploaded_file.type}")

    else:
        st.info("ğŸ‘† CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")

        # ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®èª¬æ˜
        with st.expander("ğŸ“„ å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼", expanded=False):
            st.write("**CSVãƒ•ã‚¡ã‚¤ãƒ«ã®è¦ä»¶:**")
            st.write("- ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: UTF-8")
            st.write("- å¿…é ˆåˆ—: question, answer")
            st.write("- ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: .csv")

            st.write("**ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä¾‹:**")
            sample_data = {
                "question": [
                    "å¥‘ç´„é•åãŒã‚ã£ãŸå ´åˆã®æå®³è³ å„Ÿã¯ã©ã†ãªã‚Šã¾ã™ã‹ï¼Ÿ",
                    "æ°‘æ³•ç¬¬415æ¡ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
                    "åˆ‘äº‹äº‹ä»¶ã¨æ°‘äº‹äº‹ä»¶ã®é•ã„ã¯ä½•ã§ã™ã‹ï¼Ÿ"
                ],
                "answer"  : [
                    "å¥‘ç´„é•åã«ã‚ˆã‚‹æå®³è³ å„Ÿã¯æ°‘æ³•ç¬¬415æ¡ã«åŸºã¥ãã€å‚µå‹™ä¸å±¥è¡Œã«ã‚ˆã‚Šç”Ÿã˜ãŸæå®³ã®è³ å„Ÿã‚’æ±‚ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™...",
                    "æ°‘æ³•ç¬¬415æ¡ã¯å‚µå‹™ä¸å±¥è¡Œã«ã‚ˆã‚‹æå®³è³ å„Ÿã«ã¤ã„ã¦å®šã‚ãŸæ¡æ–‡ã§ã€å‚µå‹™è€…ãŒãã®å‚µå‹™ã®æœ¬æ—¨ã«å¾“ã£ãŸå±¥è¡Œã‚’ã—ãªã„ã¨ã...",
                    "åˆ‘äº‹äº‹ä»¶ã¯å›½å®¶ãŒçŠ¯ç½ªè€…ã‚’å‡¦ç½°ã™ã‚‹æ‰‹ç¶šãã§ã€æ°‘äº‹äº‹ä»¶ã¯ç§äººé–“ã®æ¨©åˆ©ç¾©å‹™é–¢ä¿‚ã‚’è§£æ±ºã™ã‚‹æ‰‹ç¶šãã§ã™..."
                ]
            }
            sample_df = pd.DataFrame(sample_data)
            st.dataframe(sample_df, use_container_width=True)

    # ä½¿ç”¨æ–¹æ³•ã®èª¬æ˜ï¼ˆå…±é€šé–¢æ•°åˆ©ç”¨ï¼‰
    show_usage_instructions(DATASET_TYPE)

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
    if st.sidebar.checkbox("ğŸ”§ ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’è¡¨ç¤º", value=False):
        with st.sidebar.expander("ãƒ‡ãƒãƒƒã‚°æƒ…å ±", expanded=False):
            st.write(f"**é¸æŠãƒ¢ãƒ‡ãƒ«**: {selected_model}")
            st.write(f"**ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†æ¸ˆã¿**: {st.session_state.get('file_processed', False)}")

            if 'original_df' in st.session_state:
                df = st.session_state['original_df']
                st.write(f"**å…ƒãƒ‡ãƒ¼ã‚¿**: {len(df):,}è¡Œ")

            if 'processed_df' in st.session_state:
                df_processed = st.session_state['processed_df']
                st.write(f"**å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿**: {len(df_processed):,}è¡Œ")

            # OUTPUTãƒ•ã‚©ãƒ«ãƒ€ã®çŠ¶æ…‹
            try:
                output_dir = Path("OUTPUT")
                if output_dir.exists():
                    file_count = len(list(output_dir.glob(f"*{DATASET_TYPE}*")))
                    st.write(f"**ä¿å­˜æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«**: {file_count:,}å€‹")
                else:
                    st.write("**OUTPUTãƒ•ã‚©ãƒ«ãƒ€**: æœªä½œæˆ")
            except Exception as e:
                st.write(f"**ãƒ•ã‚©ãƒ«ãƒ€çŠ¶æ…‹**: ã‚¨ãƒ©ãƒ¼ ({e})")


# ==================================================
# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
# ==================================================
if __name__ == "__main__":
    main()

# å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰:
# streamlit run a30_015_make_rag_data_legal.py --server.port=8505
