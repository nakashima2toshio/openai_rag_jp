a10_00_responses_api.pya10_00_responses_api.py# streamlit run a10_00_responses_api.py --server.port=8510
# --------------------------------------------------
# OpenAI Responses API ãƒ‡ãƒ¢ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰
# Streamlitã‚’ä½¿ç”¨ã—ãŸã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªAPIãƒ†ã‚¹ãƒˆãƒ„ãƒ¼ãƒ«
# çµ±ä¸€åŒ–ç‰ˆ: æ§‹æˆãƒ»æ§‹é€ ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒ»ã‚¨ãƒ©ãƒ¼å‡¦ç†ã®å®Œå…¨çµ±ä¸€
# --------------------------------------------------
import os
import sys
import json
import base64
import glob
import logging
from datetime import datetime
import time
from abc import ABC, abstractmethod
from typing import Optional, List, Dict, Any, Literal
from pathlib import Path

import streamlit as st
import pandas as pd
import requests
from pydantic import BaseModel, ValidationError

from openai import OpenAI
from openai.types.responses import (
    EasyInputMessageParam,
    ResponseInputTextParam,
    ResponseInputImageParam,
    ResponseFormatTextJSONSchemaConfigParam,
    ResponseTextConfigParam,
    FileSearchToolParam,
    WebSearchToolParam,
    ComputerToolParam,
    Response,  # â† ã“ã®è¡Œã‚’è¿½åŠ 
)
from openai.types.responses.web_search_tool_param import UserLocation

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š
BASE_DIR = Path(__file__).resolve().parent.parent
THIS_DIR = Path(__file__).resolve().parent

# PYTHONPATHã«è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¿½åŠ 
sys.path.insert(0, str(BASE_DIR))

# ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆçµ±ä¸€åŒ–ï¼‰
try:
    from helper_st import (
        UIHelper, MessageManagerUI, ResponseProcessorUI,
        SessionStateManager, error_handler_ui, timer_ui,
        InfoPanelManager, safe_streamlit_json
    )
    from helper_api import (
        config, logger, TokenManager, OpenAIClient,
        EasyInputMessageParam, ResponseInputTextParam,
        ConfigManager, MessageManager, sanitize_key,
        error_handler, timer, get_default_messages,
        ResponseProcessor, format_timestamp
    )
except ImportError as e:
    st.error(f"ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    st.info("å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„: helper_st.py, helper_api.py")
    st.stop()


# ãƒšãƒ¼ã‚¸è¨­å®š
def setup_page_config():
    """ãƒšãƒ¼ã‚¸è¨­å®šï¼ˆé‡è¤‡å®Ÿè¡Œã‚¨ãƒ©ãƒ¼å›é¿ï¼‰"""
    try:
        st.set_page_config(
            page_title=config.get("ui.page_title", "OpenAI Responses API ãƒ‡ãƒ¢"),
            page_icon=config.get("ui.page_icon", "ğŸ¤–"),
            layout=config.get("ui.layout", "wide"),
            initial_sidebar_state="expanded"
        )
    except st.errors.StreamlitAPIException:
        # æ—¢ã«è¨­å®šæ¸ˆã¿ã®å ´åˆã¯ç„¡è¦–
        pass


# ãƒšãƒ¼ã‚¸è¨­å®šã®å®Ÿè¡Œ
setup_page_config()

# ã‚µãƒ³ãƒ—ãƒ«ç”»åƒ URLï¼ˆconfig.ymlã‹ã‚‰å–å¾—ï¼‰
image_path_sample = config.get(
    "samples.images.nature",
    "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/"
    "Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-"
    "Gfp-wisconsin-madison-the-nature-boardwalk.jpg"
)

# https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg

# ==================================================
# å…±é€šUIé–¢æ•°ï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰
# ==================================================
def setup_common_ui(demo_name: str) -> str:
    """å…±é€šUIè¨­å®šï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰"""
    safe_key = sanitize_key(demo_name)

    # ãƒ˜ãƒƒãƒ€ãƒ¼è¡¨ç¤º
    st.write(f"# {demo_name}")

    # ãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆçµ±ä¸€ã•ã‚ŒãŸUIï¼‰
    model = UIHelper.select_model(f"model_{safe_key}")
    st.write("é¸æŠã—ãŸãƒ¢ãƒ‡ãƒ«:", model)

    return model


def setup_sidebar_panels(selected_model: str):
    """ã‚µã‚¤ãƒ‰ãƒãƒ¼ãƒ‘ãƒãƒ«ã®çµ±ä¸€è¨­å®šï¼ˆhelper_st.pyã®InfoPanelManagerã‚’ä½¿ç”¨ï¼‰"""
    st.sidebar.write("### ğŸ“‹ æƒ…å ±ãƒ‘ãƒãƒ«")

    InfoPanelManager.show_model_info(selected_model)
    InfoPanelManager.show_session_info()
    InfoPanelManager.show_performance_info()
    InfoPanelManager.show_cost_info(selected_model)
    InfoPanelManager.show_debug_panel()
    InfoPanelManager.show_settings()


# ==================================================
# åŸºåº•ã‚¯ãƒ©ã‚¹
# ==================================================
class BaseDemo(ABC):
    """ãƒ‡ãƒ¢æ©Ÿèƒ½ã®åŸºåº•ã‚¯ãƒ©ã‚¹ï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰"""

    def __init__(self, demo_name: str):
        self.demo_name = demo_name
        self.config = ConfigManager("config.yml")

        # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ï¼ˆçµ±ä¸€ã•ã‚ŒãŸã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ï¼‰
        try:
            self.client = OpenAIClient()
        except Exception as e:
            st.error(f"OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            st.stop()

        self.safe_key = sanitize_key(demo_name)
        self.message_manager = MessageManagerUI(f"messages_{self.safe_key}")
        self.model = None

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–ï¼ˆçµ±ä¸€åŒ–ï¼‰
        SessionStateManager.init_session_state()
        self._initialize_session_state()

    def _initialize_session_state(self):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®çµ±ä¸€çš„åˆæœŸåŒ–"""
        session_key = f"demo_state_{self.safe_key}"
        if session_key not in st.session_state:
            st.session_state[session_key] = {
                'initialized'    : True,
                'model'          : self.config.get("models.default", "gpt-4o-mini"),
                'execution_count': 0
            }

    def get_model(self) -> str:
        """é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—ï¼ˆçµ±ä¸€åŒ–ï¼‰"""
        return st.session_state.get(f"model_{self.safe_key}",
                                    config.get("models.default", "gpt-4o-mini"))

    def is_reasoning_model(self, model: str = None) -> bool:
        """æ¨è«–ç³»ãƒ¢ãƒ‡ãƒ«ã‹ã©ã†ã‹ã‚’åˆ¤å®šï¼ˆçµ±ä¸€åŒ–ï¼‰"""
        if model is None:
            model = self.get_model()

        # config.ymlã‹ã‚‰å–å¾—ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚ã‚Š
        reasoning_models = config.get("models.categories.reasoning",
                                      ["o1", "o1-mini", "o3", "o3-mini", "o4", "o4-mini"])
        
        # GPT-5ç³»ãƒ¢ãƒ‡ãƒ«ã‚‚æ¨è«–ç³»ã¨ã—ã¦æ‰±ã†ï¼ˆtemperatureã‚µãƒãƒ¼ãƒˆãªã—ï¼‰
        frontier_models = config.get("models.categories.frontier",
                                    ["gpt-5", "gpt-5-mini", "gpt-5-nano"])

        # ãƒ¢ãƒ‡ãƒ«åã«æ¨è«–ç³»ãƒ¢ãƒ‡ãƒ«ã®è­˜åˆ¥å­ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        reasoning_indicators = ["o1", "o3", "o4", "gpt-5"]
        return any(indicator in model.lower() for indicator in reasoning_indicators) or \
            any(reasoning_model in model for reasoning_model in reasoning_models) or \
            any(frontier_model in model for frontier_model in frontier_models)

    def create_temperature_control(self, default_temp: float = 0.3, help_text: str = None) -> Optional[float]:
        """Temperatureã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã‚’ä½œæˆï¼ˆçµ±ä¸€åŒ–ãƒ»æ¨è«–ç³»ãƒ¢ãƒ‡ãƒ«ãƒ»GPT-5ç³»ã§ã¯ç„¡åŠ¹åŒ–ï¼‰"""
        model = self.get_model()

        if self.is_reasoning_model(model):
            st.info("â„¹ï¸ æ¨è«–ç³»ãƒ¢ãƒ‡ãƒ«ï¼ˆo1, o3, o4, gpt-5ç³»ï¼‰ã§ã¯temperatureãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ä½¿ç”¨ã•ã‚Œã¾ã›ã‚“")
            return None
        else:
            return st.slider(
                "Temperature",
                0.0, 1.0, default_temp, 0.05,
                help=help_text or "ä½ã„å€¤ã»ã©ä¸€è²«æ€§ã®ã‚ã‚‹å›ç­”"
            )

    def initialize(self):
        """å…±é€šã®åˆæœŸåŒ–å‡¦ç†ï¼ˆçµ±ä¸€åŒ–ï¼‰"""
        self.model = setup_common_ui(self.demo_name)
        setup_sidebar_panels(self.model)

    def handle_error(self, e: Exception):
        """çµ±ä¸€çš„ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°"""
        # å¤šè¨€èªå¯¾å¿œã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        lang = config.get("i18n.default_language", "ja")
        error_msg = config.get(f"error_messages.{lang}.general_error", "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
        st.error(f"{error_msg}: {str(e)}")

        if config.get("experimental.debug_mode", False):
            with st.expander("ğŸ”§ è©³ç´°ã‚¨ãƒ©ãƒ¼æƒ…å ±", expanded=False):
                st.exception(e)

    def show_debug_info(self):
        """ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®çµ±ä¸€è¡¨ç¤º"""
        if st.sidebar.checkbox("ğŸ”§ ãƒ‡ãƒ¢çŠ¶æ…‹ã‚’è¡¨ç¤º", value=False, key=f"debug_{self.safe_key}"):
            with st.sidebar.expander("ãƒ‡ãƒ¢ãƒ‡ãƒãƒƒã‚°æƒ…å ±", expanded=False):
                st.write(f"**ãƒ‡ãƒ¢å**: {self.demo_name}")
                st.write(f"**é¸æŠãƒ¢ãƒ‡ãƒ«**: {self.model}")

                session_state = st.session_state.get(f"demo_state_{self.safe_key}", {})
                st.write(f"**å®Ÿè¡Œå›æ•°**: {session_state.get('execution_count', 0)}")

    @error_handler_ui
    @timer_ui
    def call_api_unified(self, messages: List[EasyInputMessageParam], temperature: Optional[float] = None, **kwargs):
        """çµ±ä¸€ã•ã‚ŒãŸAPIå‘¼ã³å‡ºã—ï¼ˆtemperatureãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¯¾å¿œï¼‰"""
        model = self.get_model()

        # APIå‘¼ã³å‡ºã—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æº–å‚™
        api_params = {
            "input": messages,
            "model": model
        }

        # temperatureã‚µãƒãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯ï¼ˆreasoningç³»ãƒ¢ãƒ‡ãƒ«ã¯é™¤å¤–ï¼‰
        if not self.is_reasoning_model(model) and temperature is not None:
            api_params["temperature"] = temperature

        # ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        api_params.update(kwargs)

        # responses.create ã‚’ä½¿ç”¨ï¼ˆçµ±ä¸€ã•ã‚ŒãŸAPIå‘¼ã³å‡ºã—ï¼‰
        return self.client.create_response(**api_params)

    @abstractmethod
    def run(self):
        """å„ãƒ‡ãƒ¢ã®å®Ÿè¡Œå‡¦ç†ï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰"""
        pass


# ==================================================
# ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”ãƒ‡ãƒ¢
# ==================================================
class TextResponseDemo(BaseDemo):
    """åŸºæœ¬çš„ãªãƒ†ã‚­ã‚¹ãƒˆå¿œç­”ã®ãƒ‡ãƒ¢ï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰"""

    @error_handler_ui
    @timer_ui
    def run(self):
        """ãƒ‡ãƒ¢ã®å®Ÿè¡Œï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰"""
        self.initialize()
        with st.expander("OpenAI API(IPO):å®Ÿè£…ä¾‹", expanded=False):
            st.write(
                "responses.create()ã®åŸºæœ¬çš„ãªãƒ†ã‚­ã‚¹ãƒˆå¿œç­”ãƒ‡ãƒ¢ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸+ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã§One-Shotå¿œç­”ã‚’å®Ÿè¡Œã€‚EasyInputMessageParamã§ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ§‹ç¯‰ã—ã€ResponseProcessorUIã§çµæœè¡¨ç¤ºã€‚")
            st.code("""
            messages = get_default_messages()
            messages.append(
                EasyInputMessageParam(role="user", content=user_input)
            )

        # çµ±ä¸€ã•ã‚ŒãŸAPIå‘¼ã³å‡ºã—ï¼ˆtemperatureãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¯¾å¿œï¼‰
        response = self.call_api_unified(messages, temperature=temperature)
        ã€€â”— api_params = {
            "input": messages,
            "model": model
            }
            self.client.responses.create(**params)
        ResponseProcessorUI.display_response(response)
            """)

        example_query = config.get("samples.prompts.responses_query",
                                   "OpenAIã®APIã§ã€responses.createã‚’èª¬æ˜ã—ãªã•ã„ã€‚")
        st.write(f"è³ªå•ã®ä¾‹: {example_query}")

        with st.form(key=f"text_form_{self.safe_key}"):
            user_input = st.text_area(
                "è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:",
                height=config.get("ui.text_area_height", 75)
            )

            # çµ±ä¸€ã•ã‚ŒãŸtemperatureã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«
            temperature = self.create_temperature_control(
                default_temp=0.3,
                help_text="ä½ã„å€¤ã»ã©ä¸€è²«æ€§ã®ã‚ã‚‹å›ç­”"
            )

            submitted = st.form_submit_button("é€ä¿¡")

        if submitted and user_input:
            self._process_query(user_input, temperature)

        self.show_debug_info()

    def _process_query(self, user_input: str, temperature: Optional[float]):
        """ã‚¯ã‚¨ãƒªã®å‡¦ç†ï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰"""
        # å®Ÿè¡Œå›æ•°ã‚’æ›´æ–°
        session_key = f"demo_state_{self.safe_key}"
        if session_key in st.session_state:
            st.session_state[session_key]['execution_count'] += 1

        # ãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±ã®è¡¨ç¤º
        UIHelper.show_token_info(user_input, self.model, position="sidebar")

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—ï¼ˆconfig.ymlã‹ã‚‰ï¼‰
        messages = get_default_messages()
        messages.append(
            EasyInputMessageParam(role="user", content=user_input)
        )

        with st.spinner("å‡¦ç†ä¸­..."):
            response = self.call_api_unified(messages, temperature=temperature)

        st.success("å¿œç­”ã‚’å–å¾—ã—ã¾ã—ãŸ")
        ResponseProcessorUI.display_response(response)


# ==================================================
# å¿…è¦ãªã‚¤ãƒ³ãƒãƒ¼ãƒˆã®è¿½åŠ ï¼ˆã‚¨ãƒ©ãƒ¼ä¿®æ­£ï¼‰
# ==================================================
import pandas as pd
from openai.types.responses import Response

# ==================================================
# ãƒ¡ãƒ¢ãƒªå¿œç­”ãƒ‡ãƒ¢ï¼ˆæ”¹ä¿®ç‰ˆãƒ»ã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‰ˆï¼‰- é€£ç¶šä¼šè©±å¯¾å¿œ
# ==================================================
class MemoryResponseDemo(BaseDemo):
    """é€£ç¶šä¼šè©±ã‚’ç®¡ç†ã™ã‚‹ãƒ‡ãƒ¢ï¼ˆæ”¹ä¿®ç‰ˆãƒ»ã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‰ˆï¼‰"""

    def __init__(self, demo_name: str):
        super().__init__(demo_name)
        # ä¼šè©±ã‚¹ãƒ†ãƒƒãƒ—ã®ç®¡ç†
        self.conversation_steps = []
        self._initialize_conversation_state()

    def _initialize_conversation_state(self):
        """ä¼šè©±çŠ¶æ…‹ã®åˆæœŸåŒ–"""
        session_key = f"conversation_steps_{self.safe_key}"
        if session_key not in st.session_state:
            st.session_state[session_key] = []

        self.conversation_steps = st.session_state[session_key]

    @error_handler_ui
    @timer_ui
    def run(self):
        """ãƒ‡ãƒ¢ã®å®Ÿè¡Œï¼ˆæ”¹ä¿®ç‰ˆï¼‰"""
        self.initialize()
        st.write(
            "**é€£ç¶šä¼šè©±ãƒ‡ãƒ¢ï¼ˆæ”¹ä¿®ç‰ˆï¼‰**\n"
            "responses.create()ã§é€£ç¶šã—ãŸä¼šè©±ã‚’å®Ÿç¾ã€‚å„ã‚¹ãƒ†ãƒƒãƒ—ã§ã€Œãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + å›ç­”ã€ã®å±¥æ­´ã‚’ä¿æŒã—ã€"
            "æ–°ã—ã„è³ªå•ã‚’è¿½åŠ ã—ã¦é€£ç¶šå®Ÿè¡Œã—ã¾ã™ã€‚ä¼šè©±ã®æµã‚Œã¨å„ã‚¹ãƒ†ãƒƒãƒ—ãŒè¦–è¦šçš„ã«ç¢ºèªã§ãã¾ã™ã€‚"
        )

        with st.expander("OpenAI API(IPO):å®Ÿè£…ä¾‹", expanded=False):
            st.code("""
            # 1å›ç›®: åˆå›è³ªå•
            messages = get_default_messages()
            messages.append(EasyInputMessageParam(role="user", content=user_input_1))
            response_1 = self.call_api_unified(messages, temperature=temperature)
              â”— api_params = {
                "input": messages,
                "model": model
                }
                self.client.responses.create(**params)
    
            # 2å›ç›®ä»¥é™: å±¥æ­´ + æ–°ã—ã„è³ªå•
            messages.append(EasyInputMessageParam(role="assistant", content=response_1_text))
            messages.append(EasyInputMessageParam(role="user", content=user_input_2))
            response_2 = self.call_api_unified(messages, temperature=temperature)
    
            # é€£ç¶šå®Ÿè¡Œ...
            """)

        # ä¼šè©±å±¥æ­´ã®è¡¨ç¤º
        self._display_conversation_history()

        # æ–°ã—ã„è³ªå•ã®å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
        self._create_input_form()

        # ä¼šè©±ç®¡ç†ãƒœã‚¿ãƒ³
        self._create_conversation_controls()

        self.show_debug_info()

    def _display_conversation_history(self):
        """ä¼šè©±å±¥æ­´ã®è¡¨ç¤º"""
        if not self.conversation_steps:
            st.info("ğŸ’¬ ä¼šè©±ã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚è³ªå•ã‚’å…¥åŠ›ã™ã‚‹ã¨ä¼šè©±å±¥æ­´ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")
            return

        st.subheader("ğŸ“ ä¼šè©±å±¥æ­´")

        # ä¼šè©±çµ±è¨ˆã®è¡¨ç¤º
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ä¼šè©±ã‚¹ãƒ†ãƒƒãƒ—æ•°", len(self.conversation_steps))
        with col2:
            total_tokens = sum(step.get('total_tokens', 0) for step in self.conversation_steps)
            st.metric("ç´¯è¨ˆãƒˆãƒ¼ã‚¯ãƒ³æ•°", f"{total_tokens:,}")
        with col3:
            if self.conversation_steps:
                latest_step = self.conversation_steps[-1]
                latest_time = latest_step.get('timestamp', 'N/A')
                st.metric("æœ€æ–°è³ªå•æ™‚åˆ»", latest_time[-8:] if len(latest_time) > 8 else latest_time)  # æ™‚åˆ»éƒ¨åˆ†ã®ã¿è¡¨ç¤º

        # å„ä¼šè©±ã‚¹ãƒ†ãƒƒãƒ—ã®è¡¨ç¤º
        for i, step in enumerate(self.conversation_steps, 1):
            with st.expander(
                    f"ğŸ”„ ã‚¹ãƒ†ãƒƒãƒ— {i}: {step['user_input'][:50]}{'...' if len(step['user_input']) > 50 else ''}",
                    expanded=(i == len(self.conversation_steps))):

                # ã‚¹ãƒ†ãƒƒãƒ—è©³ç´°æƒ…å ±
                col1, col2 = st.columns([3, 1])
                with col1:
                    st.write(f"**è³ªå•æ™‚åˆ»**: {step.get('timestamp', 'N/A')}")
                    st.write(f"**ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«**: {step.get('model', 'N/A')}")
                with col2:
                    if 'usage' in step and step['usage']:
                        usage = step['usage']
                        st.write(f"**ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨**")
                        st.write(f"å…¥åŠ›: {usage.get('prompt_tokens', 0)}")
                        st.write(f"å‡ºåŠ›: {usage.get('completion_tokens', 0)}")
                        st.write(f"åˆè¨ˆ: {usage.get('total_tokens', 0)}")

                # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
                st.write("**ğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•:**")
                st.markdown(f"> {step['user_input']}")

                # AIã®å›ç­”
                st.write("**ğŸ¤– AIã®å›ç­”:**")
                with st.chat_message("assistant", avatar="ğŸ¤–"):
                    st.markdown(step['assistant_response'])

                # ã“ã®æ™‚ç‚¹ã§ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´
                if st.checkbox(f"ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã‚’è¡¨ç¤º (ã‚¹ãƒ†ãƒƒãƒ— {i})", key=f"show_messages_{i}_{self.safe_key}"):
                    st.write("**ğŸ“‹ ã“ã®æ™‚ç‚¹ã§ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´:**")
                    messages = step.get('messages_at_step', [])
                    for j, msg in enumerate(messages):
                        role = msg.get('role', 'unknown')
                        content = msg.get('content', '')
                        content_preview = content[:100] + '...' if len(content) > 100 else content
                        st.write(f"{j + 1}. **{role}**: {content_preview}")

    def _create_input_form(self):
        """å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ ã®ä½œæˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ ä¸ä½¿ç”¨ç‰ˆï¼‰"""
        st.subheader("ğŸ’­ æ–°ã—ã„è³ªå•")

        # ç¾åœ¨ã®ä¼šè©±ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±
        if self.conversation_steps:
            st.info(
                f"â„¹ï¸ ç¾åœ¨ {len(self.conversation_steps)} ã‚¹ãƒ†ãƒƒãƒ—ã®ä¼šè©±å±¥æ­´ãŒã‚ã‚Šã¾ã™ã€‚æ–°ã—ã„è³ªå•ã¯ã“ã®å±¥æ­´ã‚’è¸ã¾ãˆã¦å›ç­”ã•ã‚Œã¾ã™ã€‚")
        else:
            st.info("â„¹ï¸ æœ€åˆã®è³ªå•ã§ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨å…±ã«é€ä¿¡ã•ã‚Œã¾ã™ã€‚")

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’ç®¡ç†
        input_key = f"user_input_{self.safe_key}"
        temp_key = f"temperature_{self.safe_key}"

        # åˆæœŸåŒ–
        if input_key not in st.session_state:
            st.session_state[input_key] = ""
        if temp_key not in st.session_state:
            st.session_state[temp_key] = 0.3

        # è³ªå•ä¾‹ã®è¡¨ç¤º
        example_questions = self._get_example_questions()
        if example_questions:
            st.write("**è³ªå•ä¾‹:** ï¼ˆã‚¯ãƒªãƒƒã‚¯ã§å…¥åŠ›æ¬„ã«è¨­å®šï¼‰")
            col1, col2, col3 = st.columns(3)

            with col1:
                if st.button(f"ğŸ“ {example_questions[0][:30]}...", key=f"example_0_{self.safe_key}",
                             help=example_questions[0]):
                    st.session_state[input_key] = example_questions[0]
                    st.rerun()
            with col2:
                if len(example_questions) > 1:
                    if st.button(f"ğŸ“ {example_questions[1][:30]}...", key=f"example_1_{self.safe_key}",
                                 help=example_questions[1]):
                        st.session_state[input_key] = example_questions[1]
                        st.rerun()
            with col3:
                if len(example_questions) > 2:
                    if st.button(f"ğŸ“ {example_questions[2][:30]}...", key=f"example_2_{self.safe_key}",
                                 help=example_questions[2]):
                        st.session_state[input_key] = example_questions[2]
                        st.rerun()

        # å…¥åŠ›ã‚¨ãƒªã‚¢ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã¨é€£å‹•ï¼‰
        user_input = st.text_area(
            "è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:",
            value=st.session_state[input_key],
            height=config.get("ui.text_area_height", 75),
            key=f"text_area_{self.safe_key}",
            placeholder="å‰å›ã®ä¼šè©±ã‚’è¸ã¾ãˆãŸè³ªå•ã‚’ã—ã¦ãã ã•ã„...",
            on_change=self._on_text_change
        )

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®åŒæœŸ
        st.session_state[input_key] = user_input

        # Temperatureè¨­å®š
        col1, col2 = st.columns([2, 1])
        with col1:
            if not self.is_reasoning_model(self.model):
                temperature = st.slider(
                    "Temperature",
                    0.0, 1.0, st.session_state[temp_key], 0.05,
                    help="ä½ã„å€¤ã»ã©ä¸€è²«æ€§ã®ã‚ã‚‹å›ç­”",
                    key=f"temp_slider_{self.safe_key}"
                )
                st.session_state[temp_key] = temperature
            else:
                st.info("â„¹ï¸ æ¨è«–ç³»ãƒ¢ãƒ‡ãƒ«ï¼ˆo1, o3, o4ç³»ï¼‰ã§ã¯temperatureãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ä½¿ç”¨ã•ã‚Œã¾ã›ã‚“")
                temperature = None

        # ãƒœã‚¿ãƒ³ç¾¤
        with col2:
            col2_1, col2_2, col2_3 = st.columns(3)

            with col2_1:
                submitted = st.button(
                    "ğŸš€ é€ä¿¡",
                    key=f"submit_{self.safe_key}",
                    use_container_width=True,
                    type="primary"
                )

            with col2_2:
                clear_clicked = st.button(
                    "ğŸ”„ ã‚¯ãƒªã‚¢",
                    key=f"clear_{self.safe_key}",
                    use_container_width=True
                )

            with col2_3:
                st.write("")  # ã‚¹ãƒšãƒ¼ã‚¹èª¿æ•´

        # ãƒœã‚¿ãƒ³å‡¦ç†
        if clear_clicked:
            st.session_state[input_key] = ""
            st.rerun()

        if submitted and user_input.strip():
            self._process_conversation_step(user_input, temperature)
        elif submitted and not user_input.strip():
            st.warning("âš ï¸ è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    def _on_text_change(self):
        """ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢ã®å¤‰æ›´æ™‚ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        # ã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¯å¿…è¦ã«å¿œã˜ã¦å‡¦ç†ã‚’è¿½åŠ 
        pass

    def _get_example_questions(self):
        """ä¼šè©±ã‚¹ãƒ†ãƒƒãƒ—ã«å¿œã˜ãŸè³ªå•ä¾‹ã‚’å–å¾—"""
        if not self.conversation_steps:
            # åˆå›è³ªå•ã®ä¾‹
            return [
                "Python ã§ã‚¦ã‚§ãƒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®åŸºæœ¬çš„ãªæ–¹æ³•ã‚’æ•™ãˆã¦ãã ã•ã„",
                "æ©Ÿæ¢°å­¦ç¿’ã®æ•™å¸«ã‚ã‚Šå­¦ç¿’ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„",
                "REST APIã¨ã¯ä½•ã‹ã€åŸºæœ¬çš„ãªæ¦‚å¿µã‚’æ•™ãˆã¦ãã ã•ã„"
            ]
        else:
            # ç¶™ç¶šè³ªå•ã®ä¾‹
            return [
                "ã‚‚ã†å°‘ã—è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„",
                "å…·ä½“çš„ãªã‚³ãƒ¼ãƒ‰ä¾‹ã‚’ç¤ºã—ã¦ãã ã•ã„",
                "é–¢é€£ã™ã‚‹æŠ€è¡“ã‚„ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚‚æ•™ãˆã¦ãã ã•ã„",
                "å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ã©ã®ã‚ˆã†ã«æ´»ç”¨ã—ã¾ã™ã‹ï¼Ÿ",
                "ã“ã‚Œã®æ³¨æ„ç‚¹ã‚„ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"
            ]

    def _process_conversation_step(self, user_input: str, temperature: Optional[float]):
        """ä¼šè©±ã‚¹ãƒ†ãƒƒãƒ—ã®å‡¦ç†"""
        # å®Ÿè¡Œå›æ•°ã‚’æ›´æ–°
        session_key = f"demo_state_{self.safe_key}"
        if session_key in st.session_state:
            st.session_state[session_key]['execution_count'] += 1

        # ãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±ã®è¡¨ç¤º
        UIHelper.show_token_info(user_input, self.model, position="sidebar")

        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã®æ§‹ç¯‰
        messages = self._build_conversation_messages(user_input)

        # APIã‚³ãƒ¼ãƒ«
        with st.spinner("ğŸ¤– AIãŒæ€è€ƒä¸­..."):
            response = self.call_api_unified(messages, temperature=temperature)

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        assistant_texts = ResponseProcessor.extract_text(response)
        assistant_response = assistant_texts[0] if assistant_texts else "å¿œç­”ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ"

        # ä¼šè©±ã‚¹ãƒ†ãƒƒãƒ—ã®è¨˜éŒ²
        step_data = {
            'step_number'       : len(self.conversation_steps) + 1,
            'timestamp'         : format_timestamp(),
            'model'             : self.model,
            'user_input'        : user_input,
            'assistant_response': assistant_response,
            'messages_at_step'  : [dict(msg) for msg in messages],  # EasyInputMessageParamã‚’è¾æ›¸ã«å¤‰æ›
            'temperature'       : temperature,
            'usage'             : self._extract_usage_info(response),
            'total_tokens'      : self._calculate_total_tokens(response)
        }

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
        self.conversation_steps.append(step_data)
        st.session_state[f"conversation_steps_{self.safe_key}"] = self.conversation_steps

        # æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨å³åº§ã®è¡¨ç¤ºæ›´æ–°
        st.success(f"âœ… ã‚¹ãƒ†ãƒƒãƒ— {step_data['step_number']} ã®å¿œç­”ã‚’å–å¾—ã—ã¾ã—ãŸ")

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®è¡¨ç¤º
        st.subheader("ğŸ¤– æœ€æ–°ã®å›ç­”")
        ResponseProcessorUI.display_response(response)

        # ãƒ•ã‚©ãƒ¼ãƒ ã®å†æç”»ï¼ˆå…¥åŠ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒã‚¯ãƒªã‚¢ã•ã‚Œã‚‹ï¼‰
        st.rerun()

    def _build_conversation_messages(self, new_user_input: str) -> List[EasyInputMessageParam]:
        """ä¼šè©±å±¥æ­´ã‚’åŸºã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã‚’æ§‹ç¯‰"""
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰é–‹å§‹
        messages = get_default_messages()

        # éå»ã®ä¼šè©±ã‚¹ãƒ†ãƒƒãƒ—ã‚’è¿½åŠ 
        for step in self.conversation_steps:
            messages.append(EasyInputMessageParam(role="user", content=step['user_input']))
            messages.append(EasyInputMessageParam(role="assistant", content=step['assistant_response']))

        # æ–°ã—ã„ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’è¿½åŠ 
        messages.append(EasyInputMessageParam(role="user", content=new_user_input))

        return messages

    def _extract_usage_info(self, response: Response) -> Dict[str, Any]:
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ä½¿ç”¨é‡æƒ…å ±ã‚’æŠ½å‡º"""
        try:
            if hasattr(response, 'usage') and response.usage:
                usage_obj = response.usage

                # Pydantic ãƒ¢ãƒ‡ãƒ«ã®å ´åˆ
                if hasattr(usage_obj, 'model_dump'):
                    return usage_obj.model_dump()
                elif hasattr(usage_obj, 'dict'):
                    return usage_obj.dict()
                else:
                    # æ‰‹å‹•ã§å±æ€§ã‚’æŠ½å‡º
                    return {
                        'prompt_tokens'    : getattr(usage_obj, 'prompt_tokens', 0),
                        'completion_tokens': getattr(usage_obj, 'completion_tokens', 0),
                        'total_tokens'     : getattr(usage_obj, 'total_tokens', 0)
                    }
            return {}
        except Exception as e:
            logger.error(f"ä½¿ç”¨é‡æƒ…å ±ã®æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return {}

    def _calculate_total_tokens(self, response: Response) -> int:
        """ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®è¨ˆç®—"""
        usage_info = self._extract_usage_info(response)
        return usage_info.get('total_tokens', 0)

    def _create_conversation_controls(self):
        """ä¼šè©±ç®¡ç†ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«"""
        st.subheader("ğŸ› ï¸ ä¼šè©±ç®¡ç†")

        col1, col2, col3, col4 = st.columns(4)

        with col1:
            if st.button("ğŸ—‘ï¸ ä¼šè©±å±¥æ­´ã‚¯ãƒªã‚¢", key=f"clear_conv_{self.safe_key}"):
                self.conversation_steps.clear()
                st.session_state[f"conversation_steps_{self.safe_key}"] = []
                st.success("ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
                st.rerun()

        with col2:
            if st.button("ğŸ“¥ ä¼šè©±å±¥æ­´ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ", key=f"export_conv_{self.safe_key}"):
                self._export_conversation()

        with col3:
            uploaded_file = st.file_uploader(
                "ğŸ“¤ ä¼šè©±å±¥æ­´ã‚¤ãƒ³ãƒãƒ¼ãƒˆ",
                type=['json'],
                key=f"import_conv_{self.safe_key}",
                help="éå»ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ãŸä¼šè©±å±¥æ­´ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"
            )
            if uploaded_file is not None:
                self._import_conversation(uploaded_file)

        with col4:
            if self.conversation_steps and st.button("ğŸ“Š ä¼šè©±çµ±è¨ˆ", key=f"stats_conv_{self.safe_key}"):
                self._show_conversation_statistics()

    def _export_conversation(self):
        """ä¼šè©±å±¥æ­´ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        if not self.conversation_steps:
            st.warning("ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ä¼šè©±å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“")
            return

        export_data = {
            "export_info"       : {
                "timestamp"   : format_timestamp(),
                "total_steps" : len(self.conversation_steps),
                "model_used"  : self.model,
                "demo_version": "MemoryResponseDemo_v2.0"
            },
            "conversation_steps": self.conversation_steps
        }

        try:
            UIHelper.create_download_button(
                export_data,
                f"conversation_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                "application/json",
                "ğŸ“¥ ä¼šè©±å±¥æ­´JSONãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"
            )
        except Exception as e:
            st.error(f"ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")

    def _import_conversation(self, uploaded_file):
        """ä¼šè©±å±¥æ­´ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"""
        try:
            content = uploaded_file.read()
            data = json.loads(content)

            if "conversation_steps" in data:
                imported_steps = data["conversation_steps"]

                # ç¾åœ¨ã®å±¥æ­´ã«è¿½åŠ  or ç½®æ›
                replace_option = st.radio(
                    "ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ–¹æ³•",
                    ["ç¾åœ¨ã®å±¥æ­´ã«è¿½åŠ ", "ç¾åœ¨ã®å±¥æ­´ã‚’ç½®æ›"],
                    key=f"import_option_{self.safe_key}"
                )

                if st.button("ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Ÿè¡Œ", key=f"execute_import_{self.safe_key}"):
                    if replace_option == "ç¾åœ¨ã®å±¥æ­´ã‚’ç½®æ›":
                        self.conversation_steps = imported_steps
                    else:
                        self.conversation_steps.extend(imported_steps)

                    st.session_state[f"conversation_steps_{self.safe_key}"] = self.conversation_steps
                    st.success(f"{len(imported_steps)}ã‚¹ãƒ†ãƒƒãƒ—ã®ä¼šè©±å±¥æ­´ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã—ãŸ")
                    st.rerun()
            else:
                st.error("æœ‰åŠ¹ãªä¼šè©±å±¥æ­´ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        except Exception as e:
            st.error(f"ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            logger.error(f"Conversation import error: {e}")

    def _show_conversation_statistics(self):
        """ä¼šè©±çµ±è¨ˆã®è¡¨ç¤º"""
        if not self.conversation_steps:
            return

        with st.expander("ğŸ“Š è©³ç´°çµ±è¨ˆ", expanded=True):
            # åŸºæœ¬çµ±è¨ˆ
            total_steps = len(self.conversation_steps)
            total_user_chars = sum(len(step['user_input']) for step in self.conversation_steps)
            total_assistant_chars = sum(len(step['assistant_response']) for step in self.conversation_steps)
            total_tokens = sum(step.get('total_tokens', 0) for step in self.conversation_steps)

            col1, col2 = st.columns(2)
            with col1:
                st.metric("ç·ä¼šè©±ã‚¹ãƒ†ãƒƒãƒ—", total_steps)
                st.metric("ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›æ–‡å­—æ•°", f"{total_user_chars:,}")
                st.metric("AIå¿œç­”æ–‡å­—æ•°", f"{total_assistant_chars:,}")
            with col2:
                st.metric("ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°", f"{total_tokens:,}")
                if total_steps > 0:
                    avg_tokens = total_tokens / total_steps
                    st.metric("å¹³å‡ãƒˆãƒ¼ã‚¯ãƒ³/ã‚¹ãƒ†ãƒƒãƒ—", f"{avg_tokens:.1f}")

                # ã‚³ã‚¹ãƒˆæ¨å®š
                try:
                    estimated_cost = TokenManager.estimate_cost(
                        total_tokens // 2,  # æ¦‚ç®—ã§åŠåˆ†ã‚’å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ã¨ä»®å®š
                        total_tokens // 2,  # åŠåˆ†ã‚’å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ã¨ä»®å®š
                        self.model
                    )
                    st.metric("æ¨å®šç·ã‚³ã‚¹ãƒˆ", f"${estimated_cost:.6f}")
                except Exception as e:
                    st.warning(f"ã‚³ã‚¹ãƒˆæ¨å®šã‚¨ãƒ©ãƒ¼: {e}")

            # æ™‚ç³»åˆ—ã‚°ãƒ©ãƒ•ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            st.write("**ã‚¹ãƒ†ãƒƒãƒ—åˆ¥ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡**")
            step_tokens = [step.get('total_tokens', 0) for step in self.conversation_steps]
            if step_tokens:
                try:
                    df = pd.DataFrame({
                        'ã‚¹ãƒ†ãƒƒãƒ—'  : range(1, len(step_tokens) + 1),
                        'ãƒˆãƒ¼ã‚¯ãƒ³æ•°': step_tokens
                    })
                    st.bar_chart(df.set_index('ã‚¹ãƒ†ãƒƒãƒ—'))
                except Exception as e:
                    st.warning(f"ã‚°ãƒ©ãƒ•è¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}")

            # è³ªå•ã®å‚¾å‘åˆ†æï¼ˆç°¡æ˜“ç‰ˆï¼‰
            st.write("**è³ªå•ã®é•·ã•åˆ†å¸ƒ**")
            question_lengths = [len(step['user_input']) for step in self.conversation_steps]
            if question_lengths:
                avg_length = sum(question_lengths) / len(question_lengths)
                max_length = max(question_lengths)
                min_length = min(question_lengths)

                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("å¹³å‡è³ªå•é•·", f"{avg_length:.1f}æ–‡å­—")
                with col2:
                    st.metric("æœ€é•·è³ªå•", f"{max_length}æ–‡å­—")
                with col3:
                    st.metric("æœ€çŸ­è³ªå•", f"{min_length}æ–‡å­—")

# ==================================================
# ç”»åƒå¿œç­”ãƒ‡ãƒ¢
# ==================================================
class ImageResponseDemo(BaseDemo):
    """ç”»åƒå…¥åŠ›ã®ãƒ‡ãƒ¢ï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰"""

    def __init__(self, demo_name: str, use_base64: bool = False):
        super().__init__(demo_name)
        self.use_base64 = use_base64

    @error_handler_ui
    @timer_ui
    def run(self):
        """ãƒ‡ãƒ¢ã®å®Ÿè¡Œï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰"""
        self.initialize()
        with st.expander("OpenAI API(IPO):å®Ÿè£…ä¾‹", expanded=False):
            st.write(
                "ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¯¾å¿œã®responses.create()ãƒ‡ãƒ¢ã€‚URLãƒ»Base64å½¢å¼ã®ç”»åƒå…¥åŠ›ã«å¯¾å¿œã€‚ResponseInputTextParamã¨ResponseInputImageParamã‚’çµ„ã¿åˆã‚ã›ã¦ç”»åƒè§£æã‚’å®Ÿè¡Œã€‚GPT-4oã®è¦–è¦šæ©Ÿèƒ½æ´»ç”¨ä¾‹ã€‚")
            st.code("""
            messages = get_default_messages()
            messages.append(
                EasyInputMessageParam(
                    role="user",
                    content=[
                        ResponseInputTextParam(type="input_text", text=question),
                        ResponseInputImageParam(
                            type="input_image",
                            image_url=image_url,
                            detail="auto"
                        ),
                    ],
                )
            )
            response = self.call_api_unified(messages, temperature=temperature)
            ResponseProcessorUI.display_response(response)
            """)

        if self.use_base64:
            self._run_base64_demo()
        else:
            self._run_url_demo()

    def _run_url_demo(self):
        """URLç”»åƒã®ãƒ‡ãƒ¢ï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰"""
        st.write("ä¾‹: ã“ã®ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’æ—¥æœ¬èªã§èª¬æ˜ã—ãªã•ã„ã€‚")

        image_url = st.text_input(
            "ç”»åƒURLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
            value=image_path_sample,
            key=f"img_url_{self.safe_key}"
        )

        if image_url:
            try:
                st.image(image_url, caption="å…¥åŠ›ç”»åƒ", use_container_width=True)
            except Exception as e:
                st.error(f"ç”»åƒã®è¡¨ç¤ºã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

        with st.form(key=f"img_form_{self.safe_key}"):
            question = st.text_input("è³ªå•", value="ã“ã®ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’æ—¥æœ¬èªã§èª¬æ˜ã—ãªã•ã„ã€‚")

            # çµ±ä¸€ã•ã‚ŒãŸtemperatureã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«
            temperature = self.create_temperature_control(
                default_temp=0.3,
                help_text="ä½ã„å€¤ã»ã©ä¸€è²«æ€§ã®ã‚ã‚‹å›ç­”"
            )

            submitted = st.form_submit_button("ç”»åƒã§è³ªå•")

        if submitted and image_url and question:
            self._process_image_question(question, image_url, temperature)

    def _run_base64_demo(self):
        """Base64ç”»åƒã®ãƒ‡ãƒ¢ï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰"""
        images_dir = config.get("paths.images_dir", "images")
        files = self._get_image_files(images_dir)

        if not files:
            st.warning(f"{images_dir} ã«ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
            return

        file_path = st.selectbox("ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ", files, key=f"img_select_{self.safe_key}")

        with st.form(key=f"img_b64_form_{self.safe_key}"):
            # çµ±ä¸€ã•ã‚ŒãŸtemperatureã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«
            temperature = self.create_temperature_control(
                default_temp=0.3,
                help_text="ä½ã„å€¤ã»ã©ä¸€è²«æ€§ã®ã‚ã‚‹å›ç­”"
            )

            submitted = st.form_submit_button("é¸æŠç”»åƒã§å®Ÿè¡Œ")

        if submitted and file_path:
            self._process_base64_image(file_path, temperature)

    def _get_image_files(self, images_dir: str) -> List[str]:
        """ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        patterns = ["*.png", "*.jpg", "*.jpeg", "*.webp", "*.gif"]
        files = []
        for pattern in patterns:
            files.extend(glob.glob(f"{images_dir}/{pattern}"))
        return sorted(files)

    def _encode_image(self, path: str) -> str:
        """ç”»åƒã‚’Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
        try:
            with open(path, "rb") as f:
                return base64.b64encode(f.read()).decode()
        except Exception as e:
            st.error(f"ç”»åƒã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return ""

    def _process_image_question(self, question: str, image_url: str, temperature: Optional[float]):
        """ç”»åƒè³ªå•ã®å‡¦ç†ï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰"""
        messages = get_default_messages()
        messages.append(
            EasyInputMessageParam(
                role="user",
                content=[
                    ResponseInputTextParam(type="input_text", text=question),
                    ResponseInputImageParam(
                        type="input_image",
                        image_url=image_url,
                        detail="auto"
                    ),
                ],
            )
        )

        with st.spinner("å‡¦ç†ä¸­..."):
            response = self.call_api_unified(messages, temperature=temperature)

        st.subheader("å›ç­”:")
        ResponseProcessorUI.display_response(response)

    def _process_base64_image(self, file_path: str, temperature: Optional[float]):
        """Base64ç”»åƒã®å‡¦ç†ï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰"""
        b64 = self._encode_image(file_path)
        if not b64:
            return

        st.image(file_path, caption="é¸æŠç”»åƒ", width=320)

        messages = get_default_messages()
        messages.append(
            EasyInputMessageParam(
                role="user",
                content=[
                    ResponseInputTextParam(
                        type="input_text",
                        text="ã“ã®ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’æ—¥æœ¬èªã§èª¬æ˜ã—ãªã•ã„ã€‚"
                    ),
                    ResponseInputImageParam(
                        type="input_image",
                        image_url=f"data:image/jpeg;base64,{b64}",
                        detail="auto"
                    ),
                ],
            )
        )

        with st.spinner("å‡¦ç†ä¸­..."):
            response = self.call_api_unified(messages, temperature=temperature)

        st.subheader("å‡ºåŠ›ãƒ†ã‚­ã‚¹ãƒˆ:")
        ResponseProcessorUI.display_response(response)


# ==================================================
# æ§‹é€ åŒ–å‡ºåŠ›ãƒ‡ãƒ¢ï¼ˆä¿®æ­£ç‰ˆãƒ»å·¦ãƒšã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«é¸æŠçµ±ä¸€ï¼‰
# ==================================================
class StructuredOutputDemo(BaseDemo):
    """æ§‹é€ åŒ–å‡ºåŠ›ã®ãƒ‡ãƒ¢"""

    class Event(BaseModel):
        """ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã®Pydanticãƒ¢ãƒ‡ãƒ«"""
        name: str
        date: str
        participants: List[str]

    def __init__(self, demo_name: str, use_parse: bool = False):
        super().__init__(demo_name)
        self.use_parse = use_parse

    @error_handler_ui
    @timer_ui
    def run(self):
        """ãƒ‡ãƒ¢ã®å®Ÿè¡Œï¼ˆä¿®æ­£ç‰ˆãƒ»å·¦ãƒšã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«é¸æŠçµ±ä¸€ï¼‰"""
        self.initialize()  # å·¦ãƒšã‚¤ãƒ³ã«ãƒ¢ãƒ‡ãƒ«é¸æŠãŒä½œæˆã•ã‚Œã‚‹
        st.write(
            "æ§‹é€ åŒ–å‡ºåŠ›ç‰¹åŒ–ã®responses.create()/responses.parse()ãƒ‡ãƒ¢ã€‚Pydanticãƒ¢ãƒ‡ãƒ«ã¨JSON Schemaã«ã‚ˆã‚‹å‹å®‰å…¨ãªå‡ºåŠ›æŠ½å‡ºã€‚"
            "ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã®æ§‹é€ åŒ–æŠ½å‡ºä¾‹ã‚’é€šã˜ã¦ã€ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¢ãƒ—ãƒªã§ã®APIæ´»ç”¨ã‚’å­¦ç¿’ã€‚"
        )
        with st.expander("OpenAI API(IPO):å®Ÿè£…ä¾‹", expanded=False):
            st.code("""
            # ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã®Pydanticãƒ¢ãƒ‡ãƒ«
            class Event(BaseModel):
                name: str
                date: str
                participants: List[str]
            
            # å®Ÿè¡Œæ–¹å¼ã‚’é¸æŠ
            # ["responses.create() ã‚’ä½¿ç”¨", "responses.parse() ã‚’ä½¿ç”¨"]
            # responses.createã‚’ä½¿ç”¨ã—ãŸå®Ÿè¡Œ ---------------------------
            schema = {
                "type"                : "object",
                "properties"          : {
                    "name"        : {
                        "type"       : "string",
                        "description": "ã‚¤ãƒ™ãƒ³ãƒˆã®åå‰"
                    },
                    "date"        : {
                        "type"       : "string",
                        "description": "ã‚¤ãƒ™ãƒ³ãƒˆã®é–‹å‚¬æ—¥ï¼ˆYYYY-MM-DDå½¢å¼ï¼‰"
                    },
                    "participants": {
                        "type"       : "array",
                        "items"      : {"type": "string"},
                        "description": "å‚åŠ è€…ãƒªã‚¹ãƒˆ"
                    },
                },
                "required"            : ["name", "date", "participants"],
                "additionalProperties": False,
            }
    
            messages = [
                EasyInputMessageParam(
                    role="developer",
                    content="Extract event details from the text. Extract name, date, and participants."
                ),
                EasyInputMessageParam(
                    role="user",
                    content=[ResponseInputTextParam(type="input_text", text=text)]
                ),
            ]
            text_cfg = ResponseTextConfigParam(
                format=ResponseFormatTextJSONSchemaConfigParam(
                    name="event_extraction",
                    type="json_schema",
                    schema=schema,
                    strict=True,
                )
            )
            api_params = {
                "model": model,
                "input": messages,
                "text" : text_cfg
            }
            response = self.client.create_response(**api_params)
            # ----------------------------------------------------
            
            # responses.parseã‚’ä½¿ç”¨ã—ãŸå®Ÿè¡Œ
            # Responses APIç”¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼
            messages = [
                EasyInputMessageParam(
                    role="developer",
                    content="Extract event details from the text. Extract name, date, and participants."
                ),
                EasyInputMessageParam(
                    role="user",
                    content=[ResponseInputTextParam(type="input_text", text=text)]
                ),
            ]
            api_params = {
                "model"      : model,
                "input"      : messages,
                "text_format": self.Event,
            }
            response = self.client.parse_response(**api_params)
            event = response.output_parsed
            
            """)

        # é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®è¡¨ç¤ºï¼ˆæƒ…å ±ã¨ã—ã¦ï¼‰
        st.info(f"ğŸ¤– ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: **{self.model}**")

        # ãƒ¢ãƒ‡ãƒ«ã®æ¨å¥¨äº‹é …
        if "gpt-4o" in self.model:
            st.success("âœ… æ§‹é€ åŒ–å‡ºåŠ›ã«é©ã—ãŸãƒ¢ãƒ‡ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã™")
        elif self.model.startswith("o"):
            st.warning("âš ï¸ æ¨è«–ç³»ãƒ¢ãƒ‡ãƒ«ã¯æ§‹é€ åŒ–å‡ºåŠ›ã§åˆ¶é™ã•ã‚Œã‚‹å ´åˆãŒã‚ã‚Šã¾ã™")
        else:
            st.info("â„¹ï¸ æ§‹é€ åŒ–å‡ºåŠ›ã«ã¯ gpt-4o ç³»ãƒ¢ãƒ‡ãƒ«ãŒæ¨å¥¨ã•ã‚Œã¾ã™")

        # ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±å…¥åŠ›
        default_event = config.get("samples.prompts.event_example",
                                   "å°æ¹¾ãƒ•ã‚§ã‚¹2025-08-21 ï½ã‚ã¤ã¾ã‚Œï¼ç©¶æ¥µã®å°æ¹¾ã‚°ãƒ«ãƒ¡ï½")

        st.subheader("ğŸ“ ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±å…¥åŠ›")
        text = st.text_input(
            "ã‚¤ãƒ™ãƒ³ãƒˆè©³ç´°ã‚’å…¥åŠ›",
            value=default_event,
            key=f"struct_input_{self.safe_key}",
            help="ã‚¤ãƒ™ãƒ³ãƒˆåã€æ—¥ä»˜ã€å‚åŠ è€…æƒ…å ±ã‚’å«ã‚€ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"
        )

        # çµ±ä¸€ã•ã‚ŒãŸtemperatureã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«
        temperature = self.create_temperature_control(
            default_temp=0.1,
            help_text="æ§‹é€ åŒ–å‡ºåŠ›ã§ã¯ä½ã„å€¤ã‚’æ¨å¥¨"
        )

        # å®Ÿè¡Œæ–¹å¼ã®é¸æŠ
        st.subheader("âš™ï¸ å®Ÿè¡Œæ–¹å¼")
        use_parse_option = st.radio(
            "å®Ÿè¡Œæ–¹å¼ã‚’é¸æŠ",
            ["responses.create() ã‚’ä½¿ç”¨", "responses.parse() ã‚’ä½¿ç”¨"],
            index=0 if not self.use_parse else 1,
            key=f"parse_option_{self.safe_key}",
            help="responses.create()ã¯æ±ç”¨çš„ã€chat.completions.parse()ã¯Pydanticç‰¹åŒ–"
        )

        # é¸æŠã«åŸºã¥ã„ã¦use_parseã‚’æ›´æ–°
        self.use_parse = (use_parse_option == "responses.parse() ã‚’ä½¿ç”¨")

        # å®Ÿè¡Œãƒœã‚¿ãƒ³
        col1, col2, col3 = st.columns([1, 2, 1])
        with col2:
            execute_button = st.button(
                "ğŸš€ ã‚¤ãƒ™ãƒ³ãƒˆæŠ½å‡ºã‚’å®Ÿè¡Œ",
                key=f"struct_btn_{self.safe_key}",
                use_container_width=True,
                type="primary"
            )

        # å®Ÿè¡Œå‡¦ç†
        if execute_button and text.strip():
            if self.use_parse:
                self._run_with_parse(self.model, text, temperature)
            else:
                self._run_with_create(self.model, text, temperature)
        elif execute_button and not text.strip():
            st.warning("âš ï¸ ã‚¤ãƒ™ãƒ³ãƒˆè©³ç´°ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

        # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
        self._show_sample_output()

    def _run_with_create(self, model: str, text: str, temperature: Optional[float]):
        """responses.createã‚’ä½¿ç”¨ã—ãŸå®Ÿè¡Œï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        try:
            st.info("ğŸ”„ responses.create() ã§ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‚’æŠ½å‡ºä¸­...")

            schema = {
                "type"                : "object",
                "properties"          : {
                    "name"        : {
                        "type"       : "string",
                        "description": "ã‚¤ãƒ™ãƒ³ãƒˆã®åå‰"
                    },
                    "date"        : {
                        "type"       : "string",
                        "description": "ã‚¤ãƒ™ãƒ³ãƒˆã®é–‹å‚¬æ—¥ï¼ˆYYYY-MM-DDå½¢å¼ï¼‰"
                    },
                    "participants": {
                        "type"       : "array",
                        "items"      : {"type": "string"},
                        "description": "å‚åŠ è€…ãƒªã‚¹ãƒˆ"
                    },
                },
                "required"            : ["name", "date", "participants"],
                "additionalProperties": False,
            }

            messages = [
                EasyInputMessageParam(
                    role="developer",
                    content="Extract event details from the text. Extract name, date, and participants."
                ),
                EasyInputMessageParam(
                    role="user",
                    content=[ResponseInputTextParam(type="input_text", text=text)]
                ),
            ]

            text_cfg = ResponseTextConfigParam(
                format=ResponseFormatTextJSONSchemaConfigParam(
                    name="event_extraction",
                    type="json_schema",
                    schema=schema,
                    strict=True,
                )
            )

            with st.spinner("ğŸ¤– AI ãŒã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‚’æŠ½å‡ºã—ã¦ã„ã¾ã™..."):
                # çµ±ä¸€ã•ã‚ŒãŸAPIå‘¼ã³å‡ºã—ï¼ˆå·¦ãƒšã‚¤ãƒ³ã§é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼‰
                api_params = {
                    "model": model,
                    "input": messages,
                    "text" : text_cfg
                }

                # temperatureã‚µãƒãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯
                if not self.is_reasoning_model(model) and temperature is not None:
                    api_params["temperature"] = temperature

                response = self.client.create_response(**api_params)

            # çµæœã®è¡¨ç¤º
            st.success("âœ… ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã®æŠ½å‡ºãŒå®Œäº†ã—ã¾ã—ãŸ")

            # JSONå‡ºåŠ›ã‚’Pydanticãƒ¢ãƒ‡ãƒ«ã§æ¤œè¨¼
            event = self.Event.model_validate_json(response.output_text)

            st.subheader("ğŸ“‹ æŠ½å‡ºçµæœ (responses.create)")
            self._display_extracted_event(event, response)

        except (ValidationError, json.JSONDecodeError) as e:
            st.error("âŒ æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸ")
            with st.expander("ğŸ”§ ã‚¨ãƒ©ãƒ¼è©³ç´°", expanded=False):
                st.exception(e)
        except Exception as e:
            self.handle_error(e)

    def _run_with_parse(self, model: str, text: str, temperature: Optional[float]):
        """responses.parseã‚’ä½¿ç”¨ã—ãŸå®Ÿè¡Œ"""
        try:
            st.info("ğŸ”„ responses.parse() ã§ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‚’æŠ½å‡ºä¸­...")

            # Responses APIç”¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼ã«å¤‰æ›´
            messages = [
                EasyInputMessageParam(
                    role="developer",
                    content="Extract event details from the text. Extract name, date, and participants."
                ),
                EasyInputMessageParam(
                    role="user",
                    content=[ResponseInputTextParam(type="input_text", text=text)]
                ),
            ]

            with st.spinner("ğŸ¤– AI ãŒã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‚’æŠ½å‡ºã—ã¦ã„ã¾ã™..."):
                # çµ±ä¸€ã•ã‚ŒãŸAPIå‘¼ã³å‡ºã—ï¼ˆresponses.parseã‚’ä½¿ç”¨ï¼‰
                api_params = {
                    "model": model,
                    "input": messages,
                    "text_format": self.Event
                }

                # temperatureã‚µãƒãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯
                if not self.is_reasoning_model(model) and temperature is not None:
                    api_params["temperature"] = temperature

                response = self.client.parse_response(**api_params)

            # çµæœã®è¡¨ç¤º
            st.success("âœ… ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã®æŠ½å‡ºãŒå®Œäº†ã—ã¾ã—ãŸ")

            # responses.parseã®çµæœã¯output_parsedã«æ ¼ç´ã•ã‚Œã‚‹
            event = response.output_parsed

            st.subheader("ğŸ“‹ æŠ½å‡ºçµæœ (responses.parse)")
            self._display_extracted_event(event, response)

        except Exception as e:
            st.error(f"âŒ responses.parseå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}")
            self.handle_error(e)

    def _display_extracted_event(self, event: Event, response):
        """æŠ½å‡ºã•ã‚ŒãŸã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã®è¡¨ç¤ºï¼ˆã‚¨ã‚¯ã‚¹ãƒ‘ãƒ³ãƒ€ãƒ¼å…¥ã‚Œå­ä¿®æ­£ç‰ˆï¼‰"""
        # ãƒ¡ã‚¤ãƒ³ã®çµæœè¡¨ç¤º
        col1, col2 = st.columns([2, 1])

        with col1:
            # ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‚’è¦‹ã‚„ã™ãè¡¨ç¤º
            st.write("**ğŸ‰ ã‚¤ãƒ™ãƒ³ãƒˆå**")
            st.success(event.name)

            st.write("**ğŸ“… é–‹å‚¬æ—¥**")
            st.info(event.date)

            st.write("**ğŸ‘¥ å‚åŠ è€…**")
            if event.participants:
                for i, participant in enumerate(event.participants, 1):
                    st.write(f"{i}. {participant}")
            else:
                st.write("å‚åŠ è€…æƒ…å ±ãªã—")

        with col2:
            # çµ±è¨ˆæƒ…å ±
            st.metric("å‚åŠ è€…æ•°", len(event.participants))
            if hasattr(response, 'usage') and response.usage:
                usage = response.usage
                if hasattr(usage, 'total_tokens'):
                    st.metric("ä½¿ç”¨ãƒˆãƒ¼ã‚¯ãƒ³æ•°", getattr(usage, 'total_tokens', 0))

        # æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤ºï¼ˆã‚¨ã‚¯ã‚¹ãƒ‘ãƒ³ãƒ€ãƒ¼ãªã—ã§ç›´æ¥è¡¨ç¤ºï¼‰
        st.write("---")
        st.write("**ğŸ”§ æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ (Pydantic):**")

        # Pydanticãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦è¡¨ç¤º
        safe_streamlit_json(event.model_dump())

        # Pythonã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã—ã¦è¡¨ç¤º
        st.write("**Python ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ:**")
        st.code(repr(event), language="python")

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹è©³ç´°ï¼ˆã‚¨ã‚¯ã‚¹ãƒ‘ãƒ³ãƒ€ãƒ¼ãªã—ã§ç°¡æ½”ã«è¡¨ç¤ºï¼‰
        st.write("---")
        st.write("**ğŸ“Š API ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ¦‚è¦:**")

        # åŸºæœ¬æƒ…å ±ã®ã¿è¡¨ç¤º
        info_cols = st.columns(3)
        with info_cols[0]:
            model_name = getattr(response, 'model', 'N/A')
            st.write(f"**ãƒ¢ãƒ‡ãƒ«**: {model_name}")
        with info_cols[1]:
            response_id = getattr(response, 'id', 'N/A')
            st.write(f"**ID**: {response_id[:10]}..." if len(str(response_id)) > 10 else f"**ID**: {response_id}")
        with info_cols[2]:
            st.write(f"**å½¢å¼**: Structured JSON")

    def _show_sample_output(self):
        """ã‚µãƒ³ãƒ—ãƒ«å‡ºåŠ›ã®è¡¨ç¤ºï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        with st.expander("ğŸ“– ã‚µãƒ³ãƒ—ãƒ«å‡ºåŠ›ä¾‹", expanded=False):
            st.write("**å…¥åŠ›ä¾‹:**")
            st.code('å°æ¹¾ãƒ•ã‚§ã‚¹2025 ï½ã‚ã¤ã¾ã‚Œï¼ç©¶æ¥µã®å°æ¹¾ã‚°ãƒ«ãƒ¡ï½ in Kawasaki Spark', language="text")

            st.write("**æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:**")
            sample_event = {
                "name"        : "å°æ¹¾ãƒ•ã‚§ã‚¹2025 ï½ã‚ã¤ã¾ã‚Œï¼ç©¶æ¥µã®å°æ¹¾ã‚°ãƒ«ãƒ¡ï½",
                "date"        : "2025-08-15",
                "participants": ["ã‚°ãƒ«ãƒ¡æ„›å¥½å®¶", "å°æ¹¾æ–™ç†ãƒ•ã‚¡ãƒ³", "åœ°åŸŸä½æ°‘"]
            }
            safe_streamlit_json(sample_event)

            st.write("**å®Ÿè¡Œæ–¹å¼ã®é•ã„:**")
            st.write("- **responses.create()**: JSON Schemaã‚’ä½¿ç”¨ã—ãŸæ±ç”¨çš„ãªæ§‹é€ åŒ–å‡ºåŠ›")
            st.write("- **responses.parse()**: Pydanticãƒ¢ãƒ‡ãƒ«ã‚’ç›´æ¥ä½¿ç”¨ã—ãŸå‹å®‰å…¨ãªå‡ºåŠ›")

            st.write("**Pydantic ãƒ¢ãƒ‡ãƒ«å®šç¾©:**")
            st.code('''
class Event(BaseModel):
    """ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã®Pydanticãƒ¢ãƒ‡ãƒ«"""
    name: str
    date: str
    participants: List[str]
            ''', language="python")


# ==================================================
# å¤©æ°—ãƒ‡ãƒ¢
# ==================================================
class WeatherDemo(BaseDemo):
    """OpenWeatherMap APIã‚’ä½¿ç”¨ã—ãŸå¤©æ°—ãƒ‡ãƒ¢ï¼ˆæ”¹ä¿®ç‰ˆãƒ»ãƒœã‚¿ãƒ³å®Ÿè¡Œå¯¾å¿œï¼‰"""

    @error_handler_ui
    @timer_ui
    def run(self):
        """ãƒ‡ãƒ¢ã®å®Ÿè¡Œï¼ˆæ”¹ä¿®ç‰ˆï¼‰"""
        self.initialize()
        st.header("æ§‹é€ åŒ–å‡ºåŠ›: å¤©æ°—ãƒ‡ãƒ¢")
        st.write(
            "å¤–éƒ¨APIé€£æºãƒ‡ãƒ¢ï¼ˆæ”¹ä¿®ç‰ˆï¼‰ã€‚éƒ½å¸‚é¸æŠå¾Œã€ã€ŒAPIã‚’å®Ÿè¡Œã€ãƒœã‚¿ãƒ³ã§OpenWeatherMap APIã‚’å‘¼ã³å‡ºã—ã€"
            "å¤©æ°—æƒ…å ±ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚å®Ÿä¸–ç•Œãƒ‡ãƒ¼ã‚¿çµ±åˆã¨UIæ“ä½œãƒ•ãƒ­ãƒ¼ã®å®Ÿè£…ä¾‹ã€‚"
        )
        with st.expander("åˆ©ç”¨ï¼šOpenWeatherMap API(æ¯”è¼ƒç”¨)", expanded=False):
            st.code("""
            df_jp = self._load_japanese_cities(cities_json)
            # def _get_current_weather
            url = "http://api.openweathermap.org/data/2.5/weather"
                params = {
                    "lat"  : lat,
                    "lon"  : lon,
                    "appid": api_key,
                    "units": unit,
                    "lang" : "ja"  # æ—¥æœ¬èªã§ã®å¤©æ°—èª¬æ˜
                }
            response = requests.get(url, params=params, timeout=config.get("api.timeout", 30))
            """)

        # éƒ½å¸‚ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ï¼ˆJSONã‹ã‚‰æ—¥æœ¬éƒ½å¸‚ã®ã¿ï¼‰
        cities_json = config.get("paths.cities_json", "data/city_jp.list.json")
        if not Path(cities_json).exists():
            st.error(f"éƒ½å¸‚ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {cities_json}")
            return

        df_jp = self._load_japanese_cities(cities_json)

        # éƒ½å¸‚é¸æŠUI
        city, lat, lon = self._select_city(df_jp)

        # APIã‚’å®Ÿè¡Œãƒœã‚¿ãƒ³ã®è¿½åŠ 
        col1, col2, col3 = st.columns([2, 1, 2])
        with col2:
            api_execute = st.button(
                "ğŸŒ¤ï¸ APIã‚’å®Ÿè¡Œ",
                key=f"weather_api_{self.safe_key}",
                use_container_width=True,
                type="primary",
                help=f"é¸æŠã—ãŸéƒ½å¸‚ï¼ˆ{city}ï¼‰ã®å¤©æ°—æƒ…å ±ã‚’å–å¾—ã—ã¾ã™"
            )

        # é¸æŠã•ã‚ŒãŸéƒ½å¸‚ã®æƒ…å ±è¡¨ç¤º
        if city and lat and lon:
            with st.expander("ğŸ“ é¸æŠã•ã‚ŒãŸéƒ½å¸‚æƒ…å ±", expanded=False):
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("éƒ½å¸‚å", city)
                with col2:
                    st.metric("ç·¯åº¦", f"{lat:.4f}")
                with col3:
                    st.metric("çµŒåº¦", f"{lon:.4f}")

        # APIã‚­ãƒ¼ã®ç¢ºèª
        api_key = os.getenv("OPENWEATHER_API_KEY")
        if not api_key:
            st.warning("âš ï¸ OPENWEATHER_API_KEYç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            st.info("å¤©æ°—APIã‚’åˆ©ç”¨ã™ã‚‹ã«ã¯ã€OpenWeatherMapã®APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™ã€‚")
            st.code("export OPENWEATHER_API_KEY='your-api-key'", language="bash")
            return

        # APIã‚’å®Ÿè¡Œãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚ŒãŸå ´åˆ
        if api_execute:
            if city and lat and lon:
                st.info(f"ğŸ” {city}ã®å¤©æ°—æƒ…å ±ã‚’å–å¾—ä¸­...")
                self._display_weather(lat, lon, city)
            else:
                st.error("âŒ éƒ½å¸‚ãŒæ­£ã—ãé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚éƒ½å¸‚ã‚’é¸æŠã—ã¦ã‹ã‚‰å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

    def _load_japanese_cities(self, json_path: str) -> pd.DataFrame:
        """æ—¥æœ¬ã®éƒ½å¸‚ãƒ‡ãƒ¼ã‚¿ã‚’ city_jp.list.json ã‹ã‚‰èª­ã¿è¾¼ã¿"""
        try:
            with open(json_path, "r", encoding="utf-8") as f:
                cities_list = json.load(f)
            # å¿…è¦ãªã‚«ãƒ©ãƒ ã®ã¿æŠ½å‡º
            df = pd.DataFrame([
                {
                    "name": city["name"],
                    "lat" : city["coord"]["lat"],
                    "lon" : city["coord"]["lon"],
                    "id"  : city["id"]
                }
                for city in cities_list
            ])
            # éƒ½å¸‚åã§ã‚½ãƒ¼ãƒˆ
            return df.sort_values("name").reset_index(drop=True)
        except Exception as e:
            st.error(f"éƒ½å¸‚ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return pd.DataFrame()

    def _select_city(self, df: pd.DataFrame) -> tuple:
        """éƒ½å¸‚é¸æŠUIï¼ˆæ”¹ä¿®ç‰ˆï¼‰"""
        if df.empty:
            st.error("éƒ½å¸‚ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
            return "Tokyo", 35.6895, 139.69171

        # éƒ½å¸‚é¸æŠã®èª¬æ˜
        st.subheader("ğŸ™ï¸ éƒ½å¸‚é¸æŠ")
        st.write("å¤©æ°—æƒ…å ±ã‚’å–å¾—ã—ãŸã„éƒ½å¸‚ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼š")

        # éƒ½å¸‚é¸æŠãƒœãƒƒã‚¯ã‚¹
        city = st.selectbox(
            "éƒ½å¸‚ã‚’é¸æŠã—ã¦ãã ã•ã„",
            df["name"].tolist(),
            key=f"city_{self.safe_key}",
            help="æ—¥æœ¬å›½å†…ã®ä¸»è¦éƒ½å¸‚ã‹ã‚‰é¸æŠã§ãã¾ã™"
        )

        row = df[df["name"] == city].iloc[0]

        return city, row["lat"], row["lon"]

    def _display_weather(self, lat: float, lon: float, city_name: str = None):
        """å¤©æ°—æƒ…å ±ã®è¡¨ç¤ºï¼ˆæ”¹ä¿®ç‰ˆï¼‰"""
        try:
            # å®Ÿè¡Œæ™‚é–“ã®è¨ˆæ¸¬é–‹å§‹
            start_time = time.time()

            # ç¾åœ¨ã®å¤©æ°—
            with st.spinner(f"ğŸŒ¤ï¸ {city_name or 'é¸æŠã—ãŸéƒ½å¸‚'}ã®ç¾åœ¨ã®å¤©æ°—ã‚’å–å¾—ä¸­..."):
                today = self._get_current_weather(lat, lon)

            if today:
                st.success("âœ… ç¾åœ¨ã®å¤©æ°—æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸ")

                # ç¾åœ¨ã®å¤©æ°—è¡¨ç¤º
                with st.container():
                    st.write("### ğŸ“ æœ¬æ—¥ã®å¤©æ°—")

                    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("ğŸ™ï¸ éƒ½å¸‚", today['city'])
                    with col2:
                        st.metric("ğŸŒ¡ï¸ æ°—æ¸©", f"{today['temperature']}â„ƒ")
                    with col3:
                        st.metric("ğŸ’¨ å¤©æ°—", today['description'])
                    with col4:
                        # åº§æ¨™æƒ…å ±
                        coord = today.get('coord', {})
                        st.metric("ğŸ“ åº§æ¨™", f"{coord.get('lat', 'N/A'):.2f}, {coord.get('lon', 'N/A'):.2f}")

            # é€±é–“äºˆå ±
            with st.spinner("ğŸ“Š 5æ—¥é–“äºˆå ±ã‚’å–å¾—ä¸­..."):
                forecast = self._get_weekly_forecast(lat, lon)

            if forecast:
                st.success("âœ… é€±é–“äºˆå ±ã‚’å–å¾—ã—ã¾ã—ãŸ")

                # 5æ—¥é–“äºˆå ±è¡¨ç¤º
                with st.container():
                    st.write("### ğŸ“… 5æ—¥é–“äºˆå ± ï¼ˆ3æ™‚é–“æ¯ãƒ‡ãƒ¼ã‚¿ã®æ—¥åˆ¥å¹³å‡ï¼‰")

                    # ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ã§è¡¨ç¤º
                    forecast_df = pd.DataFrame(forecast)

                    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚«ãƒ©ãƒ åã‚’æ—¥æœ¬èªã«å¤‰æ›´
                    forecast_df = forecast_df.rename(columns={
                        'date'    : 'æ—¥ä»˜',
                        'temp_avg': 'å¹³å‡æ°—æ¸©(â„ƒ)',
                        'weather' : 'å¤©æ°—'
                    })

                    st.dataframe(
                        forecast_df,
                        use_container_width=True,
                        hide_index=True
                    )

                    # ã‚°ãƒ©ãƒ•è¡¨ç¤º
                    if len(forecast) > 1:
                        st.write("### ğŸ“ˆ æ°—æ¸©æ¨ç§»")
                        temp_data = pd.DataFrame({
                            'æ—¥ä»˜'    : [item['date'] for item in forecast],
                            'å¹³å‡æ°—æ¸©': [item['temp_avg'] for item in forecast]
                        })
                        st.line_chart(temp_data.set_index('æ—¥ä»˜'))

            # å®Ÿè¡Œæ™‚é–“ã®è¡¨ç¤º
            end_time = time.time()
            execution_time = end_time - start_time

            with st.expander("ğŸ”§ APIå®Ÿè¡Œè©³ç´°", expanded=False):
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("å®Ÿè¡Œæ™‚é–“", f"{execution_time:.2f}ç§’")
                with col2:
                    st.metric("APIå‘¼ã³å‡ºã—æ•°", "2å›")  # ç¾åœ¨å¤©æ°— + 5æ—¥é–“äºˆå ±
                with col3:
                    st.metric("ãƒ‡ãƒ¼ã‚¿å½¢å¼", "JSON")

                st.write("**APIè©³ç´°:**")
                st.write("- ç¾åœ¨ã®å¤©æ°—: OpenWeatherMap Current Weather API")
                st.write("- 5æ—¥é–“äºˆå ±: OpenWeatherMap 5 Day Weather Forecast API")
                st.write("- ãƒ‡ãƒ¼ã‚¿æ›´æ–°é »åº¦: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ")

        except Exception as e:
            st.error(f"âŒ å¤©æ°—æƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
            logger.error(f"Weather API error: {e}")

            # ã‚¨ãƒ©ãƒ¼ã®è©³ç´°è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰æ™‚ï¼‰
            if config.get("experimental.debug_mode", False):
                with st.expander("ğŸ”§ ã‚¨ãƒ©ãƒ¼è©³ç´°", expanded=False):
                    st.exception(e)

    def _get_current_weather(self, lat: float, lon: float, unit: str = "metric") -> dict[str, Any] | None:
        """ç¾åœ¨ã®å¤©æ°—ã‚’å–å¾—ï¼ˆæ”¹ä¿®ç‰ˆï¼‰"""
        api_key = os.getenv("OPENWEATHER_API_KEY")
        if not api_key:
            st.error("âŒ OPENWEATHER_API_KEYç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return None

        try:
            url = "http://api.openweathermap.org/data/2.5/weather"
            params = {
                "lat"  : lat,
                "lon"  : lon,
                "appid": api_key,
                "units": unit,
                "lang" : "ja"  # æ—¥æœ¬èªã§ã®å¤©æ°—èª¬æ˜
            }

            response = requests.get(url, params=params, timeout=config.get("api.timeout", 30))
            response.raise_for_status()
            data = response.json()

            return {
                "city"       : data["name"],
                "temperature": round(data["main"]["temp"], 1),
                "description": data["weather"][0]["description"],
                "coord"      : data["coord"],
                "humidity"   : data["main"]["humidity"],
                "pressure"   : data["main"]["pressure"],
                "wind_speed" : data.get("wind", {}).get("speed", 0)
            }
        except requests.exceptions.RequestException as e:
            st.error(f"âŒ å¤©æ°—APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
            logger.error(f"Weather API request error: {e}")
            return None
        except Exception as e:
            st.error(f"âŒ å¤©æ°—ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            logger.error(f"Weather data processing error: {e}")
            return None

    def _get_weekly_forecast(self, lat: float, lon: float, unit: str = "metric") -> List[dict]:
        """é€±é–“äºˆå ±ã‚’å–å¾—ï¼ˆæ”¹ä¿®ç‰ˆï¼‰"""
        api_key = os.getenv("OPENWEATHER_API_KEY")
        if not api_key:
            return []

        try:
            url = "http://api.openweathermap.org/data/2.5/forecast"
            params = {
                "lat"  : lat,
                "lon"  : lon,
                "units": unit,
                "appid": api_key,
                "lang" : "ja"  # æ—¥æœ¬èªã§ã®å¤©æ°—èª¬æ˜
            }

            response = requests.get(url, params=params, timeout=config.get("api.timeout", 30))
            response.raise_for_status()
            data = response.json()

            # æ—¥åˆ¥ã«é›†è¨ˆ
            daily = {}
            for item in data["list"]:
                date = item["dt_txt"].split(" ")[0]
                temp = item["main"]["temp"]
                weather = item["weather"][0]["description"]

                if date not in daily:
                    daily[date] = {"temps": [], "weather": weather}
                daily[date]["temps"].append(temp)

            # å¹³å‡æ°—æ¸©ã‚’è¨ˆç®—
            result = []
            for date, info in daily.items():
                avg_temp = round(sum(info["temps"]) / len(info["temps"]), 1)
                result.append({
                    "date"    : date,
                    "temp_avg": avg_temp,
                    "weather" : info["weather"]
                })

            return result

        except requests.exceptions.RequestException as e:
            st.error(f"âŒ äºˆå ±APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
            logger.error(f"Forecast API request error: {e}")
            return []
        except Exception as e:
            st.error(f"âŒ äºˆå ±ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            logger.error(f"Forecast data processing error: {e}")
            return []

# ==================================================
# FileSearchãƒ‡ãƒ¢
# ==================================================
# ä½œæˆ: POST /v1/vector_stores
# ä¸€è¦§å–å¾—: GET /v1/vector_stores
# è©³ç´°å–å¾—: GET /v1/vector_stores/{vector_store_id}
# æ›´æ–°: POST /v1/vector_stores/{vector_store_id}
# å‰Šé™¤: DELETE /v1/vector_stores/{vector_store_id}
# æ¤œç´¢: POST /v1/vector_stores/{vector_store_id}/search
# ==================================================
class FileSearchVectorStoreDemo(BaseDemo):
    """FileSearchå°‚ç”¨ãƒ‡ãƒ¢ï¼ˆæ­£ã—ã„OpenAI APIå¯¾å¿œç‰ˆï¼‰"""

    def __init__(self, demo_name: str):
        super().__init__(demo_name)
        self._vector_stores_cache = None
        self._cache_timestamp = None
        self._cache_ttl = 300  # 5åˆ†é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥

    @error_handler_ui
    @timer_ui
    def run(self):
        """ãƒ‡ãƒ¢ã®å®Ÿè¡Œï¼ˆæ­£ã—ã„APIå¯¾å¿œç‰ˆï¼‰"""
        self.initialize()
        st.header("FileSearchãƒ‡ãƒ¢")
        st.write(
            "ï¼ˆæ³¨ï¼‰Vector Storeã®ãƒ‡ãƒ¼ã‚¿ã¯ã€è‹±èªãªã®ã§ã€è³ªå•ã¯è‹±èªã®å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚"
            "ã‚»ãƒ¬ã‚¯ã‚¿ã§é¸æŠå¯èƒ½ã€‚responses.create()ã§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢ã‚’å®Ÿè¡Œã—ã€"
            "Vector Storeæ¤œç´¢APIã§ã®ç›´æ¥æ¤œç´¢ã‚‚å¯èƒ½ã€‚"
        )
        with st.expander("åˆ©ç”¨ï¼šOpenWeatherMap API(æ¯”è¼ƒç”¨)", expanded=False):
            st.code("""
            # FileSearchãƒ„ãƒ¼ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä½œæˆ
            fs_tool = FileSearchToolParam(
                type="file_search",
                vector_store_ids=[vector_store_id],
                max_num_results=max_results
            )
            # APIå‘¼ã³å‡ºã—
            response = self.call_api_unified(
                messages=[EasyInputMessageParam(role="user", content=query)],
                tools=[fs_tool],
                include=["file_search_call.results"]
            )
            
            # self.call_api_unified
            # APIå‘¼ã³å‡ºã—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æº–å‚™
            api_params = {
                "input": messages,
                "model": model
            }
            # responses.create ã‚’ä½¿ç”¨ï¼ˆçµ±ä¸€ã•ã‚ŒãŸAPIå‘¼ã³å‡ºã—ï¼‰
            
            return self.client.create_response(**api_params)
            """)

        # Vector Storeã®å–å¾—ã¨é¸æŠ
        vector_store_info = self._get_vector_store_selection()

        if not vector_store_info:
            st.warning("âš ï¸ åˆ©ç”¨å¯èƒ½ãªVector StoreãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            st.info("ğŸ’¡ OpenAI Playgroundã§Vector Storeã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚")
            return

        vector_store_id = vector_store_info["id"]
        vector_store_name = vector_store_info["name"]

        # é¸æŠã•ã‚ŒãŸVector Storeæƒ…å ±ã‚’è¡¨ç¤º
        with st.expander("ğŸ“‚ é¸æŠã•ã‚ŒãŸVector Storeæƒ…å ±", expanded=False):
            col1, col2, col3 = st.columns(3)
            with col1:
                st.write("**åå‰**:", vector_store_name)
                st.write("**ID**:", f"`{vector_store_id}`")

            with col2:
                if "file_counts" in vector_store_info:
                    file_counts = vector_store_info["file_counts"]
                    total_files = file_counts.get("total", 0)
                    completed_files = file_counts.get("completed", 0)
                    in_progress = file_counts.get("in_progress", 0)
                    failed = file_counts.get("failed", 0)

                    st.write("**ãƒ•ã‚¡ã‚¤ãƒ«æ•°**:", f"{completed_files}/{total_files}")
                    if in_progress > 0:
                        st.info(f"â³ å‡¦ç†ä¸­: {in_progress}ä»¶")
                    if failed > 0:
                        st.warning(f"âš ï¸ å¤±æ•—: {failed}ä»¶")

            with col3:
                if "created_at" in vector_store_info:
                    created_date = datetime.fromtimestamp(vector_store_info["created_at"]).strftime("%Y-%m-%d %H:%M")
                    st.write("**ä½œæˆæ—¥æ™‚**:", created_date)
                if "bytes" in vector_store_info:
                    bytes_size = vector_store_info["bytes"]
                    if bytes_size > 0:
                        mb_size = bytes_size / (1024 * 1024)
                        st.write("**å®¹é‡**:", f"{mb_size:.2f} MB")

        #
        with st.expander("ğŸ“‚ è‹±æ–‡-è³ªå•ä¾‹", expanded=False):
            st.code("""
customer_support_faq.csv ï¼šã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆãƒ»FAQãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    "How do I create a new account?",
    "What payment methods are available?",
    "Can I return a product?",
    "I forgot my password",
    "How can I contact the support team?"

sciq_qa.csv  ï¼šç§‘å­¦ãƒ»æŠ€è¡“QAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    "What are the latest trends in artificial intelligence?",
    "What is the principle of quantum computing?",
    "What are the types and characteristics of renewable energy?",
    "What are the current status and challenges of gene editing technology?",
    "What are the latest technologies in space exploration?"

medical_qa.csv   ï¼š åŒ»ç™‚è³ªå•å›ç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    "How to prevent high blood pressure?",
    "What are the symptoms and treatment of diabetes?",
    "What are the risk factors for heart disease?",
    "What are the guidelines for healthy eating?",
    "What is the relationship between exercise and health?"

legal_qa.csv  ï¼šæ³•å¾‹ãƒ»åˆ¤ä¾‹QAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    "What are the important clauses in contracts?",
    "How to protect intellectual property rights?",
    "What are the basic principles of labor law?",
    "What is an overview of personal data protection law?",
    "What is the scope of application of consumer protection law?"
            """)

        st.write(
            "AIæ¤œç´¢ã§å›ç­”ãŒå¾—ã‚‰ã‚Œãªã„å›ºæœ‰ã®æƒ…å ±ã«ã‚‚[Vector Store]ã‹ã‚‰æ¤œç´¢ã§ãã¾ã™ã€‚"
            "Vector Storeã®ã‚¹ã‚³ã‚¢ãŒ0.8ä»¥ä¸Šã¯ã‹ãªã‚Šè‰¯ã„ã€0.6ä»¥ä¸ŠãŒè‰¯ã„ã§ã™ã€‚"
        )
        # æ¤œç´¢ã‚¿ãƒ–
        tab1, tab2 = st.tabs(["ğŸ¤– AIæ¤œç´¢ (Responses API)", "ğŸ” ç›´æ¥æ¤œç´¢ (Vector Store API)"])


        with tab1:
            self._run_responses_search(vector_store_id)

        with tab2:
            self._run_direct_search(vector_store_id)

        # Vector Storeä¸€è¦§æ›´æ–°
        col1, col2 = st.columns([3, 1])
        with col2:
            if st.button("ğŸ”„ Vector Storeä¸€è¦§æ›´æ–°", key=f"refresh_{self.safe_key}"):
                self._clear_vector_stores_cache()
                st.rerun()

    def _run_responses_search(self, vector_store_id: str):
        """Responses APIã‚’ä½¿ç”¨ã—ãŸFileSearch"""
        st.subheader("ğŸ¤– AIæ¤œç´¢ (Responses API)")
        st.write("Responses APIã®file_searchãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ãŸAIå›ç­”ä»˜ãæ¤œç´¢")

        # æ¤œç´¢ã‚¯ã‚¨ãƒªå…¥åŠ›
        query = st.text_input(
            "ğŸ” æ¤œç´¢ã‚¯ã‚¨ãƒª",
            value="When is the payment deadline for the invoice? return policy?",
            help="Vector Storeå†…ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰æ¤œç´¢ã—ãŸã„å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
            key=f"ai_search_query_{self.safe_key}"
        )

        # æ¤œç´¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        with st.expander("ğŸ”§ æ¤œç´¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³", expanded=False):
            max_results = st.slider(
                "æœ€å¤§æ¤œç´¢çµæœæ•°",
                min_value=1,
                max_value=20,
                value=5,
                help="æ¤œç´¢ã§å–å¾—ã™ã‚‹æœ€å¤§çµæœæ•°"
            )

        # FileSearchå®Ÿè¡Œ
        if st.button("ğŸš€ AIæ¤œç´¢å®Ÿè¡Œ", key=f"ai_search_exec_{self.safe_key}", use_container_width=True):
            if query.strip():
                self._execute_ai_search(vector_store_id, query, max_results)
            else:
                st.error("âŒ æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    def _run_direct_search(self, vector_store_id: str):
        """Vector Store APIã‚’ä½¿ç”¨ã—ãŸç›´æ¥æ¤œç´¢"""
        st.subheader("ğŸ” ç›´æ¥æ¤œç´¢ (Vector Store API)")
        st.write("Vector Store APIã®æ¤œç´¢æ©Ÿèƒ½ã‚’ç›´æ¥ä½¿ç”¨ã—ãŸæ¤œç´¢")

        # æ¤œç´¢ã‚¯ã‚¨ãƒªå…¥åŠ›
        query = st.text_input(
            "ğŸ” æ¤œç´¢ã‚¯ã‚¨ãƒª",
            value="When is the payment deadline for the invoice? return policy?",
            help="Vector Storeå†…ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰æ¤œç´¢ã—ãŸã„å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
            key=f"direct_search_query_{self.safe_key}"
        )

        # æ¤œç´¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        with st.expander("ğŸ”§ è©³ç´°æ¤œç´¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³", expanded=False):
            col1, col2 = st.columns(2)
            with col1:
                max_results = st.slider(
                    "æœ€å¤§æ¤œç´¢çµæœæ•°",
                    min_value=1,
                    max_value=50,
                    value=10,
                    help="æ¤œç´¢ã§å–å¾—ã™ã‚‹æœ€å¤§çµæœæ•°ï¼ˆ1-50ï¼‰"
                )
                rewrite_query = st.checkbox(
                    "ã‚¯ã‚¨ãƒªæ›¸ãæ›ãˆ",
                    value=False,
                    help="è‡ªç„¶è¨€èªã‚¯ã‚¨ãƒªã‚’ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ç”¨ã«æ›¸ãæ›ãˆã‚‹"
                )
            with col2:
                # ãƒ•ã‚£ãƒ«ã‚¿ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆå°†æ¥ã®æ‹¡å¼µç”¨ï¼‰
                st.write("**ãƒ•ã‚£ãƒ«ã‚¿ã‚ªãƒ—ã‚·ãƒ§ãƒ³**")
                st.info("ä»Šå¾Œã®ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã§è¿½åŠ äºˆå®š")

        # ç›´æ¥æ¤œç´¢å®Ÿè¡Œ
        if st.button("ğŸ” ç›´æ¥æ¤œç´¢å®Ÿè¡Œ", key=f"direct_search_exec_{self.safe_key}", use_container_width=True):
            if query.strip():
                self._execute_direct_search(vector_store_id, query, max_results, rewrite_query)
            else:
                st.error("âŒ æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    def _get_vector_stores(self) -> List[Dict[str, Any]]:
        """æ­£ã—ã„OpenAI APIã‹ã‚‰Vector Storeã®ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if (self._vector_stores_cache is not None and
                self._cache_timestamp is not None and
                time.time() - self._cache_timestamp < self._cache_ttl):
            return self._vector_stores_cache

        try:
            with st.spinner("ğŸ”„ Vector Storeä¸€è¦§ã‚’å–å¾—ä¸­..."):
                # ã‚ˆã‚Šæ˜ç¢ºãªã‚¢ã‚¯ã‚»ã‚¹æ–¹æ³•
                openai_client = self.client.client
                response = openai_client.vector_stores.list(
                    limit=20,
                    order="desc"  # æ–°ã—ã„é †ã«å–å¾—
                )

                vector_stores = []
                # æœ€æ–°ã®4ã¤ã®ã¿ã‚’å‡¦ç†
                for vs in response.data[:4]:
                    # file_countsã¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãªã®ã§å±æ€§ã¨ã—ã¦ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹
                    file_counts_obj = getattr(vs, 'file_counts', None)
                    if file_counts_obj:
                        file_counts_dict = {
                            "total"      : getattr(file_counts_obj, 'total', 0),
                            "completed"  : getattr(file_counts_obj, 'completed', 0),
                            "failed"     : getattr(file_counts_obj, 'failed', 0),
                            "in_progress": getattr(file_counts_obj, 'in_progress', 0),
                            "cancelled"  : getattr(file_counts_obj, 'cancelled', 0)
                        }
                    else:
                        file_counts_dict = {
                            "total"      : 0,
                            "completed"  : 0,
                            "failed"     : 0,
                            "in_progress": 0,
                            "cancelled"  : 0
                        }

                    vector_store_info = {
                        "id"         : vs.id,
                        "name"       : vs.name or f"Vector Store {vs.id[:8]}",
                        "created_at" : vs.created_at,
                        "bytes"      : getattr(vs, 'bytes', 0),
                        "file_counts": file_counts_dict,
                        "object"     : getattr(vs, 'object', 'vector_store'),
                    }
                    vector_stores.append(vector_store_info)

                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°
                self._vector_stores_cache = vector_stores
                self._cache_timestamp = time.time()

                logger.info(f"å–å¾—ã—ãŸVector Storeæ•°: {len(vector_stores)}")
                return vector_stores

        except Exception as e:
            logger.error(f"Vector Storeå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            error_message = str(e)

            # å…·ä½“çš„ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è¡¨ç¤º
            if "authentication" in error_message.lower():
                st.error("ğŸ” èªè¨¼ã‚¨ãƒ©ãƒ¼: OpenAI APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            elif "rate limit" in error_message.lower():
                st.error("â±ï¸ ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼: ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚")
            elif "permission" in error_message.lower() or "forbidden" in error_message.lower():
                st.error("ğŸš« æ¨©é™ã‚¨ãƒ©ãƒ¼: Vector Store APIã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            else:
                st.error(f"âŒ Vector Storeã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {error_message}")

            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®Vector Store
            st.info("ğŸ’¡ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®Vector Storeã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            fallback_stores = [{
                "id"         : "vs_68345a403a548191817b3da8404e2d82",
                "name"       : "ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ Vector Store (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)",
                "created_at" : time.time(),
                "bytes"      : 0,
                "file_counts": {"total": "ä¸æ˜", "completed": "ä¸æ˜", "failed": 0, "in_progress": 0}
            }]
            return fallback_stores

    def _clear_vector_stores_cache(self):
        """Vector Storeã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        self._vector_stores_cache = None
        self._cache_timestamp = None
        st.success("âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")

    def _get_vector_store_selection(self) -> Optional[Dict[str, Any]]:
        """Vector Storeé¸æŠUIã¨ãƒ‡ãƒ¼ã‚¿å–å¾—"""
        vector_stores = self._get_vector_stores()

        if not vector_stores:
            return None

        # ã‚»ãƒ¬ã‚¯ã‚¿ç”¨ã®é¸æŠè‚¢ã‚’ä½œæˆ
        options = []
        for vs in vector_stores:
            # ãƒ•ã‚¡ã‚¤ãƒ«æ•°æƒ…å ±
            file_counts = vs.get('file_counts', {})
            total_files = file_counts.get('total', 0)
            completed_files = file_counts.get('completed', 0)
            file_info = f"({completed_files}/{total_files} files)"

            # å®¹é‡æƒ…å ±
            bytes_size = vs.get('bytes', 0)
            if bytes_size > 0:
                mb_size = bytes_size / (1024 * 1024)
                size_info = f" | {mb_size:.1f}MB"
            else:
                size_info = ""

            option_text = f"ğŸ“‚ {vs['name']} - {vs['id'][:20]}... {file_info}{size_info}"
            options.append(option_text)

        # Vector Storeé¸æŠ
        selected_index = st.selectbox(
            "ğŸ“‚ Vector Storeã‚’é¸æŠ",
            range(len(options)),
            format_func=lambda x: options[x],
            key=f"vs_select_{self.safe_key}",
            help="æ¤œç´¢å¯¾è±¡ã®Vector Storeã‚’é¸æŠã—ã¦ãã ã•ã„"
        )

        return vector_stores[selected_index]

    def _execute_ai_search(self, vector_store_id: str, query: str, max_results: int = 5):
        """Responses APIã‚’ä½¿ç”¨ã—ãŸAIæ¤œç´¢ã®å®Ÿè¡Œ"""
        try:
            # FileSearchãƒ„ãƒ¼ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä½œæˆ
            fs_tool = FileSearchToolParam(
                type="file_search",
                vector_store_ids=[vector_store_id],
                max_num_results=max_results
            )

            # å®Ÿè¡Œæ™‚é–“ã‚’æ¸¬å®š
            start_time = time.time()

            with st.spinner("ğŸ¤– AIæ¤œç´¢ä¸­..."):
                # APIå‘¼ã³å‡ºã—
                response = self.call_api_unified(
                    messages=[EasyInputMessageParam(role="user", content=query)],
                    tools=[fs_tool],
                    include=["file_search_call.results"]
                )

            execution_time = time.time() - start_time

            # çµæœè¡¨ç¤º
            st.success(f"âœ… AIæ¤œç´¢å®Œäº† ({execution_time:.2f}ç§’)")

            # ãƒ¡ã‚¤ãƒ³å›ç­”ã®è¡¨ç¤º
            st.subheader("ğŸ¤– AIå›ç­”")
            ResponseProcessorUI.display_response(response, show_details=False)

            # FileSearchè©³ç´°çµæœã®è¡¨ç¤º
            if hasattr(response, "file_search_call") and response.file_search_call:
                with st.expander("ğŸ“„ FileSearchè©³ç´°çµæœ", expanded=True):
                    if hasattr(response.file_search_call, "results") and response.file_search_call.results:
                        self._display_ai_search_results(response.file_search_call.results)
                    else:
                        st.info("â„¹ï¸ è©³ç´°ãªæ¤œç´¢çµæœãŒè¿”ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            else:
                st.info("â„¹ï¸ FileSearchå‘¼ã³å‡ºã—çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±
            self._show_performance_info(response, execution_time, vector_store_id, max_results)

        except Exception as e:
            self.handle_error(e)
            logger.error(f"AIæ¤œç´¢å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")

    def _execute_direct_search(self, vector_store_id: str, query: str, max_results: int = 10,
                               rewrite_query: bool = False):
        """Vector Store APIã‚’ä½¿ç”¨ã—ãŸç›´æ¥æ¤œç´¢ã®å®Ÿè¡Œ"""
        try:
            start_time = time.time()

            with st.spinner("ğŸ” ç›´æ¥æ¤œç´¢ä¸­..."):
                # ã‚ˆã‚Šæ˜ç¢ºãªã‚¢ã‚¯ã‚»ã‚¹æ–¹æ³•
                openai_client = self.client.client
                search_response = openai_client.vector_stores.search(
                    vector_store_id=vector_store_id,
                    query=query,
                    max_num_results=max_results,
                    rewrite_query=rewrite_query
                )

            execution_time = time.time() - start_time

            # çµæœè¡¨ç¤º
            st.success(f"âœ… ç›´æ¥æ¤œç´¢å®Œäº† ({execution_time:.2f}ç§’)")

            # æ¤œç´¢æƒ…å ±ã®è¡¨ç¤º
            st.subheader("ğŸ” æ¤œç´¢æƒ…å ±")
            col1, col2 = st.columns(2)
            with col1:
                st.write(f"**æ¤œç´¢ã‚¯ã‚¨ãƒª**: {getattr(search_response, 'search_query', query)}")
                st.write(f"**çµæœæ•°**: {len(search_response.data)}ä»¶")
            with col2:
                st.write(f"**å®Ÿè¡Œæ™‚é–“**: {execution_time:.2f}ç§’")
                st.write(f"**æ¬¡ãƒšãƒ¼ã‚¸æœ‰ç„¡**: {'æœ‰ã‚Š' if getattr(search_response, 'has_more', False) else 'ç„¡ã—'}")

            # ç›´æ¥æ¤œç´¢çµæœã®è¡¨ç¤º
            self._display_direct_search_results(search_response.data)

        except Exception as e:
            self.handle_error(e)
            logger.error(f"ç›´æ¥æ¤œç´¢å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")

    def _display_ai_search_results(self, results: List[Any]):
        """AIæ¤œç´¢çµæœã®è¡¨ç¤º"""
        try:
            if not results:
                st.info("ğŸ” æ¤œç´¢çµæœãŒã‚ã‚Šã¾ã›ã‚“")
                return

            st.write(f"**æ¤œç´¢çµæœä»¶æ•°**: {len(results)}ä»¶")

            for i, result in enumerate(results, 1):
                with st.expander(f"ğŸ“„ çµæœ {i}", expanded=i <= 2):
                    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è¡¨ç¤º
                    if hasattr(result, 'content'):
                        content = result.content
                        st.write("**å†…å®¹**:")
                        if len(content) > 500:
                            st.text_area(
                                "æ¤œç´¢çµæœå†…å®¹",
                                content,
                                height=150,
                                key=f"ai_content_{i}_{self.safe_key}",
                                help="æ¤œç´¢ã§ãƒ’ãƒƒãƒˆã—ãŸæ–‡æ›¸ã®å†…å®¹"
                            )
                        else:
                            st.markdown(f"> {content}")

                    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æƒ…å ±
                    col1, col2 = st.columns(2)
                    with col1:
                        if hasattr(result, 'file_name'):
                            st.write(f"**ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«å**: {result.file_name}")
                        if hasattr(result, 'file_id'):
                            st.write(f"**ğŸ†” ãƒ•ã‚¡ã‚¤ãƒ«ID**: {result.file_id}")
                    with col2:
                        if hasattr(result, 'score'):
                            st.write(f"**ğŸ¯ é–¢é€£åº¦**: {result.score:.4f}")

                    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
                    if config.get("experimental.debug_mode", False):
                        with st.expander("ğŸ”§ Raw Data", expanded=False):
                            safe_streamlit_json(result)

        except Exception as e:
            st.error(f"âŒ AIæ¤œç´¢çµæœè¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}")

    def _display_direct_search_results(self, results: List[Any]):
        """ç›´æ¥æ¤œç´¢çµæœã®è¡¨ç¤º"""
        try:
            if not results:
                st.info("ğŸ” æ¤œç´¢çµæœãŒã‚ã‚Šã¾ã›ã‚“")
                return

            st.subheader("ğŸ“‹ æ¤œç´¢çµæœ")

            for i, result in enumerate(results, 1):
                # ã‚¹ã‚³ã‚¢ã«ã‚ˆã‚‹è‰²åˆ†ã‘
                score = getattr(result, 'score', 0)
                if score >= 0.9:
                    score_color = "ğŸŸ¢"  # é«˜é–¢é€£åº¦
                elif score >= 0.7:
                    score_color = "ğŸŸ¡"  # ä¸­é–¢é€£åº¦
                else:
                    score_color = "ğŸ”´"  # ä½é–¢é€£åº¦

                with st.expander(f"{score_color} çµæœ {i} (ã‚¹ã‚³ã‚¢: {score:.3f})", expanded=i <= 3):
                    # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
                    col1, col2 = st.columns(2)
                    with col1:
                        if hasattr(result, 'filename'):
                            st.write(f"**ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«å**: {result.filename}")
                        if hasattr(result, 'file_id'):
                            st.write(f"**ğŸ†” ãƒ•ã‚¡ã‚¤ãƒ«ID**: {result.file_id}")
                    with col2:
                        st.write(f"**ğŸ¯ ã‚¹ã‚³ã‚¢**: {score:.4f}")

                    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„è¡¨ç¤º
                    if hasattr(result, 'content') and result.content:
                        st.write("**ğŸ“ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„**:")
                        for content_item in result.content:
                            if hasattr(content_item, 'type') and content_item.type == "text":
                                text_content = getattr(content_item, 'text', '')
                                if len(text_content) > 500:
                                    st.text_area(
                                        "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„",
                                        text_content,
                                        height=150,
                                        key=f"direct_content_{i}_{self.safe_key}"
                                    )
                                else:
                                    st.markdown(f"> {text_content}")

                    # å±æ€§æƒ…å ±
                    if hasattr(result, 'attributes') and result.attributes:
                        st.write("**ğŸ·ï¸ å±æ€§**:")
                        for key, value in result.attributes.items():
                            st.write(f"- **{key}**: {value}")

                    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
                    if config.get("experimental.debug_mode", False):
                        with st.expander("ğŸ”§ Raw Data", expanded=False):
                            safe_streamlit_json(result)

        except Exception as e:
            st.error(f"âŒ ç›´æ¥æ¤œç´¢çµæœè¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}")

    def _show_performance_info(self, response: Any, execution_time: float, vector_store_id: str, max_results: int):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±ã®è¡¨ç¤º"""
        with st.expander("ğŸ“Š å®Ÿè¡Œæƒ…å ±", expanded=False):
            col1, col2 = st.columns(2)

            with col1:
                st.write("**å®Ÿè¡Œè©³ç´°**")
                st.write(f"- å®Ÿè¡Œæ™‚é–“: {execution_time:.2f}ç§’")
                st.write(f"- Vector Store ID: `{vector_store_id}`")
                st.write(f"- æœ€å¤§æ¤œç´¢çµæœæ•°: {max_results}")
                st.write(f"- ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {self.get_model()}")

            with col2:
                if hasattr(response, 'usage') and response.usage:
                    st.write("**ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡**")
                    usage_data = ResponseProcessor._serialize_usage(response.usage)

                    prompt_tokens = usage_data.get('prompt_tokens', 0)
                    completion_tokens = usage_data.get('completion_tokens', 0)
                    total_tokens = usage_data.get('total_tokens', 0)

                    st.write(f"- å…¥åŠ›: {prompt_tokens:,} tokens")
                    st.write(f"- å‡ºåŠ›: {completion_tokens:,} tokens")
                    st.write(f"- åˆè¨ˆ: {total_tokens:,} tokens")

                    # ã‚³ã‚¹ãƒˆè¨ˆç®—
                    model = self.get_model()
                    cost = TokenManager.estimate_cost(prompt_tokens, completion_tokens, model)
                    st.write(f"- **æ¨å®šã‚³ã‚¹ãƒˆ**: ${cost:.6f}")

    def show_debug_info(self):
        """ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®è¡¨ç¤ºï¼ˆæ‹¡å¼µç‰ˆï¼‰"""
        super().show_debug_info()

        if config.get("experimental.debug_mode", False):
            with st.sidebar.expander("ğŸ” FileSearch Debug", expanded=False):
                st.write("**APIæƒ…å ±**")
                st.write("- ä½¿ç”¨API: Vector Stores API")
                st.write("- ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ: /v1/vector_stores")

                st.write("**ã‚­ãƒ£ãƒƒã‚·ãƒ¥çŠ¶æ…‹**")
                if self._vector_stores_cache:
                    st.write(f"- ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸVector Storeæ•°: {len(self._vector_stores_cache)}")
                    if self._cache_timestamp:
                        cache_age = time.time() - self._cache_timestamp
                        st.write(f"- ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµŒéæ™‚é–“: {cache_age:.1f}ç§’")
                else:
                    st.write("- ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—")

                if st.button("ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¼·åˆ¶ã‚¯ãƒªã‚¢", key="debug_clear_cache"):
                    self._clear_vector_stores_cache()

# ==================================================
# WebSearch Toolsãƒ‡ãƒ¢
# ==================================================
class WebSearchToolsDemo(BaseDemo):
    """WebSearchå°‚ç”¨ãƒ‡ãƒ¢ï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰"""

    @error_handler_ui
    @timer_ui
    def run(self):
        """ãƒ‡ãƒ¢ã®å®Ÿè¡Œï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰"""
        self.initialize()
        st.header("WebSearch Toolsãƒ‡ãƒ¢ã€€APIæƒ…å ±")
        with st.expander("åˆ©ç”¨ï¼šWebSearch Toolsãƒ‡ãƒ¢", expanded=False):
            st.code("""
            user_location = UserLocation(
                type="approximate",
                country="JP",
                city="Tokyo",
                region="Tokyo"
            )

            ws_tool = WebSearchToolParam(
                type="web_search_preview",
                user_location=user_location,
                search_context_size=context_size
            )
            
            default_query = config.get("samples.prompts.weather_query",
                                   "é€±æœ«ã®æ±äº¬ã®æ–°å®¿ã®å¤©æ°—ã¨ãŠã™ã™ã‚ã®å±‹å†…ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ã¯ï¼Ÿ")
            query = st.text_input("æ¤œç´¢ã‚¯ã‚¨ãƒª", value=default_query)
            
            response = self.call_api_unified(
                    messages=[EasyInputMessageParam(role="user", content=query)],
                    tools=[ws_tool]
                )
                â”—# APIå‘¼ã³å‡ºã—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æº–å‚™
                api_params = {
                    "input": messages,
                    "model": model
                }
                self.client.create_response(**api_params)
                
            ResponseProcessorUI.display_response(response)
                
            """)

        st.write(
            "WebSearchãƒ„ãƒ¼ãƒ«å°‚ç”¨ãƒ‡ãƒ¢ã€‚WebSearchToolParamã§åœ°åŸŸè¨­å®šãƒ»æ¤œç´¢ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚ºã‚’æŒ‡å®šã—ã€responses.create()ã§Webæ¤œç´¢ã‚’å®Ÿè¡Œã€‚æ—¥æœ¬ã®æ±äº¬åœ°åŸŸè¨­å®šã§å®Ÿç”¨çš„ãªæ¤œç´¢æ©Ÿèƒ½ã‚’å®Ÿè£…ã€‚")

        default_query = config.get("samples.prompts.weather_query",
                                   "é€±æœ«ã®æ±äº¬ã®æ–°å®¿ã®å¤©æ°—ã¨ãŠã™ã™ã‚ã®å±‹å†…ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ã¯ï¼Ÿ")
        query = st.text_input("æ¤œç´¢ã‚¯ã‚¨ãƒª", value=default_query)

        # Literalå‹ã®åˆ¶ç´„ã«å¯¾å¿œ
        context_size: Literal["low", "medium", "high"] = st.selectbox(
            "æ¤œç´¢ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚º",
            ["low", "medium", "high"],
            index=1,
            key=f"ws_context_{self.safe_key}"
        )

        if st.button("WebSearchå®Ÿè¡Œ", key=f"ws_exec_{self.safe_key}"):
            self._execute_web_search(query, context_size)

    def _execute_web_search(self, query: str, context_size: Literal["low", "medium", "high"]):
        """WebSearchã®å®Ÿè¡Œï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰"""
        try:
            user_location = UserLocation(
                type="approximate",
                country="JP",
                city="Tokyo",
                region="Tokyo"
            )

            ws_tool = WebSearchToolParam(
                type="web_search_preview",
                user_location=user_location,
                search_context_size=context_size
            )

            with st.spinner("æ¤œç´¢ä¸­..."):
                response = self.call_api_unified(
                    messages=[EasyInputMessageParam(role="user", content=query)],
                    tools=[ws_tool]
                )

            st.subheader("æ¤œç´¢çµæœ")
            ResponseProcessorUI.display_response(response)

        except Exception as e:
            self.handle_error(e)


# ==================================================
# Computer Useãƒ‡ãƒ¢ï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰
# ==================================================
class ComputerUseDemo(BaseDemo):
    """Computer Use Tool ã®ãƒ‡ãƒ¢ï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰"""

    @error_handler_ui
    @timer_ui
    def run(self):
        """ãƒ‡ãƒ¢ã®å®Ÿè¡Œï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰"""
        self.initialize()
        st.header("Computer Useãƒ‡ãƒ¢")
        st.write("åˆ©ç”¨ï¼šOpenAI API")
        st.warning("Computer Use APIã¯å®Ÿé¨“çš„ãªæ©Ÿèƒ½ã§ã™ã€‚å®Ÿè¡Œã«ã¯ç‰¹åˆ¥ãªæ¨©é™ãŒå¿…è¦ã§ã™ã€‚")

        model = "computer-use-preview"
        st.write("ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«:", model)

        instruction = st.text_area(
            "å®Ÿè¡ŒæŒ‡ç¤º",
            value="ãƒ–ãƒ©ã‚¦ã‚¶ã§ https://news.ycombinator.com ã‚’é–‹ã„ã¦ã€"
                  "ãƒˆãƒƒãƒ—è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ãƒ¡ãƒ¢å¸³ã«è²¼ã‚Šä»˜ã‘ã¦",
            height=100
        )

        # Literalå‹ã®åˆ¶ç´„ã«å¯¾å¿œ
        environment: Literal["browser", "mac", "windows", "ubuntu", "linux"] = st.selectbox(
            "å®Ÿè¡Œç’°å¢ƒ",
            ["browser", "mac", "windows", "ubuntu"],
            key=f"cu_env_{self.safe_key}"
        )

        if st.button("Computer Useå®Ÿè¡Œ", key=f"cu_exec_{self.safe_key}"):
            self._execute_computer_use(model, instruction, environment)

    def _execute_computer_use(self, model: str, instruction: str,
                              environment: Literal["windows", "mac", "linux", "ubuntu", "browser"]):
        """Computer Useã®å®Ÿè¡Œï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰"""
        try:
            cu_tool = ComputerToolParam(
                type="computer_use_preview",
                display_width=1280,
                display_height=800,
                environment=environment,
            )

            messages = [
                EasyInputMessageParam(
                    role="user",
                    content=[
                        ResponseInputTextParam(
                            type="input_text",
                            text=instruction
                        )
                    ]
                )
            ]

            with st.spinner("å®Ÿè¡Œä¸­..."):
                response = self.call_api_unified(
                    messages=messages,
                    model=model,
                    tools=[cu_tool],
                    truncation="auto",
                    stream=False,
                    include=["computer_call_output.output.image_url"]
                )

            st.subheader("å®Ÿè¡Œçµæœ")
            ResponseProcessorUI.display_response(response)

            # Computer Useç‰¹æœ‰ã®å‡ºåŠ›å‡¦ç†
            for output in response.output:
                if hasattr(output, 'type') and output.type == 'computer_call':
                    st.subheader("Computer Use ã‚¢ã‚¯ã‚·ãƒ§ãƒ³")
                    if hasattr(output, 'action'):
                        st.write('å®Ÿè¡Œã‚¢ã‚¯ã‚·ãƒ§ãƒ³:', output.action)
                    if hasattr(output, 'image_url'):
                        st.image(output.image_url, caption="ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ")

        except Exception as e:
            self.handle_error(e)

# ==================================================
# ãƒ‡ãƒ¢ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
# ==================================================
class DemoManager:
    """ãƒ‡ãƒ¢ã®ç®¡ç†ã‚¯ãƒ©ã‚¹ï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰"""

    def __init__(self):
        self.config = ConfigManager("config.yml")
        self.demos = self._initialize_demos()

    def _initialize_demos(self) -> Dict[str, BaseDemo]:
        """ãƒ‡ãƒ¢ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®åˆæœŸåŒ–ï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰"""
        return {
            "Text Responses (One Shot)"  : TextResponseDemo("Text Responses(one shot)"),
            "Text Responses (Memory)"    : MemoryResponseDemo("Text Responses(memory)"),
            "Image to Text ç”»åƒå…¥åŠ›(URL)"   : ImageResponseDemo("Image_URL", use_base64=False),
            "Image to Text ç”»åƒå…¥åŠ›(base64)": ImageResponseDemo("Image_Base64", use_base64=True),
            "Structured Output æ§‹é€ åŒ–å‡ºåŠ›" : StructuredOutputDemo("Structured_Output_create", use_parse=False),
            "Open Weather API" : WeatherDemo("OpenWeatherAPI"),
            "File Search-Tool vector store": FileSearchVectorStoreDemo("FileSearch_vsid"),
            "Tools - Web Search Tools"     : WebSearchToolsDemo("WebSearch"),
            "Computer Use Tool Param"      : ComputerUseDemo("Computer_Use"),
        }

    @error_handler_ui
    @timer_ui
    def run(self):
        """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å®Ÿè¡Œï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰"""
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–ï¼ˆçµ±ä¸€åŒ–ï¼‰
        SessionStateManager.init_session_state()

        # ãƒ‡ãƒ¢é¸æŠ
        demo_name = st.sidebar.radio(
            "ãƒ‡ãƒ¢ã‚’é¸æŠ",
            list(self.demos.keys()),
            key="demo_selection"
        )

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®æ›´æ–°
        if "current_demo" not in st.session_state:
            st.session_state.current_demo = demo_name
        elif st.session_state.current_demo != demo_name:
            st.session_state.current_demo = demo_name

        # é¸æŠã•ã‚ŒãŸãƒ‡ãƒ¢ã®å®Ÿè¡Œ
        demo = self.demos.get(demo_name)
        if demo:
            demo.run()
        else:
            st.error(f"ãƒ‡ãƒ¢ '{demo_name}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        # ãƒ•ãƒƒã‚¿ãƒ¼æƒ…å ±ï¼ˆçµ±ä¸€åŒ–ï¼‰
        self._display_footer()

    def _display_footer(self):
        """ãƒ•ãƒƒã‚¿ãƒ¼æƒ…å ±ã®è¡¨ç¤ºï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰"""
        st.sidebar.markdown("---")
        st.sidebar.markdown("### æƒ…å ±")

        # ç¾åœ¨ã®è¨­å®šæƒ…å ±
        with st.sidebar.expander("ç¾åœ¨ã®è¨­å®š"):
            safe_streamlit_json({
                "default_model": config.get("models.default"),
                "api_timeout"  : config.get("api.timeout"),
                "ui_layout"    : config.get("ui.layout"),
            })

        # ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±
        st.sidebar.markdown("### ãƒãƒ¼ã‚¸ãƒ§ãƒ³")
        st.sidebar.markdown("- OpenAI Responses API Demo v3.0 (çµ±ä¸€åŒ–ç‰ˆ)")
        st.sidebar.markdown("- Streamlit " + st.__version__)

        # ãƒªãƒ³ã‚¯
        st.sidebar.markdown("### ãƒªãƒ³ã‚¯")
        st.sidebar.markdown("[OpenAI API ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://platform.openai.com/docs)")
        st.sidebar.markdown("[Streamlit ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://docs.streamlit.io)")

        # çµ±è¨ˆæƒ…å ±
        with st.sidebar.expander("ğŸ“Š çµ±è¨ˆæƒ…å ±"):
            st.metric("åˆ©ç”¨å¯èƒ½ãƒ‡ãƒ¢æ•°", len(self.demos))
            st.metric("ç¾åœ¨ã®ãƒ‡ãƒ¢", st.session_state.get("current_demo", "æœªé¸æŠ"))


# ==================================================
# ãƒ¡ã‚¤ãƒ³é–¢æ•°
# ==================================================
def main():
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆï¼ˆçµ±ä¸€åŒ–ç‰ˆï¼‰"""

    try:
        # ãƒ­ã‚®ãƒ³ã‚°ã®è¨­å®š
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )

        # ç’°å¢ƒå¤‰æ•°ã®ãƒã‚§ãƒƒã‚¯
        if not os.getenv("OPENAI_API_KEY"):
            st.error("ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            st.info("export OPENAI_API_KEY='your-api-key' ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            st.stop()

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–ï¼ˆçµ±ä¸€åŒ–ï¼‰
        SessionStateManager.init_session_state()

        # ãƒ‡ãƒ¢ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®ä½œæˆã¨å®Ÿè¡Œ
        manager = DemoManager()
        manager.run()

    except Exception as e:
        st.error(f"ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®èµ·å‹•ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
        logger.error(f"Application startup error: {e}")

        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®è¡¨ç¤º
        if config.get("experimental.debug_mode", False):
            with st.expander("ğŸ”§ è©³ç´°ã‚¨ãƒ©ãƒ¼æƒ…å ±", expanded=False):
                st.exception(e)


if __name__ == "__main__":
    main()

# streamlit run a10_00_responses_api.py --server.port=8510

