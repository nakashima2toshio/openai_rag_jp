#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
a01_load_set_rag_data_trivia.py - TriviaQAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå°‚ç”¨å‡¦ç†ãƒ„ãƒ¼ãƒ«
====================================================================
èµ·å‹•: python a01_load_set_rag_data_trivia.py

ã€ä¸»è¦æ©Ÿèƒ½ã€‘
âœ… HuggingFaceã‹ã‚‰TriviaQAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
âœ… ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ã¨å‰å‡¦ç†
âœ… RAGç”¨ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
âœ… datasets/trivia_qa.csv ã«ä¿å­˜

ã€TriviaQAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¤ã„ã¦ã€‘
- ä¸€èˆ¬çš„ãªé›‘å­¦çŸ¥è­˜ã«é–¢ã™ã‚‹è³ªå•å¿œç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
- è³ªå•ã€å›ç­”ã€ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ï¼ˆæ ¹æ‹ ã¨ãªã‚‹æ–‡ç« ï¼‰ã‚’å«ã‚€
- Wikipediaã‚„Webãƒšãƒ¼ã‚¸ã‹ã‚‰ã®ã‚½ãƒ¼ã‚¹æƒ…å ±ä»˜ã
"""

import pandas as pd
import json
from datetime import datetime
from pathlib import Path
import logging
from typing import Dict, Optional
from datasets import load_dataset

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class TriviaQAProcessor:
    """TriviaQAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‡¦ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.dataset_name = "trivia_qa"
        self.output_dir = Path("datasets")
        self.output_dir.mkdir(exist_ok=True)
        self.output_file = self.output_dir / "trivia_qa.csv"
        
    def download_dataset(self, subset: str = "rc", split: str = "train", sample_size: int = 1000) -> pd.DataFrame:
        """
        HuggingFaceã‹ã‚‰TriviaQAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        
        Args:
            subset: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µãƒ–ã‚»ãƒƒãƒˆ ("rc" or "unfiltered")
                - rc: Reading Comprehension (ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä»˜ã)
                - unfiltered: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãªã—ã®å®Œå…¨ç‰ˆ
            split: ãƒ‡ãƒ¼ã‚¿åˆ†å‰² ("train", "validation", "test")
            sample_size: å–å¾—ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°
            
        Returns:
            pd.DataFrame: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ‡ãƒ¼ã‚¿
        """
        try:
            logger.info(f"TriviaQAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­... (subset={subset}, split={split})")
            
            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰
            dataset = load_dataset("trivia_qa", subset, split=split)
            
            # ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’åˆ¶é™
            if len(dataset) > sample_size:
                dataset = dataset.select(range(sample_size))
            
            # DataFrameã«å¤‰æ›
            data_list = []
            for item in dataset:
                # TriviaQAã®æ§‹é€ ã«åŸºã¥ã„ã¦ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                record = {
                    'question_id': item.get('question_id', ''),
                    'question': item.get('question', ''),
                    'answer': self._extract_answer(item.get('answer', {})),
                    'entity_pages': self._extract_entity_pages(item.get('entity_pages', [])),
                    'search_results': self._extract_search_results(item.get('search_results', [])),
                }
                data_list.append(record)
            
            df = pd.DataFrame(data_list)
            
            logger.info(f"âœ… {len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸ")
            logger.info(f"ã‚«ãƒ©ãƒ : {df.columns.tolist()}")
            
            return df
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            raise
    
    def _extract_answer(self, answer_dict: Dict) -> str:
        """å›ç­”æƒ…å ±ã‚’æŠ½å‡º"""
        if isinstance(answer_dict, dict):
            # æ­£è¦åŒ–ã•ã‚ŒãŸå›ç­”ã‚’å„ªå…ˆçš„ã«ä½¿ç”¨
            normalized_value = answer_dict.get('normalized_value', '')
            if normalized_value:
                return normalized_value
            
            # aliasesãŒã‚ã‚‹å ´åˆã¯æœ€åˆã®å€¤ã‚’ä½¿ç”¨
            aliases = answer_dict.get('aliases', [])
            if aliases:
                return aliases[0]
            
            # valueãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨
            value = answer_dict.get('value', '')
            if value:
                return value
        
        return str(answer_dict) if answer_dict else ''
    
    def _extract_entity_pages(self, entity_pages) -> str:
        """ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒšãƒ¼ã‚¸æƒ…å ±ã‚’æŠ½å‡ºï¼ˆWikipediaãªã©ï¼‰"""
        if not entity_pages:
            return ''
        
        # entity_pagesãŒè¾æ›¸ã®å ´åˆã®å‡¦ç†
        if isinstance(entity_pages, dict):
            titles = []
            # è¾æ›¸ã®ã‚­ãƒ¼ã‚’ã‚¤ãƒ†ãƒ¬ãƒ¼ãƒˆï¼ˆæœ€åˆã®3ã¤ã¾ã§ï¼‰
            for i, (key, page) in enumerate(entity_pages.items()):
                if i >= 3:
                    break
                if isinstance(page, dict):
                    title = page.get('title', '')
                    if title:
                        titles.append(title)
                elif isinstance(page, str):
                    titles.append(page)
            return ' | '.join(titles)
        
        # entity_pagesãŒãƒªã‚¹ãƒˆã®å ´åˆã®å‡¦ç†
        elif isinstance(entity_pages, list):
            titles = []
            for i, page in enumerate(entity_pages):
                if i >= 3:  # æœ€åˆã®3ã¤ã¾ã§
                    break
                if isinstance(page, dict):
                    title = page.get('title', '')
                    if title:
                        titles.append(title)
                elif isinstance(page, str):
                    titles.append(page)
            return ' | '.join(titles)
        
        return ''
    
    def _extract_search_results(self, search_results) -> str:
        """æ¤œç´¢çµæœï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã‚’æŠ½å‡º"""
        if not search_results:
            return ''
        
        contexts = []
        
        # search_resultsãŒè¾æ›¸ã®å ´åˆã®å‡¦ç†
        if isinstance(search_results, dict):
            # è¾æ›¸ã®ã‚­ãƒ¼ã‚’ã‚¤ãƒ†ãƒ¬ãƒ¼ãƒˆï¼ˆæœ€åˆã®2ã¤ã¾ã§ï¼‰
            for i, (key, result) in enumerate(search_results.items()):
                if i >= 2:
                    break
                if isinstance(result, dict):
                    # search_contextã¾ãŸã¯descriptionã‚’æ¢ã™
                    search_context = result.get('search_context', result.get('description', ''))
                    if search_context:
                        # é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚ã‚‹
                        if len(search_context) > 500:
                            search_context = search_context[:500] + '...'
                        contexts.append(search_context)
                elif isinstance(result, str):
                    # ç›´æ¥æ–‡å­—åˆ—ã®å ´åˆ
                    if len(result) > 500:
                        result = result[:500] + '...'
                    contexts.append(result)
        
        # search_resultsãŒãƒªã‚¹ãƒˆã®å ´åˆã®å‡¦ç†
        elif isinstance(search_results, list):
            for i, result in enumerate(search_results):
                if i >= 2:  # æœ€åˆã®2ã¤ã¾ã§
                    break
                if isinstance(result, dict):
                    # search_contextã¾ãŸã¯descriptionã‚’æ¢ã™
                    search_context = result.get('search_context', result.get('description', ''))
                    if search_context:
                        # é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚ã‚‹
                        if len(search_context) > 500:
                            search_context = search_context[:500] + '...'
                        contexts.append(search_context)
                elif isinstance(result, str):
                    # ç›´æ¥æ–‡å­—åˆ—ã®å ´åˆ
                    if len(result) > 500:
                        result = result[:500] + '...'
                    contexts.append(result)
        
        return ' '.join(contexts)
    
    def validate_data(self, df: pd.DataFrame) -> bool:
        """ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼"""
        logger.info("ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚’å®Ÿè¡Œä¸­...")
        
        is_valid = True
        
        # å¿…é ˆã‚«ãƒ©ãƒ ã®ãƒã‚§ãƒƒã‚¯
        required_columns = ['question', 'answer']
        for col in required_columns:
            if col not in df.columns:
                logger.error(f"âŒ å¿…é ˆã‚«ãƒ©ãƒ  '{col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                is_valid = False
        
        # ãƒ‡ãƒ¼ã‚¿ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        if len(df) == 0:
            logger.error("âŒ ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
            is_valid = False
        
        # è³ªå•ã¨å›ç­”ã®æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯
        if 'question' in df.columns:
            null_questions = df['question'].isnull().sum()
            empty_questions = (df['question'] == '').sum()
            if null_questions > 0 or empty_questions > 0:
                logger.warning(f"âš ï¸ ç©ºã®è³ªå•ãŒ{null_questions + empty_questions}ä»¶ã‚ã‚Šã¾ã™")
        
        if 'answer' in df.columns:
            null_answers = df['answer'].isnull().sum()
            empty_answers = (df['answer'] == '').sum()
            if null_answers > 0 or empty_answers > 0:
                logger.warning(f"âš ï¸ ç©ºã®å›ç­”ãŒ{null_answers + empty_answers}ä»¶ã‚ã‚Šã¾ã™")
        
        if is_valid:
            logger.info("âœ… ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã«åˆæ ¼ã—ã¾ã—ãŸ")
        
        return is_valid
    
    def preprocess_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """RAGç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†"""
        logger.info("ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã‚’å®Ÿè¡Œä¸­...")
        
        # NaNå€¤ã‚’ç©ºæ–‡å­—åˆ—ã«ç½®æ›
        df = df.fillna('')
        
        # RAGç”¨ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
        combined_texts = []
        for _, row in df.iterrows():
            parts = []
            
            # è³ªå•ã‚’è¿½åŠ 
            if row['question']:
                parts.append(f"Question: {row['question']}")
            
            # å›ç­”ã‚’è¿½åŠ 
            if row['answer']:
                parts.append(f"Answer: {row['answer']}")
            
            # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒšãƒ¼ã‚¸æƒ…å ±ã‚’è¿½åŠ ï¼ˆã‚ã‚Œã°ï¼‰
            if 'entity_pages' in row and row['entity_pages']:
                parts.append(f"Related Pages: {row['entity_pages']}")
            
            # æ¤œç´¢çµæœï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã‚’è¿½åŠ ï¼ˆã‚ã‚Œã°ï¼‰
            if 'search_results' in row and row['search_results']:
                parts.append(f"Context: {row['search_results']}")
            
            combined_text = '\n'.join(parts)
            combined_texts.append(combined_text)
        
        df['combined_text'] = combined_texts
        
        logger.info(f"âœ… {len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†ã—ã¾ã—ãŸ")
        
        return df
    
    def save_data(self, df: pd.DataFrame) -> tuple:
        """ãƒ‡ãƒ¼ã‚¿ã‚’CSVã€ãƒ†ã‚­ã‚¹ãƒˆã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
        
        Returns:
            tuple: (csv_path, txt_path, metadata_path)
        """
        try:
            # OUTPUTãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
            output_dir = Path("OUTPUT")
            output_dir.mkdir(exist_ok=True)
            
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ç”Ÿæˆ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # 1. CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ï¼ˆdatasetsãƒ•ã‚©ãƒ«ãƒ€ï¼‰
            df.to_csv(self.output_file, index=False, encoding='utf-8')
            logger.info(f"âœ… CSVãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {self.output_file}")
            
            # 2. ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ï¼ˆOUTPUTãƒ•ã‚©ãƒ«ãƒ€ï¼‰
            txt_filename = f"trivia_qa_{timestamp}.txt"
            txt_path = output_dir / txt_filename
            
            # combined_textã‚«ãƒ©ãƒ ã®å†…å®¹ã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            if 'combined_text' in df.columns:
                with open(txt_path, 'w', encoding='utf-8') as f:
                    for idx, text in enumerate(df['combined_text'].dropna(), 1):
                        f.write(f"=== Document {idx} ===\n")
                        f.write(text)
                        f.write("\n\n")
                logger.info(f"âœ… ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {txt_path}")
            else:
                logger.warning("âš ï¸ combined_textã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¯ä½œæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                txt_path = None
            
            # 3. å‡¦ç†æ¸ˆã¿CSVãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆOUTPUTãƒ•ã‚©ãƒ«ãƒ€ï¼‰
            processed_csv_filename = f"preprocessed_trivia_qa_{timestamp}.csv"
            processed_csv_path = output_dir / processed_csv_filename
            df.to_csv(processed_csv_path, index=False, encoding='utf-8')
            logger.info(f"âœ… å‡¦ç†æ¸ˆã¿CSVãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {processed_csv_path}")
            
            # 4. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆOUTPUTãƒ•ã‚©ãƒ«ãƒ€ï¼‰
            metadata_filename = f"metadata_trivia_qa_{timestamp}.json"
            metadata_path = output_dir / metadata_filename
            
            metadata = {
                'dataset_name': 'trivia_qa',
                'dataset_type': 'trivia_qa',
                'created_at': datetime.now().isoformat(),
                'processed_at': datetime.now().isoformat(),
                'row_count': len(df),
                'column_count': len(df.columns),
                'columns': df.columns.tolist(),
                'file_paths': {
                    'original_csv': str(self.output_file),
                    'processed_csv': str(processed_csv_path),
                    'text_file': str(txt_path) if txt_path else None
                },
                'statistics': {
                    'total_records': int(len(df)),
                    'non_empty_questions': int(df['question'].notna().sum()) if 'question' in df.columns else 0,
                    'non_empty_answers': int(df['answer'].notna().sum()) if 'answer' in df.columns else 0,
                    'avg_question_length': float(df['question'].str.len().mean()) if 'question' in df.columns else 0.0,
                    'avg_answer_length': float(df['answer'].str.len().mean()) if 'answer' in df.columns else 0.0,
                    'avg_combined_text_length': float(df['combined_text'].str.len().mean()) if 'combined_text' in df.columns else 0.0
                }
            }
            
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {metadata_path}")
            
            return str(self.output_file), str(txt_path) if txt_path else None, str(metadata_path)
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            raise
    
    def display_statistics(self, df: pd.DataFrame):
        """ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆã‚’è¡¨ç¤º"""
        logger.info("\n" + "="*50)
        logger.info("ğŸ“Š ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ")
        logger.info("="*50)
        logger.info(f"ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(df)}")
        logger.info(f"ã‚«ãƒ©ãƒ æ•°: {len(df.columns)}")
        logger.info(f"ã‚«ãƒ©ãƒ å: {', '.join(df.columns)}")
        
        if 'question' in df.columns:
            avg_question_length = df['question'].str.len().mean()
            logger.info(f"è³ªå•ã®å¹³å‡æ–‡å­—æ•°: {avg_question_length:.1f}")
        
        if 'answer' in df.columns:
            avg_answer_length = df['answer'].str.len().mean()
            logger.info(f"å›ç­”ã®å¹³å‡æ–‡å­—æ•°: {avg_answer_length:.1f}")
        
        if 'combined_text' in df.columns:
            avg_combined_length = df['combined_text'].str.len().mean()
            logger.info(f"çµåˆãƒ†ã‚­ã‚¹ãƒˆã®å¹³å‡æ–‡å­—æ•°: {avg_combined_length:.1f}")
        
        logger.info("="*50)
    
    def display_samples(self, df: pd.DataFrame, n: int = 3):
        """ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ã‚’è¡¨ç¤º"""
        logger.info("\n" + "="*50)
        logger.info(f"ğŸ“ ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€åˆã®{n}ä»¶ï¼‰")
        logger.info("="*50)
        
        for i in range(min(n, len(df))):
            logger.info(f"\n--- ã‚µãƒ³ãƒ—ãƒ« {i+1} ---")
            if 'question' in df.columns:
                logger.info(f"è³ªå•: {df.iloc[i]['question'][:200]}...")
            if 'answer' in df.columns:
                logger.info(f"å›ç­”: {df.iloc[i]['answer'][:100]}...")
            if 'entity_pages' in df.columns and df.iloc[i]['entity_pages']:
                logger.info(f"é–¢é€£ãƒšãƒ¼ã‚¸: {df.iloc[i]['entity_pages'][:100]}...")
            if 'combined_text' in df.columns:
                logger.info(f"çµåˆãƒ†ã‚­ã‚¹ãƒˆï¼ˆå…ˆé ­ï¼‰: {df.iloc[i]['combined_text'][:200]}...")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    processor = TriviaQAProcessor()
    
    try:
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
        SUBSET = "rc"  # "rc" (Reading Comprehension) or "unfiltered"
        SPLIT = "train"  # "train", "validation", or "test"  
        SAMPLE_SIZE = 1000  # å–å¾—ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°
        
        logger.info("="*50)
        logger.info("TriviaQAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™")
        logger.info(f"è¨­å®š: subset={SUBSET}, split={SPLIT}, sample_size={SAMPLE_SIZE}")
        logger.info("="*50)
        
        # 1. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        logger.info("\nğŸ“¥ Step 1: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
        df = processor.download_dataset(subset=SUBSET, split=SPLIT, sample_size=SAMPLE_SIZE)
        
        # 2. ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
        logger.info("\nğŸ” Step 2: ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼")
        is_valid = processor.validate_data(df)
        if not is_valid:
            logger.warning("ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã§å•é¡ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸãŒã€å‡¦ç†ã‚’ç¶šè¡Œã—ã¾ã™")
        
        # 3. ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
        logger.info("\nâš™ï¸ Step 3: ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†")
        df = processor.preprocess_data(df)
        
        # 4. ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        logger.info("\nğŸ’¾ Step 4: ãƒ‡ãƒ¼ã‚¿ä¿å­˜")
        csv_path, txt_path, metadata_path = processor.save_data(df)
        
        # 5. çµ±è¨ˆè¡¨ç¤º
        processor.display_statistics(df)
        
        # 6. ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
        processor.display_samples(df)
        
        logger.info("\n" + "="*50)
        logger.info("âœ… å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        logger.info("\nğŸ“‚ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«:")
        logger.info(f"  - CSVãƒ•ã‚¡ã‚¤ãƒ«: {csv_path}")
        if txt_path:
            logger.info(f"  - ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: {txt_path}")
        logger.info(f"  - ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: {metadata_path}")
        logger.info("="*50)
        
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        raise

if __name__ == "__main__":
    main()